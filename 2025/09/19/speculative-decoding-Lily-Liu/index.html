

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/avatar.png">
  <link rel="icon" href="/img/avatar.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="gdymind">
  <meta name="keywords" content="">
  
    <meta name="description" content="Speaker: Lily Liu  Working at OpenAI Graduated from UC Berkeley in early 2025 vLLM speculative decoding TL  1. Why is LLM generation slow? GPU memory hierarchy. A100 example: SRAM is super fast (19 TB">
<meta property="og:type" content="article">
<meta property="og:title" content="speculative decoding 02">
<meta property="og:url" content="https://gdymind.github.io/2025/09/19/speculative-decoding-Lily-Liu/index.html">
<meta property="og:site_name" content="gdymind&#39;s Blog">
<meta property="og:description" content="Speaker: Lily Liu  Working at OpenAI Graduated from UC Berkeley in early 2025 vLLM speculative decoding TL  1. Why is LLM generation slow? GPU memory hierarchy. A100 example: SRAM is super fast (19 TB">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/6dd03710-9530-4361-8e95-c3297b974442.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/cb549902-f9eb-4e25-8348-8007eac03e69.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%201.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/68671193-3377-4fe0-9bcb-6c0724ee208b.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%202.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%203.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%204.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%205.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%206.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%207.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%208.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%209.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%2010.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%2011.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%2012.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/14f4714f-2fa3-4856-b5f3-ae40243c199e.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%2013.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%2014.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/0f0b9d97-fafa-4dc3-8b95-8ec3a72998c5.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%2015.png">
<meta property="og:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/image%2016.png">
<meta property="article:published_time" content="2025-09-20T06:04:57.000Z">
<meta property="article:modified_time" content="2026-01-20T07:26:00.976Z">
<meta property="article:author" content="gdymind">
<meta property="article:tag" content="LLM inference">
<meta property="article:tag" content="vLLM">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://gdymind.github.io/img/lily-liu-spec-decoding/6dd03710-9530-4361-8e95-c3297b974442.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>speculative decoding 02 - gdymind&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"gdymind.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>gdymind&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/sky.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="speculative decoding 02"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-09-19 23:04" pubdate>
          September 19, 2025 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          504 words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          5 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">speculative decoding 02</h1>
            
            
              <div class="markdown-body">
                
                <p><strong>Speaker</strong>: Lily Liu</p>
<ul>
<li>Working at OpenAI</li>
<li>Graduated from UC Berkeley in early 2025</li>
<li>vLLM speculative decoding TL</li>
</ul>
<h1 id="1-Why-is-LLM-generation-slow"><a href="#1-Why-is-LLM-generation-slow" class="headerlink" title="1. Why is LLM generation slow?"></a>1. Why is LLM generation slow?</h1><ul>
<li>GPU memory hierarchy. A100 example:<ul>
<li>SRAM is super fast (19 TB&#x2F;s) but super small (20 MB)</li>
<li>HBM: large (40 or 80 GB) but slower (1.5 TB&#x2F;s)</li>
<li>Main memory: very large but super slow</li>
</ul>
</li>
</ul>
<p><img src="/img/lily-liu-spec-decoding/6dd03710-9530-4361-8e95-c3297b974442.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<ul>
<li>a llama3-70B model with BF16 data type occupies 140 GB for its model weights<ul>
<li>the size is even too big to fit into HBM</li>
</ul>
</li>
<li>Suppose we can store the whole model in HBM. You still need to load model weights from HBM to SRAM</li>
</ul>
<p>Auto-regressive token generation: one token is generated each time → it’s <strong>memory-bound (need to load model weights and KV cache without enough compute)</strong></p>
<p><img src="/img/lily-liu-spec-decoding/cb549902-f9eb-4e25-8348-8007eac03e69.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<h1 id="2-What-is-speculative-decoding-SD"><a href="#2-What-is-speculative-decoding-SD" class="headerlink" title="2. What is speculative decoding (SD)?"></a>2. What is speculative decoding (SD)?</h1><ul>
<li><strong>Goal</strong>: reduce generation latency by amortizing bandwidth.<ul>
<li>The latency is critical to user experience</li>
<li>The two main techniques to reduce inference latency: speculative decoding and sharding<ul>
<li><strong>Sharding</strong>: partition LLM to multiple GPUs, which can do inference together</li>
<li><strong>Speculative decoding</strong>: kind of an algorithm-wise optimization, which does not need more hardware</li>
</ul>
</li>
</ul>
</li>
<li><strong>Intuition</strong>: Some tokens are easier to generate than others.<ul>
<li>Example:<ul>
<li>Prompt: What is your name?</li>
<li>Response: My name is Lily</li>
<li>There are some tokens that are easier to guess, for example, “is” in the response</li>
</ul>
</li>
<li>For easier tokens, maybe we don’t need a powerful LLM to generate</li>
</ul>
</li>
<li><strong>Idea</strong>: Use a small Language Model (LM) to propose tokens, and LLMs to verify multiple tokens in a single forward pass.<ul>
<li>The verification is done in a single forward pass.<ul>
<li>You pull the model weight <strong>once</strong> from HBM to SRAM, run the LLM forward pass once, then you know if T1, T2, T3 are correct or not.</li>
<li>It reduces memory accesses by amortizing model weights loading cost.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/img/lily-liu-spec-decoding/image.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<ul>
<li>Lossless!<ul>
<li>It is mathematically proven that SD will not decrease LLM’s accuracy. The output tokens would be exactly the same as if only the LLM generates tokens.</li>
</ul>
</li>
</ul>
<h1 id="3-Factors-that-affect-SD-speedup"><a href="#3-Factors-that-affect-SD-speedup" class="headerlink" title="3. Factors that affect SD speedup"></a>3. Factors that affect SD speedup</h1><ul>
<li><strong>Token acceptance rate</strong><ul>
<li>Assume in the extreme case, you have a very smart small model with only 1M parameters, that can guess all the tokens right, then you don’t need an LLM.</li>
<li>The point here is that how smart the small model is (i.e., how many tokens it can guess correctly) affects the SD performance.</li>
</ul>
</li>
<li><strong>Speculation length (k)</strong><ul>
<li>This defines how many tokens the small model generates for the LLM to verify</li>
</ul>
</li>
<li><strong>Cost of the drafting method (c &#x3D; ExecTime(draft) &#x2F; ExecTime(target))</strong><ul>
<li>How expensive it is to run the small model</li>
<li>In the extreme case, if the small model has the same size as the LLM, then there is no saving at all.</li>
</ul>
</li>
</ul>
<p><img src="/img/lily-liu-spec-decoding/image%201.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<h1 id="4-Draft-model-based-SD"><a href="#4-Draft-model-based-SD" class="headerlink" title="4. Draft-model based SD"></a>4. Draft-model based SD</h1><ul>
<li>Use a small draft LM to guess, and a large LM to verify.</li>
<li>The method was proposed in year 2023 when SD was first proposed</li>
</ul>
<p><img src="/img/lily-liu-spec-decoding/68671193-3377-4fe0-9bcb-6c0724ee208b.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>Some commonly seen (draft, target) model pairs</p>
<ul>
<li>(Llama-68M, Llama-2-7B)</li>
<li>(Llama-2-7B, Llama-2-70B)</li>
<li>(Llama-3-8B, Llama-3-70B)</li>
<li>(?, Llama-3-8B): even today, we haven’t found a good draft model for Llama3-8B → You need to train one, which is expensive</li>
</ul>
<p>Challenge: How to find the draft model?</p>
<p>The first version of SD in vLLM was draft-model based. Performance:</p>
<p><img src="/img/lily-liu-spec-decoding/image%202.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<ul>
<li>The x-axis is query per second (QPS). When QPS is low, we can have up to 1.5x speedup.</li>
<li>SD helps to relieve the memory-bound issue. When QPS is high, the workload is more compute-bound, and thus SD is less helpful.</li>
</ul>
<h1 id="5-Prompt-lookup-decoding"><a href="#5-Prompt-lookup-decoding" class="headerlink" title="5. Prompt lookup decoding"></a>5. Prompt lookup decoding</h1><ul>
<li>This method is model-free — no need for a draft model.</li>
<li>This is a pretty powerful method, especially in industry.<ul>
<li>Cursor has a blog post on how they applied this technique.</li>
</ul>
</li>
<li>Idea: using the following 2-gram lookup table as an example.</li>
<li>It tries to match the words in the “2-gram” column, and return the “3 speculative tokens”</li>
</ul>
<p><img src="/img/lily-liu-spec-decoding/image%203.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>Here is the result</p>
<p><img src="/img/lily-liu-spec-decoding/image%204.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<h1 id="6-Medusa-Eagle-MLPSpeculator-MTP"><a href="#6-Medusa-Eagle-MLPSpeculator-MTP" class="headerlink" title="6. Medusa&#x2F;Eagle&#x2F;MLPSpeculator&#x2F;MTP"></a>6. Medusa&#x2F;Eagle&#x2F;MLPSpeculator&#x2F;MTP</h1><ul>
<li>We don’t have to train a new model, and we don’t want a simple lookup table</li>
<li>Instead, we will have some additional layers on top of the original LM.</li>
<li>Looking at the figure, originally you will send the last hidden states to LM Head to generate the next token.</li>
<li>Now we are sending the last hidden states to some additional heads to propose tokens.</li>
<li>This way, we can utilize some info from the main model. In addition, we don’t need to train an independent model.</li>
</ul>
<p><img src="/img/lily-liu-spec-decoding/image%205.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<h2 id="MTP-Multi-token-prediction"><a href="#MTP-Multi-token-prediction" class="headerlink" title="MTP (Multi-token prediction)"></a>MTP (Multi-token prediction)</h2><ul>
<li>MTP was first proposed by Deepseek V3, and it is used more and more by open-source models.</li>
<li>Looking at the figure below, the leftmost part is still the main model.</li>
<li>There are some additional heads, which have a single layer of Transformer.</li>
<li>You use these additional layers to propose tokens, and use the main model for verification.</li>
</ul>
<p><img src="/img/lily-liu-spec-decoding/image%206.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<ul>
<li>MTP was originally proposed mainly for training purposes.<ul>
<li>Nowadays models are super large (like 100B) and therefore very powerful.</li>
<li>Many researchers believe these models have the capability to generate more than one token.</li>
<li>During training, if we can improve the model’s accuracy to propose more tokens, then it will also improve its capability to propose a single token.</li>
</ul>
</li>
</ul>
<h1 id="7-Research-Online-Speculative-Decoding"><a href="#7-Research-Online-Speculative-Decoding" class="headerlink" title="7. Research: Online Speculative Decoding"></a>7. Research: Online Speculative Decoding</h1><p>This is a project Lily did during her PhD</p>
<p><strong>Can we make the drafting method even better?</strong></p>
<h2 id="A-typical-inference-workflow-with-SD"><a href="#A-typical-inference-workflow-with-SD" class="headerlink" title="A typical inference workflow with SD"></a>A typical inference workflow with SD</h2><ul>
<li>Given a prompt, the draft model not only proposes tokens but also gives the probability distribution of the proposed tokens.<ul>
<li>Example: the proposed token t1 has the probability of 60%.</li>
</ul>
</li>
<li>Then the draft model sends the proposed tokens to the target model.</li>
<li>Then the target model verifies the proposed tokens. For example, t1 and t2 are correct, while t3 is incorrect.</li>
</ul>
<p><img src="/img/lily-liu-spec-decoding/image%207.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>Two observations:</p>
<ul>
<li>Static process, draft model is unchanged.</li>
<li>We have labels (positions and prob distributions) for free.</li>
</ul>
<h2 id="Online-SD"><a href="#Online-SD" class="headerlink" title="Online SD"></a>Online SD</h2><p>Instead of using a static draft model, let’s update it.</p>
<ul>
<li>We store the proposed tokens and their prob distributions in a buffer, and use them to improve the draft model.</li>
</ul>
<p><img src="/img/lily-liu-spec-decoding/image%208.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>Result: the x-axis is how many records you have seen so far, and the y-axis is the token acceptance rate (higher is better).</p>
<p><img src="/img/lily-liu-spec-decoding/image%209.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>Online updates can improve token acceptance rate very quickly, achieving close to offline distillation.</p>
<h2 id="Customized-draft-model"><a href="#Customized-draft-model" class="headerlink" title="Customized draft model"></a>Customized draft model</h2><p>Use multiple draft models depending on tasks</p>
<p><img src="/img/lily-liu-spec-decoding/image%2010.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p><img src="/img/lily-liu-spec-decoding/image%2011.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<h1 id="8-Research-Dynamic-propose-length"><a href="#8-Research-Dynamic-propose-length" class="headerlink" title="8. Research: Dynamic propose length"></a>8. Research: Dynamic propose length</h1><h2 id="vLLM-SD-results"><a href="#vLLM-SD-results" class="headerlink" title="vLLM + SD results"></a>vLLM + SD results</h2><p>The results were from the first version of SD implementation in vLLM.</p>
<ul>
<li>SD has better speedup in low QPS.</li>
<li>When QPS &#x3D; 10, SD has slowdowns.</li>
</ul>
<p><img src="/img/lily-liu-spec-decoding/image%2012.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<ul>
<li>Intuition: 1) SD solves the problem if generation is memory-bound. It wastes extra computations for better memory access efficiency. When QPS is high, it’s more compute-bound. 2) What’s worse, the tokens proposed by the draft model may not be accepted. Those extra “wasted” computes have a larger impact when your workload is more compute-bound.</li>
</ul>
<h2 id="Dynamic-propose-length"><a href="#Dynamic-propose-length" class="headerlink" title="Dynamic propose length"></a>Dynamic propose length</h2><ul>
<li>Dynamically adjust the propose length based on system load and speculation accuracy.</li>
<li>Paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.14066">https://arxiv.org/pdf/2406.14066</a><ul>
<li>TurboSpec: Closed-loop Speculation Control System for Optimizing LLM Serving Goodput</li>
</ul>
</li>
</ul>
<p><img src="/img/lily-liu-spec-decoding/14f4714f-2fa3-4856-b5f3-ae40243c199e.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<h1 id="9-vLLM-SD"><a href="#9-vLLM-SD" class="headerlink" title="9. vLLM + SD"></a>9. vLLM + SD</h1><h2 id="Timeline"><a href="#Timeline" class="headerlink" title="Timeline"></a>Timeline</h2><p><img src="/img/lily-liu-spec-decoding/image%2013.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p><img src="/img/lily-liu-spec-decoding/image%2014.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>Now not only Deepseek, Qwen model also uses MTP.</p>
<h2 id="Results-in-vLLM-v0-8-5"><a href="#Results-in-vLLM-v0-8-5" class="headerlink" title="Results in vLLM v0.8.5"></a>Results in vLLM v0.8.5</h2><p>about 1.5x to 2x speedup</p>
<p><img src="/img/lily-liu-spec-decoding/0f0b9d97-fafa-4dc3-8b95-8ec3a72998c5.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>We want to make sure the implementation in vLLM is efficient.</p>
<p>Let’s look at the execution time breakdown.</p>
<ul>
<li>The leftmost figure is without SD. CPU overhead is ~15%, while LLM inference takes ~85%.</li>
<li>The middle figure is n-gram. The CPU overhead is still ~15%. The lookup time is less than 5%. N-gram is very efficient.</li>
<li>For draft-model based methods as shown in the rightmost figure, the propose overhead is ~20%. But you can still see speedups. Because the additional heads is only 5% parameters of all model parameters.</li>
</ul>
<p><img src="/img/lily-liu-spec-decoding/image%2015.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<h1 id="10-QA"><a href="#10-QA" class="headerlink" title="10. QA"></a>10. QA</h1><p><strong>Q1: which workloads are good for SD?</strong></p>
<ul>
<li>For companies without many resources, we highly recommend n-gram.<ul>
<li>Suitable for code editing and summarization, where the output and the input have large overlaps. You can think about whether the task adds new info or just compresses the original info</li>
<li>We can see 2x to even 10x speedups on some coding tasks.</li>
<li>Cursor relies on n-gram very heavily.</li>
</ul>
</li>
<li>For chatting apps or open-ended QA, try Eagle</li>
</ul>
<p><strong>Q2: how do we reinforce the draft model?</strong></p>
<ul>
<li><p>Background</p>
<blockquote>
<p>Knowledge distillation is a general framework to align the predictive distribution of a small model (i.e., student model) with that of a larger one (i.e., teacher model).</p>
</blockquote>
<blockquote>
<p>Knowledge distillation is highly effective for speculative decoding. The draft model acts as the student and the target model serves as the teacher.</p>
</blockquote>
</li>
<li><p>Try different KL divergence metrics for the student and the teacher.</p>
</li>
<li><p>Use traditional distillation methods with different loss functions.</p>
</li>
</ul>
<p><img src="/img/lily-liu-spec-decoding/image%2016.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p><strong>Q3: the small LM may co-locate with the main LM. What is their size ratio? Also in the project that has multiple draft models, are they placed in the same machine or not?</strong></p>
<ul>
<li>It’s a deep research question. No conclusions yet.</li>
<li>In production, draft models are less used due to their complexity.</li>
<li>Size ratio: say if you use a bigger draft model, will the acceptance rate be linearly increased?<ul>
<li>No. The benefit diminishes quickly.</li>
<li>Let’s say for (llama-3B draft, llama-70b target) can already have an 80% acceptance rate. But if you use llama3-8B, the acceptance rate is only ~85%.</li>
<li>Actually, for 70B-scale models, the best scale for target models is ~3B.</li>
</ul>
</li>
<li>Because the draft model is so small, we just put it on a single machine.</li>
<li>The draft model will occupy at least 20% of the main model’s GPU execution cycles. That’s why MTP, where the draft and the main LM share the same model, is more and more popular.</li>
<li>MTP also has KV Cache, but they are much smaller than the draft model’s KV Cache. MTP in general is a better approach.</li>
</ul>
<p><strong>Q3: Currently, n-gram in vLLM is purely running in CPU. Is there a way to use a GPU to speed up the longest prefix matching process?</strong></p>
<ul>
<li>We made that design on purpose. We tried the classical KMP algorithm, but it was not easy to implement. So we used numpy to implement this.</li>
<li>During benchmarking, we found that the overhead is less than 5%, which is acceptable.</li>
<li>String matching is not very GPU-friendly. But if you have a faster GPU kernel, we are happy to check it out.</li>
<li>if your input length is 10s of thousands of tokens, the overhead is usually less than 3%.</li>
<li>Can we use JAX to do this? It can be automatically used on a GPU. → We haven’t, but it’s worth a try.</li>
<li>the current implementation is already in C, not Python, so the efficiency is good enough.</li>
</ul>
<p><strong>Q4: is SD useful for reasoning models?</strong></p>
<ul>
<li>Draft-model based: it’s hard to be useful. If the model is too small, its reasoning capability may not be good enough.</li>
<li>MTP: unknown. Haven’t tested it yet. It might work because MTP’s acceptance rate is very high (80%-90%). MTP is trained with the main model. So we guess it will have some reasoning capabilities.</li>
<li>N-gram. Definitely take a try. Why? Reasoning tasks have long outputs. For long context, n-gram is still very efficient (i.e., the proposing overhead is very small). In addition, you can see many repeated patterns like “oh wait”, “no, the answer is incorrect” for reasoning models.</li>
<li>there are customers that are currently using SD for reasoning models.</li>
</ul>
<p><strong>Q5: Have you ever tried using the tokens that were not the most probable (like the 2nd most likely tokens)?</strong> - Haven’t tried that, but we are aware of this line of work. There is one sampling work called typical sampling: instead of using the most likely token, we also do sampling for the draft model. You have different ways to sample, and some have a higher acceptance rate.</p>
<ul>
<li>There is a PR that samples the draft model. We found that sampling is better than argmax(), especially when the temperature is high.</li>
<li>temperature&#x3D;0 is the worst case for SD, because it does not have error tolerance. If you propose a wrong token, it’s just wrong. But if the temperature is very high, it could be just slightly wrong.</li>
</ul>
<p><strong>Q6: are there resource limitations for the online learning project with multiple draft models?</strong></p>
<ul>
<li>We didn’t have enough engineering efforts on that.</li>
<li>In addition, you need a backward pass to update the draft model. So you kind of need a training framework to do auto-grad.</li>
<li>There are system implementation challenges. For example, are they two systems or one system?</li>
</ul>
<p><strong>Q7: for SD, what accuracy is practically useful to speedup? Is that 80%, 85%, or something else?</strong></p>
<ul>
<li>Depends on how expensive it is to run the draft model.</li>
<li>If the draft model is super fast, maybe 40% or 50% acceptance rate can already work.</li>
<li>Actually, originally we even used a 56M draft model to speculate a 70B model with acceptance rate ~50%. But it still had 1.5x speedup.</li>
</ul>
<p><strong>Q8: what is the optimal acceptance rate?</strong> - Depends on the acceptance rate.</p>
<ul>
<li>For MTP, 3 to 5 is doable. In vLLM, the most common setting is 3.</li>
</ul>
<p><strong>Q9: can we have a hierarchy of draft models? Draft1 speculates draft2, draft2 speculates draft3, …</strong></p>
<ul>
<li>Stanford has research on that. It’s not production-ready because serving multiple draft models is still challenging, especially when you need to manage their KV Cache. In addition, how to place these models among machines is also an issue.</li>
</ul>
<p>Source</p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=epOCf1Ik2Cs">https://www.youtube.com/watch?v=epOCf1Ik2Cs</a></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/LLM-inference/" class="print-no-link">#LLM inference</a>
      
        <a href="/tags/vLLM/" class="print-no-link">#vLLM</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>speculative decoding 02</div>
      <div>https://gdymind.github.io/2025/09/19/speculative-decoding-Lily-Liu/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>gdymind</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>September 19, 2025</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/11/08/Truncated-Importance-Sampling-TIS-in-RL/" title="Truncated Importance Sampling (TIS) in RL">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Truncated Importance Sampling (TIS) in RL</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/06/06/vLLM-multi-modal/" title="vLLM 05 - vLLM multi-modal support">
                        <span class="hidden-mobile">vLLM 05 - vLLM multi-modal support</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    
      <script type="text/javascript">
        var disqus_config = function() {
          this.page.url = 'https://gdymind.github.io/2025/09/19/speculative-decoding-Lily-Liu/';
          this.page.identifier = '/2025/09/19/speculative-decoding-Lily-Liu/';
        };
        Fluid.utils.loadComments('#disqus_thread', function() {
          var d = document, s = d.createElement('script');
          s.src = '//' + 'gdymind' + '.disqus.com/embed.js';
          s.setAttribute('data-timestamp', new Date());
          (d.head || d.body).appendChild(s);
        });
      </script>
    
    <noscript>Please enable JavaScript to view the comments</noscript>
  </div>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        visited 
        <span id="busuanzi_value_site_pv"></span>
         times
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        unique visitors: 
        <span id="busuanzi_value_site_uv"></span>
        
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>

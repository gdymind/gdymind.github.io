<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>vLLM 01 - P/D disaggregation</title>
    <link href="/2025/03/28/vllm-P-D-disaggregation/"/>
    <url>/2025/03/28/vllm-P-D-disaggregation/</url>
    
    <content type="html"><![CDATA[<p><strong>Recap for Tensor parallelism (TP)</strong>: all-gather ops</p><ul><li>linear MN NK → MK matmul</li><li>decompose to several MN’ N’<em>K  → M</em>K</li><li>Element-wise sum all M*K matrices</li></ul><h2 id="Why-P-D-disaggregation"><a href="#Why-P-D-disaggregation" class="headerlink" title="Why P&#x2F;D disaggregation?"></a>Why P&#x2F;D disaggregation?</h2><ul><li><strong>Initial scheduler logic in vLLM</strong>: prioritize prefill for good throughput</li><li><strong>Problem</strong>: prefill may slow down other requests’ decode</li></ul><h2 id="How-to-mix-P-and-D-together"><a href="#How-to-mix-P-and-D-together" class="headerlink" title="How to mix P and D together?"></a><strong>How to mix P and D together?</strong></h2><ul><li>Well, even their input shapes are different</li><li>Decode is vector*matrix (e.g., Q projection is 1xd * dxd). not many FLOPs + needs to load model weights and KV cache -&gt; it’s memory bound</li><li>Prefill is matrix*matrix (nxd * dxd). model weights are reused -&gt; compute bound</li><li><strong>Solutions</strong>: P&#x2F;D disaggregation, chunked prefill</li></ul><h2 id="Chunked-prefill"><a href="#Chunked-prefill" class="headerlink" title="Chunked prefill"></a>Chunked prefill</h2><h3 id="Motivation-of-chunked-prefill"><a href="#Motivation-of-chunked-prefill" class="headerlink" title="Motivation of chunked prefill"></a><strong>Motivation of chunked prefill</strong></h3><ul><li>Unify prefill and decode procedure: based on some KV cache, P or D does some attention &amp; linear computes, to generate some new tokens</li><li>The compute flow of prefill and decode are the same, and they just have difference in their input and output shapes</li><li>Chunked prefill becomes possible if the kernel can accept different shapes.</li><li>Now the scheduler can make simpler decision: only care about how many tokens to schedule in the current batch</li></ul><h3 id="Chunk-size"><a href="#Chunk-size" class="headerlink" title="Chunk size"></a><strong>Chunk size</strong></h3><ul><li>chunk size is very important</li><li>if chunk size is too large, then a decode can be slow when batching with a prefill -&gt; decode is slowed down by prefill</li><li>if chunk size is too small, then<ol><li>GPU utilization is bad, and FLOPs is low</li><li>it needs many batches to finish the prefill for a long prompt -&gt; prefill is slowed down by decode</li></ol></li></ul><h3 id="When-to-use-chunked-prefill"><a href="#When-to-use-chunked-prefill" class="headerlink" title="When to use chunked prefill"></a><strong>When to use chunked prefill</strong></h3><ul><li><p>prompts are extremely long (e.g., 10k or 100k tokens)</p></li><li><p>why? during attention compute, there will be temp buffers holding QKV, whose memory is proportional to context length. chunked prefill reduces context length, and thus reduces the buffer size</p></li><li><p>want smooth generation, e.g., SLO for p99 inter-token latency</p></li></ul><h2 id="Key-problems-P-D-disaggregation"><a href="#Key-problems-P-D-disaggregation" class="headerlink" title="Key problems P&#x2F;D disaggregation"></a>Key problems P&#x2F;D disaggregation</h2><h3 id="Connector-how-to-transfer-KV-cache"><a href="#Connector-how-to-transfer-KV-cache" class="headerlink" title="Connector: how to transfer KV cache?"></a><strong>Connector: how to transfer KV cache?</strong></h3><ul><li><strong>pooling mode</strong>: shared memory pool. both sender and receiver need high-bandwidth connection to the memory pool</li><li><strong>p2p mode</strong>: sender communicates with receiver directly; better performance; much harder to implement</li><li>Frameworks that support KV cache transfer: LMCache, Mooncake, NIXL</li></ul><p><strong>LMCache</strong> can do KV extraction and transfer</p><ul><li>support both pooling and p2p mode</li><li>current target use cases: prefill-decode disaggregation, KV cache offloading</li></ul><p><strong>Mooncake</strong></p><ul><li>KV cache storage: replicas, RDMA support, etc.</li><li>pooling mode</li></ul><p><strong>NIXL</strong>: p2p mode</p><ul><li>it does support p2p semantics directly. instead, it supports some lower-level data transfer features</li><li>backend is UXL, which is a more general data transfer library than NCCL’s own backend</li></ul><h3 id="How-to-extract-inject-KV-cache-in-vLLM"><a href="#How-to-extract-inject-KV-cache-in-vLLM" class="headerlink" title="How to extract &amp; inject KV cache in vLLM?"></a><strong>How to extract &amp; inject KV cache in vLLM?</strong></h3><p>connector API is called in model_runner</p><p>path: <code>vllm/worker/model_runner.py</code></p><ul><li><p>model runner is used to wrap model forward pass</p></li><li><p>preparing the input for forward</p></li><li><p>post-process forward outputs</p></li><li><p>one major part of model runner is to receive &amp; send KV cache</p></li></ul><p><strong>steps</strong></p><ul><li>before forward, try receiving KV cache and injecting into vLLM’s paged memory</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Receive KV cache in distributed KV cache transfer setting</span><br><span class="hljs-comment"># In disagg prefill setting, it will also recv hidden states and bypass</span><br><span class="hljs-comment"># model forwarding</span><br><span class="hljs-comment"># In KV cache database setting, it will change the model input so that</span><br><span class="hljs-comment"># we can skip prefilling on tokens that successfully received KV caches</span><br><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> The receive operation is blocking</span><br>bypass_model_exec = <span class="hljs-literal">False</span><br><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.need_recv_kv(model_input, kv_caches):<br>    hidden_or_intermediate_states, bypass_model_exec, model_input = \<br>        get_kv_transfer_group().recv_kv_caches_and_hidden_states(<br>            <span class="hljs-comment"># model is used to know which layer the current worker</span><br>            <span class="hljs-comment"># is working on, so that we can receive KV for only those</span><br>            <span class="hljs-comment"># layers.</span><br>            model_executable,<br>             ,<br>            kv_caches=kv_caches<br>        )<br><br></code></pre></td></tr></table></figure><ul><li>after forward, extract KV cache from vLLM’s paged memory, and send it outside</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Sending KV cache in distributed KV cache transfer setting</span><br>  <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> the send operation is non-blocking</span><br>  <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.need_send_kv(model_input, kv_caches):<br>      get_kv_transfer_group().send_kv_caches_and_hidden_states(<br>          <span class="hljs-comment"># model_executable is used to know which layer the current</span><br>          <span class="hljs-comment"># worker is working on, so that we can send KV for only those</span><br>          <span class="hljs-comment"># layers.</span><br>          model_executable,<br>          model_input,<br>          kv_caches,<br>          hidden_or_intermediate_states,<br>      )<br><br></code></pre></td></tr></table></figure><p>How are connector functions implemented?</p><p>check path <code>vllm/distributed/kv_transfer/kv_connector/</code></p><p>There are many possible connectors to use, let’s use <code>SimpleConnector</code> code as an example:</p><p>receive KV cache</p><ul><li>check if  <code>model_input</code>’s tokens exist in the outside world</li><li>if they do exist, we compute where the KV cache should be inserted into vLLM’s page memory (parse page table, use page index to find the right place)</li><li>additionally, it should rebuild the model input to tell the scheduler there is KV cache already, and no prefill should be done again</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">send_kv_caches_and_hidden_states</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    model_executable: torch.nn.Module,</span><br><span class="hljs-params">    model_input: <span class="hljs-string">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span>,</span><br><span class="hljs-params">    kv_caches: <span class="hljs-type">List</span>[torch.Tensor],</span><br><span class="hljs-params">    hidden_or_intermediate_states: <span class="hljs-type">Union</span>[torch.Tensor,</span><br><span class="hljs-params">                                         IntermediateTensors],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br><br>    <span class="hljs-comment"># some initial setup code</span><br>    <span class="hljs-comment"># ...</span><br><br>    <span class="hljs-comment"># query_lens contains new KV caches that are added to vLLM.</span><br>    <span class="hljs-comment"># so we will send them to decode instance</span><br>    <span class="hljs-comment"># FIXME(Kuntai): This assume that all requests are prefill.</span><br>    <span class="hljs-keyword">for</span> idx, slen <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(seq_lens):<br>        start_pos = <span class="hljs-built_in">sum</span>(seq_lens[:idx])<br>        end_pos = start_pos + slen<br><br>        <span class="hljs-keyword">if</span> start_pos &gt;= num_prefill_tokens:<br>            <span class="hljs-comment"># vllm/worker/model_runner.py::_prepare_model_input_tensors:</span><br>            <span class="hljs-comment"># - input_tokens[:num_prefill_tokens] contains prefill tokens.</span><br>            <span class="hljs-comment"># - input_tokens[num_prefill_tokens:] contains decode tokens.</span><br>            logger.warning(<span class="hljs-string">&quot;You have some decode requests while using &quot;</span><br>                           <span class="hljs-string">&quot;SimpleConnector. Their KVCache won&#x27;t be sent.&quot;</span>)<br>            <span class="hljs-keyword">break</span><br><br>        current_tokens = input_tokens_tensor[start_pos:end_pos]<br><br>        keys, values = [], []<br><br>        <span class="hljs-keyword">for</span> layer_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start_layer, end_layer):<br>            kv_cache = kv_caches[layer_id - start_layer]<br><br>            key_cache = kv_cache[<span class="hljs-number">0</span>].reshape(-<span class="hljs-number">1</span>, num_heads, head_size)<br>            value_cache = kv_cache[<span class="hljs-number">1</span>].reshape(-<span class="hljs-number">1</span>, num_heads, head_size)<br><br>            current_slot_mapping = slot_mapping_flat[start_pos:end_pos]<br><br>            keys.append(key_cache[current_slot_mapping].unsqueeze(<span class="hljs-number">0</span>))<br>            values.append(value_cache[current_slot_mapping].unsqueeze(<span class="hljs-number">0</span>))<br><br>        keys = torch.cat(keys, dim=<span class="hljs-number">0</span>)<br>        values = torch.cat(values, dim=<span class="hljs-number">0</span>)<br><br>        <span class="hljs-variable language_">self</span>.insert(current_tokens,<br>                    torch.ones_like(current_tokens,<br>                                    dtype=<span class="hljs-built_in">bool</span>), keys, values,<br>                    hidden_or_intermediate_states[start_pos:end_pos])<br><br>    logger.debug(<span class="hljs-string">&quot;[rank%d]: KV send DONE.&quot;</span>, torch.distributed.get_rank())<br><br></code></pre></td></tr></table></figure><p>Send KV is similar</p><h3 id="When-to-send-requests-to-P-and-D"><a href="#When-to-send-requests-to-P-and-D" class="headerlink" title="When to send requests to P and D?"></a>When to send requests to P and D?</h3><ul><li>First P then D: when P finishes, it will notify the router, and the router will tell D</li><li>First D then P: because D is the process to generate responses, let D be responsible for asking for KV cache from P</li></ul><h3 id="KV-offloading"><a href="#KV-offloading" class="headerlink" title="KV offloading"></a>KV offloading</h3><ul><li>Connector can also be used for KV cache offloading</li><li>In such cases, model sharding can be very useful. For example, in GPU-to-CPU KV cache offloading, if TP&#x3D;8, the total bandwidth is 8 * single_gpu_bandwidth.</li></ul><p>Source:</p><p><a href="https://www.youtube.com/watch?v=ih6fcJnhoJI">https://www.youtube.com/watch?v=ih6fcJnhoJI</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>vLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>

<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>vLLM 03 - prefix caching</title>
    <link href="/2026/01/19/vLLM-03-prefix-caching/"/>
    <url>/2026/01/19/vLLM-03-prefix-caching/</url>
    
    <content type="html"><![CDATA[<h1 id="KV-cache-aware-routing-in-multi-host-serving"><a href="#KV-cache-aware-routing-in-multi-host-serving" class="headerlink" title="KV-cache-aware routing in multi-host serving"></a>KV-cache-aware routing in multi-host serving</h1><p><a href="https://github.com/vllm-project/production-stack/issues/59">https://github.com/vllm-project/production-stack/issues/59</a></p><p><a href="https://github.com/kubernetes-sigs/gateway-api-inference-extension/issues/498">https://github.com/kubernetes-sigs/gateway-api-inference-extension/issues/498</a></p><h2 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution 1"></a>Solution 1</h2><ul><li>Use string matching instead of token-ID-based matching<ul><li>tokenization itself is pretty slow (it takes several microseconds) so running it for every request creates huge overhead.</li></ul></li><li>implement the string server (e.g., using Redis) as the storage backend</li></ul><h2 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution 2"></a>Solution 2</h2><ul><li>Router can send a request to the KV cache management system: which server has the longest matched prefix?</li><li>vLLM production stack team wants to use this solution: it decouples the logic</li></ul><p><a href="https://github.com/vllm-project/production-stack/issues/59">https://github.com/vllm-project/production-stack/issues/59</a></p><p><strong>KV-cache-aware routing vs load balancing</strong>: Needs tradeoff. KV-cache-aware routing may route requests to the same node, which is bad for load balancing.</p><h1 id="KV-cache-store-interface"><a href="#KV-cache-store-interface" class="headerlink" title="KV cache store interface"></a>KV cache store interface</h1><p>Back in 2023 when vLLM was not out, let’s take a look at Huggingface’s LLM interface:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">llm.inference(<br>    <span class="hljs-built_in">input</span>.tokens: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>], <span class="hljs-comment"># N tokens</span><br>    previous_kv_cache: <span class="hljs-built_in">list</span>[Tensor], <span class="hljs-comment"># M tokens&#x27; KV cache, where M &lt; N</span><br>) -&gt; output_tokens, new_kv_cache<br><br>output_tokens: <span class="hljs-comment"># N&#x27; new tokens</span><br>new_kv_cache: <span class="hljs-comment"># (N+N&#x27;) tokens&#x27; KV cache</span><br><br></code></pre></td></tr></table></figure><p>Let’s not worry about PagedAttention or other complicated things in vLLM.</p><p>How do you design a KV cache?</p><p>KV cache design is essentially similar to traditional <strong>key-value store</strong> system design (such as in Redis, Object store, database, etc.)</p><p>Our key-value pair in this context is:</p><ul><li>key: tokens</li><li>value: KV cache tensors</li></ul><p>Then naturally, the interface should be like</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">KVCacheStore</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">store</span>(<span class="hljs-params">tokens, kv_cache_tensors</span>):<br>        <span class="hljs-keyword">pass</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">retrieve</span>(<span class="hljs-params">tokens</span>) -&gt; kv_cache_tensors<br>        <span class="hljs-keyword">pass</span><br><br></code></pre></td></tr></table></figure><p>Traditional key-value store usually supports <strong>exact match</strong> (given the full key, return the value)</p><p>KVCacheStore needs another query called <strong>prefix matching</strong> (actually, some traditional KV store also support this)</p><h1 id="Prefix-matching"><a href="#Prefix-matching" class="headerlink" title="Prefix matching"></a>Prefix matching</h1><p>Tokens1: ABCD<strong>E</strong> → [KV1, KV2, KV3, KV4, <strong>KV5</strong>]</p><p>Tokens2: ABCD<strong>F</strong> → [KV1 KV2, KV3, KV4, <strong>KV6</strong>]</p><p><code>kv_cache_store.store(”ABCDE”, [KV1, KV2, KV3, KV4, **KV5**])</code></p><p>When do <code>kv_cache_store.retrieve(”ABCDF”)</code>,</p><p>we expect the matched prefix part <code>[KV1, KV2, KV3, KV4]</code> to be returned</p><p>Trie is a good data structure that supports prefix matching.</p><p>vLLM implements a simplified “hash of chunks” structure to simulate a Trie:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># given tokens</span><br><span class="hljs-string">&quot;ABCDEF&quot;</span><br><span class="hljs-comment"># assume chunk size = 2, chunking:</span><br><span class="hljs-string">&quot;AB&quot;</span>, <span class="hljs-string">&quot;CD&quot;</span>, <span class="hljs-string">&quot;EF&quot;</span><br><span class="hljs-comment"># chunk hashes:</span><br>h1 = <span class="hljs-built_in">hash</span>(<span class="hljs-string">&quot;AB&quot;</span>)<br>h2 = <span class="hljs-built_in">hash</span>(h1 + <span class="hljs-string">&quot;CD&quot;</span>)<br>h3 = <span class="hljs-built_in">hash</span>(h2 + <span class="hljs-string">&quot;EF&quot;</span>)<br><br></code></pre></td></tr></table></figure><p>With prefix matching, store and retrieve become:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># store</span><br><span class="hljs-keyword">for</span> chunk_hash, chunk_kv <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(...)<br>    redis.put(chunk_hash, chunk_kv)<br>    <br>    <br><span class="hljs-comment"># retrieve</span><br><span class="hljs-keyword">for</span> chunk_hash <span class="hljs-keyword">in</span> ...:<br>    kv_chunk  = redis.get(chunk_hash)<br>    <span class="hljs-keyword">if</span> kv_chunk <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">break</span><br>    ...<br><br></code></pre></td></tr></table></figure><p><strong>Chunk size</strong>: tradeoff between matching granularity (hit rate) and management overhead</p><ul><li><p>inside serving engine:</p><ul><li>in vLLM, block size &#x3D; 16, so chunk size is also 16</li><li>SGLang can support chunk size &#x3D; 1</li></ul></li><li><p>outside serving engine (on disk):</p><ul><li>performance determined by # I&#x2F;O</li><li>bad performance for small objects, so we need a large chunk size</li></ul></li></ul><h2 id="Semantic-caching"><a href="#Semantic-caching" class="headerlink" title="Semantic caching"></a><strong>Semantic caching</strong></h2><p><strong>How to deal with semantically similar prefix</strong>: When receiving a request, before sending it to LLM, do a vector similarity search for similar requests. If similar request exists, we can bypass LLM and return the previous response.</p><h1 id="Eviction-policy"><a href="#Eviction-policy" class="headerlink" title="Eviction policy"></a>Eviction policy</h1><p>Which KV cache tensors to evict?</p><p>Inside each request:</p><ul><li>“ABCDEF” → [“AB”, KV1], [”CD”, KV2], [”EF”, KV3]</li><li>Evict from tail to head: KV3, KV2, KV1</li></ul><p>Among different requests: LRU, LFU, …</p><h1 id="vLLM-implementation"><a href="#vLLM-implementation" class="headerlink" title="vLLM implementation"></a>vLLM implementation</h1><h2 id="Retrieve"><a href="#Retrieve" class="headerlink" title="Retrieve"></a>Retrieve</h2><p>pass in requests to the kv_cache_manager</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">computed_blocks, num_computed_tokens = \<br>   <span class="hljs-variable language_">self</span>.kv_cache_manager.get_computed_blocks(request)<br><br></code></pre></td></tr></table></figure><p>get_computed_block() in V1</p><ol><li>chunking tokens to blocks of 16 tokens</li><li>use <code>block_pool.get_cached_block</code> to get computed blocks</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_computed_blocks</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, request: Request</span>) -&gt; <span class="hljs-built_in">tuple</span>[<span class="hljs-built_in">list</span>[KVCacheBlock], <span class="hljs-built_in">int</span>]:<br>    <span class="hljs-string">&quot;&quot;&quot;Get the computed (cached) blocks for the request.</span><br><span class="hljs-string">    Note that the computed blocks must be full.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        request: The request to get the computed blocks.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        A tuple containing:</span><br><span class="hljs-string">            - A list of blocks that are computed for the request.</span><br><span class="hljs-string">            - The number of computed tokens.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.enable_caching:<br>        <span class="hljs-comment"># Prefix caching is disabled.</span><br>        <span class="hljs-keyword">return</span> [], <span class="hljs-number">0</span><br><br>    <span class="hljs-comment"># The block hashes for the request may already be computed</span><br>    <span class="hljs-comment"># if the scheduler has tried to schedule the request before.</span><br>    block_hashes = <span class="hljs-variable language_">self</span>.req_to_block_hashes[request.request_id]<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> block_hashes:<br>        block_hashes = hash_request_tokens(<span class="hljs-variable language_">self</span>.block_size, request)<br>        <span class="hljs-variable language_">self</span>.req_to_block_hashes[request.request_id] = block_hashes<br><br>    <span class="hljs-variable language_">self</span>.prefix_cache_stats.requests += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">if</span> request.sampling_params.prompt_logprobs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># Check for cache hits</span><br>        computed_blocks = []<br>        <span class="hljs-keyword">for</span> block_hash <span class="hljs-keyword">in</span> block_hashes:<br>            <span class="hljs-comment"># block_hashes is a chain of block hashes. If a block hash</span><br>            <span class="hljs-comment"># is not in the cached_block_hash_to_id, the following</span><br>            <span class="hljs-comment"># block hashes are not computed yet for sure.</span><br>            <span class="hljs-keyword">if</span> cached_block := <span class="hljs-variable language_">self</span>.block_pool.get_cached_block(<br>                    block_hash):<br>                computed_blocks.append(cached_block)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">break</span><br><br>        <span class="hljs-variable language_">self</span>.prefix_cache_stats.queries += <span class="hljs-built_in">len</span>(block_hashes)<br>        <span class="hljs-variable language_">self</span>.prefix_cache_stats.hits += <span class="hljs-built_in">len</span>(computed_blocks)<br><br>        <span class="hljs-comment"># NOTE(woosuk): Since incomplete blocks are not eligible for</span><br>        <span class="hljs-comment"># sharing, `num_computed_tokens` is always a multiple of</span><br>        <span class="hljs-comment"># `block_size`.</span><br>        num_computed_tokens = <span class="hljs-built_in">len</span>(computed_blocks) * <span class="hljs-variable language_">self</span>.block_size<br>        <span class="hljs-keyword">return</span> computed_blocks, num_computed_tokens<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># Skip cache hits for prompt logprobs</span><br>        <span class="hljs-keyword">return</span> [], <span class="hljs-number">0</span><br><br><br></code></pre></td></tr></table></figure><h2 id="Store"><a href="#Store" class="headerlink" title="Store"></a>Store</h2><p>Defined in <code>/vllm/v1/core/block_pool.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BlockPool</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;BlockPool that manages KVCacheBlocks.</span><br><span class="hljs-string">    It provides methods to allocate, free and cache the KV cache blocks. The </span><br><span class="hljs-string">    free_block_queue stores the free blocks in eviction order to enable </span><br><span class="hljs-string">    allocation, free, and cache eviction. The cached_block_hash_to_block </span><br><span class="hljs-string">    maps between block hash and cached block to support finding cached blocks </span><br><span class="hljs-string">    by their block hash.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        num_gpu_blocks: The number of blocks in the pool.</span><br><span class="hljs-string">        enable_caching: Whether to enable prefix caching.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br></code></pre></td></tr></table></figure><p>The function is called <code>cache_full_blocks</code></p><p>It caches a list of full blocks for prefix caching.</p><p>This function takes a list of blocks that will have their block hash metadata to be updated and cached.</p><p>Given a request, it computes the block hashes for the blocks starting from <code>num_cached_blocks</code> to <code>num_full_blocks</code>, updating the metadata for each block and caching them in the <code>cached_block_hash_to_block</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cache_full_blocks</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    request: Request,</span><br><span class="hljs-params">    blocks: <span class="hljs-built_in">list</span>[KVCacheBlock],</span><br><span class="hljs-params">    block_hashes: <span class="hljs-built_in">list</span>[BlockHashType],</span><br><span class="hljs-params">    num_cached_blocks: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    num_full_blocks: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    block_size: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Cache a list of full blocks for prefix caching.</span><br><span class="hljs-string">    This function takes a list of blocks that will have their block hash</span><br><span class="hljs-string">    metadata to be updated and cached. Given a request, it computes the</span><br><span class="hljs-string">    block hashes for the blocks starting from `num_cached_blocks` to </span><br><span class="hljs-string">    `num_full_blocks`, updating the metadata for each block</span><br><span class="hljs-string">    and caching them in the `cached_block_hash_to_block`.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        request: The request to cache the blocks.</span><br><span class="hljs-string">        blocks: All blocks in the request.</span><br><span class="hljs-string">        block_hashes: Block hashes of the blocks in the request. Note that</span><br><span class="hljs-string">        this list may be shorter than the blocks list. In this case the </span><br><span class="hljs-string">        missed block hash will be computed in this function.</span><br><span class="hljs-string">        num_cached_blocks: The number of blocks that are already cached.</span><br><span class="hljs-string">        num_full_blocks: The number of blocks that are full and should </span><br><span class="hljs-string">            be cached after this function.</span><br><span class="hljs-string">        block_size: Number of tokens in each block.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> num_cached_blocks == num_full_blocks:<br>        <span class="hljs-keyword">return</span><br>    new_full_blocks = blocks[num_cached_blocks:num_full_blocks]<br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(block_hashes) &gt;= num_cached_blocks<br>    new_block_hashes = block_hashes[num_cached_blocks:]<br><br>    <span class="hljs-comment"># Update the new blocks with the block hashes through the chain.</span><br>    <span class="hljs-keyword">if</span> num_cached_blocks == <span class="hljs-number">0</span>:<br>        prev_block_hash_value = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">else</span>:<br>        prev_block = blocks[num_cached_blocks - <span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">assert</span> prev_block.block_hash <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        prev_block_hash_value = prev_block.block_hash.hash_value<br><br>    <span class="hljs-keyword">for</span> i, blk <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(new_full_blocks):<br>        <span class="hljs-keyword">assert</span> blk.block_hash <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> i &lt; <span class="hljs-built_in">len</span>(new_block_hashes):<br>            <span class="hljs-comment"># The block hash may already be computed in</span><br>            <span class="hljs-comment"># &quot;get_computed_blocks&quot; if the tokens are not generated by</span><br>            <span class="hljs-comment"># this request (either the prompt tokens or the previously</span><br>            <span class="hljs-comment"># generated tokens with preemption). In this case we simply</span><br>            <span class="hljs-comment"># reuse the block hash.</span><br>            block_hash = new_block_hashes[i]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># Otherwise compute the block hash and cache it in the request</span><br>            <span class="hljs-comment"># in case it will be preempted in the future.</span><br>            blk_idx = num_cached_blocks + i<br>            start_token_idx = blk_idx * block_size<br>            end_token_idx = (blk_idx + <span class="hljs-number">1</span>) * block_size<br>            block_tokens = request.all_token_ids[<br>                start_token_idx:end_token_idx]<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(block_tokens) == block_size, (<br>                <span class="hljs-string">f&quot;Expected <span class="hljs-subst">&#123;block_size&#125;</span> tokens, got &quot;</span><br>                <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(block_tokens)&#125;</span> at <span class="hljs-subst">&#123;blk_idx&#125;</span>th block for request &quot;</span><br>                <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;request.request_id&#125;</span>(<span class="hljs-subst">&#123;request&#125;</span>)&quot;</span>)<br><br>            <span class="hljs-comment"># Generate extra keys for multi-modal inputs. Note that since</span><br>            <span class="hljs-comment"># we reach to this branch only when the block is completed with</span><br>            <span class="hljs-comment"># generated tokens, we only need to consider the last mm input.</span><br>            extra_keys, _ = generate_block_hash_extra_keys(<br>                request, start_token_idx, end_token_idx, -<span class="hljs-number">1</span>)<br><br>            <span class="hljs-comment"># Compute the hash of the current block.</span><br>            block_hash = hash_block_tokens(prev_block_hash_value,<br>                                           block_tokens, extra_keys)<br>            block_hashes.append(block_hash)<br><br>        <span class="hljs-comment"># Update and add the full block to the cache.</span><br>        blk.block_hash = block_hash<br>        <span class="hljs-variable language_">self</span>.cached_block_hash_to_block[block_hash][blk.block_id] = blk<br>        prev_block_hash_value = block_hash.hash_value<br><br></code></pre></td></tr></table></figure><h2 id="Eviction"><a href="#Eviction" class="headerlink" title="Eviction"></a>Eviction</h2><p>still in <code>BlockPool</code></p><p>If a block is cached in <code>cached_block_hash_to_block</code>, we reset its hash metadata and evict it from the cache.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_maybe_evict_cached_block</span>(<span class="hljs-params">self, block: KVCacheBlock</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    If a block is cached in `cached_block_hash_to_block`, we reset its hash</span><br><span class="hljs-string">    metadata and evict it from the cache.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        block: The block to evict.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        True if the block is evicted, False otherwise.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    block_hash = block.block_hash<br>    <span class="hljs-keyword">if</span> block_hash <span class="hljs-keyword">and</span> block_hash <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.cached_block_hash_to_block:<br>        block.reset_hash()<br>        <span class="hljs-keyword">del</span> <span class="hljs-variable language_">self</span>.cached_block_hash_to_block[block_hash][block.block_id]<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.cached_block_hash_to_block[block_hash]) == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">del</span> <span class="hljs-variable language_">self</span>.cached_block_hash_to_block[block_hash]<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br><br><br></code></pre></td></tr></table></figure><p>Evictor class <code>FreeKVCacheBlockQueue</code>: use a doubly linked list for LRU eviction</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FreeKVCacheBlockQueue</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;This class organizes a list of KVCacheBlock objects to a doubly linked</span><br><span class="hljs-string">    list of free blocks. We implement this class instead of using Python</span><br><span class="hljs-string">    builtin deque to support removing a block in the middle of the queue</span><br><span class="hljs-string">    in O(1) time. To close the performance gap to the builtin deque which is</span><br><span class="hljs-string">    implemented in C++, this class does not allocate any Python objects when</span><br><span class="hljs-string">    manipulating the linked list. Instead, this class manipulates the </span><br><span class="hljs-string">    prev_free_block and next_free_block attributes of the given blocks.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    The queue is ordered by block ID in the beginning. When a block is allocated</span><br><span class="hljs-string">    and then freed, it will be appended back with the eviction order:</span><br><span class="hljs-string">    1. The least recently used block is at the front (LRU).</span><br><span class="hljs-string">    2. If two blocks have the same last accessed time (allocated by the</span><br><span class="hljs-string">       same sequence), the one with more hash tokens (the tail of a block</span><br><span class="hljs-string">       chain) is at the front.</span><br><span class="hljs-string">    Note that we maintain this order by reversing the block order when free</span><br><span class="hljs-string">    blocks of a request. This operation is outside of this class.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br></code></pre></td></tr></table></figure><p>Source:</p><p><a href="https://www.youtube.com/watch?v=mWvqA_BNtsU&list=PLJj_urhaf2_qxpg8A5-6xoMvMLBKQMTX1&index=18">https://www.youtube.com/watch?v=mWvqA_BNtsU&amp;list&#x3D;PLJj_urhaf2_qxpg8A5-6xoMvMLBKQMTX1&amp;index&#x3D;18</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>vLLM 02 - speculative decoding</title>
    <link href="/2025/04/04/vLLM-02-speculative-decoding/"/>
    <url>/2025/04/04/vLLM-02-speculative-decoding/</url>
    
    <content type="html"><![CDATA[<h1 id="Why-Speculative-Decoding-SD"><a href="#Why-Speculative-Decoding-SD" class="headerlink" title="Why Speculative Decoding (SD)?"></a>Why Speculative Decoding (SD)?</h1><ul><li><p>Decoding is memory-bound: loading KV cache and model takes a long time</p></li><li><p>memory-bound cases: big matrix * small matrix; vector * matrix → O(n^2)</p></li><li><p>compute-bound cases: large matrix * matrix → O(n^3)</p></li><li><p>Find a way to <strong>increase compute</strong> while <strong>not</strong> significantly increasing GPU <strong>memory</strong> access</p></li><li><p><strong>Solution</strong>: Guess multiple tokens and verify</p><ul><li>Example: In terms of token generation per iteration:<ul><li>Guess 3 tokens, acceptance rate 2&#x2F;3</li><li>2 tokens of guessing are correct + LLM inference will generate a new token –&gt; 3 tokens</li></ul></li><li>Iteration time<ul><li>Computation: (1+3)x</li><li>Memory:<ul><li>w&#x2F;o SD: Model parameters (8x2 GB) + KV caches (n * 100 KB)</li><li>w&#x2F; SD: Model parameters (8x2 GB) + KV caches ((n+3) * 100 KB)</li></ul></li><li>Iteration time almost unchanged</li></ul></li></ul></li></ul><h1 id="How-to-guess-tokens"><a href="#How-to-guess-tokens" class="headerlink" title="How to guess tokens?"></a>How to guess tokens?</h1><h2 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h2><ul><li>most-used algorithm in production; simple and effective</li><li>N-grams essentially build a mapping like: if last 3 tokens are A, B, C, next 2 tokens are D, E</li><li>Example with “To be or not to be, this is a question”:<ul><li>[To be or] –&gt; [not to]</li><li>[be or not] –&gt; [to be]</li><li>…</li><li>[that is] –&gt; [a question]</li></ul></li></ul><p>Build N-gram from request input, use this N-gram to guess tokens.</p><ul><li>Example:</li></ul><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs applescript">The followings are Shakespeare&#x27;s <br>famous quotes:<br>.... <br>To be <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">to</span> be, this <span class="hljs-keyword">is</span> a <br>question<br>....<br>What <span class="hljs-keyword">is</span> <span class="hljs-keyword">the</span> best <span class="hljs-literal">quote</span> <span class="hljs-keyword">for</span> <span class="hljs-keyword">my</span> life?<br><br></code></pre></td></tr></table></figure><ul><li>Assume that LLM already generated:</li></ul><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-string">&quot;Sure! We recommend you</span><br>this quote<span class="hljs-punctuation">:</span> &#x27;To be or&#x27;<span class="hljs-string">&quot;</span><br><br></code></pre></td></tr></table></figure><ul><li>[To be or] –&gt; [not to]</li><li>Guess: next two tokens are [not to]</li><li>Verify: yes yes</li></ul><p>Bottleneck of SD: token-guessing algorithm</p><h3 id="Tree-verification"><a href="#Tree-verification" class="headerlink" title="Tree verification"></a>Tree verification</h3><ul><li>N-gram mapping can be one-to-many, because some phrases appear multiple times with different suffixes</li><li>[To be or] –&gt; [not to], [sleep in], [go to]</li><li>[To be or] guess multiple choices: [not to], [sleep in], [go to]</li><li>We need an efficient kernel for tree verification</li></ul><h3 id="How-to-tell-if-the-verification-is-right-or-wrong"><a href="#How-to-tell-if-the-verification-is-right-or-wrong" class="headerlink" title="How to tell if the verification is right or wrong?"></a>How to tell if the verification is right or wrong?</h3><ul><li>Deterministic sampling: bad case for SD, because only one correct answer and the acceptance rate may be low</li><li>Random sampling: correct when guess probability &gt; threshold</li></ul><p>Example: guessed tokens are <strong>right</strong></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs vim">Input: [To <span class="hljs-keyword">be</span> <span class="hljs-built_in">or</span>] (already-decoded <br>output) [not <span class="hljs-keyword">to</span>] (guessed tokens)<br>[To <span class="hljs-keyword">be</span> <span class="hljs-built_in">or</span> not <span class="hljs-keyword">to</span>]<br>[To -&gt; <span class="hljs-keyword">be</span>]<br>[<span class="hljs-keyword">be</span> -&gt; <span class="hljs-built_in">or</span>]<br>[<span class="hljs-built_in">or</span> -&gt; not] our guess <span class="hljs-string">&quot;not&quot;</span> <span class="hljs-keyword">is</span> correct<br>[not -&gt; <span class="hljs-keyword">to</span>] our guess <span class="hljs-string">&quot;to&quot;</span> <span class="hljs-keyword">is</span> correct<br>[<span class="hljs-keyword">to</span> -&gt; <span class="hljs-keyword">be</span>] <span class="hljs-keyword">be</span> <span class="hljs-keyword">is</span> the <span class="hljs-keyword">right</span> <span class="hljs-keyword">next</span> token<br><br></code></pre></td></tr></table></figure><p>Example: guessed tokens are <strong>wrong</strong></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-symbol">Input:</span> [To <span class="hljs-keyword">be </span><span class="hljs-keyword">or] </span>(already-decoded<br>output) [not <span class="hljs-keyword">be] </span>(guessed tokens)<br><span class="hljs-keyword">LLM: </span>[To <span class="hljs-keyword">be </span><span class="hljs-keyword">or </span>not <span class="hljs-keyword">be]</span><br><span class="hljs-keyword"></span>[To -&gt; <span class="hljs-keyword">be]</span><br><span class="hljs-keyword"></span>[<span class="hljs-keyword">be </span>-&gt; <span class="hljs-keyword">or]</span><br><span class="hljs-keyword"></span>[<span class="hljs-keyword">or </span>-&gt; not] our guess <span class="hljs-string">&quot;not&quot;</span> is <br>correct<br>[not -&gt; to] our guess <span class="hljs-string">&quot;be&quot;</span> is wrong, it <span class="hljs-keyword">should </span><span class="hljs-keyword">be </span><span class="hljs-string">&quot;to&quot;</span><br><br></code></pre></td></tr></table></figure><h2 id="Model-based-draft-model"><a href="#Model-based-draft-model" class="headerlink" title="Model-based (draft model)"></a>Model-based (draft model)</h2><ul><li><p>Parallel guessing: guesses next few tokens independently</p><ul><li>Fast but worse performance</li></ul></li><li><p>Autoregressive guessing</p></li></ul><h1 id="Why-deployment-production-is-so-hard"><a href="#Why-deployment-production-is-so-hard" class="headerlink" title="Why deployment&#x2F;production is so hard?"></a><strong>Why deployment&#x2F;production is so hard?</strong></h1><p>Acceptance rate is high enough (&gt; 75%)… so that’s not an issue</p><p><strong>SD is beneficial or not?</strong></p><ul><li>Workload may already be compute-bound</li><li>Batch size small: more memory-bound</li><li>Batch size large: more compute-bound</li><li>SD makes it move from memory-bound to compute-bound.</li><li>In practice, the workload may already be compute-bound, and SD will make it worse</li></ul><p><strong>How many tokens should we guess?</strong> - It should be determined based on arithmetic intensity</p><ul><li>Arithmetic intensity: metric to measure whether it’s compute-bound or memory-bound.</li><li>Arithmetic intensity &#x3D; FLOPs &#x2F; Bytes (or memory bandwidth)</li><li>Every hardware has a suitable intensity</li></ul><p><strong>Other engineering issues</strong></p><ul><li><p>Small model needs KV cache. How can we allocate that?</p><ul><li>If put with vLLM, extra mem</li><li>If not, not a single pool</li></ul></li><li><p>Small model may need different parallel config. For example, model TP&#x3D;8, draft model may need TP&#x3D;2</p><ul><li>Assume small model is no TP, and is on GPU0 + vLLM forces same GPU utilization on different GPUs → memory waste on other GPU memory</li></ul></li><li><p>Pre-allocate KV cache for guessed tokens.</p><ul><li>You break the system assumption that only one token is generated each time. You may need to change the whole vLLM interface</li><li>What if pre-allocated tokens cross vLLM’s block boundary?</li><li>Need to discard wrong tokens</li></ul></li><li><p>Sampling → verification</p></li><li><p>Minimize overhead (ngram) for loop is slow</p></li><li><p>How many # of tokens should we guess</p></li><li><p>How to distinguish between requests</p><ul><li>Different requests: different # of tokens, part of them do not run spec decode</li></ul></li></ul><p><strong>Summary of challenges above:</strong></p><ul><li>When SD is beneficial, how to find the best configuration</li><li>How to detect when SD is not beneficial? How to turn off SD in such cases?</li></ul><p>Paper reading:</p><ul><li>Optimizing Speculative Decoding for Serving Large Language Models Using Goodput</li><li>LLM Inference Unveiled: Survey and Roofline Model Insights</li></ul><p>Source</p><p><a href="https://www.youtube.com/watch?v=WF5xaQqtKUE&list=PLJj_urhaf2_qxpg8A5-6xoMvMLBKQMTX1&index=19">https://www.youtube.com/watch?v=WF5xaQqtKUE&amp;list=PLJj_urhaf2_qxpg8A5-6xoMvMLBKQMTX1&amp;index&#x3D;19</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>vLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vLLM 01 - P/D disaggregation</title>
    <link href="/2025/03/28/vllm-P-D-disaggregation/"/>
    <url>/2025/03/28/vllm-P-D-disaggregation/</url>
    
    <content type="html"><![CDATA[<h1 id="Why-P-D-disaggregation"><a href="#Why-P-D-disaggregation" class="headerlink" title="Why P&#x2F;D disaggregation?"></a>Why P&#x2F;D disaggregation?</h1><ul><li><strong>Initial scheduler logic in vLLM</strong>: prioritize prefill for good throughput</li><li><strong>Problem</strong>: prefill may slow down other requests’ decode</li></ul><h1 id="How-to-mix-P-and-D-together"><a href="#How-to-mix-P-and-D-together" class="headerlink" title="How to mix P and D together?"></a><strong>How to mix P and D together?</strong></h1><ul><li>Well, even their input shapes are different</li><li>Decode is vector*matrix (e.g., Q projection is 1xd * dxd). not many FLOPs + needs to load model weights and KV cache -&gt; it’s memory bound</li><li>Prefill is matrix*matrix (nxd * dxd). model weights are reused -&gt; compute bound</li><li><strong>Solutions</strong>: P&#x2F;D disaggregation, chunked prefill</li></ul><h1 id="Chunked-prefill"><a href="#Chunked-prefill" class="headerlink" title="Chunked prefill"></a>Chunked prefill</h1><h2 id="Motivation-of-chunked-prefill"><a href="#Motivation-of-chunked-prefill" class="headerlink" title="Motivation of chunked prefill"></a><strong>Motivation of chunked prefill</strong></h2><ul><li>Unify prefill and decode procedure: based on some KV cache, P or D does some attention &amp; linear computes, to generate some new tokens</li><li>The compute flow of prefill and decode are the same, and they just have difference in their input and output shapes</li><li>Chunked prefill becomes possible if the kernel can accept different shapes.</li><li>Now the scheduler can make simpler decision: only care about how many tokens to schedule in the current batch</li></ul><h2 id="Chunk-size"><a href="#Chunk-size" class="headerlink" title="Chunk size"></a><strong>Chunk size</strong></h2><ul><li>chunk size is very important</li><li>if chunk size is too large, then a decode can be slow when batching with a prefill -&gt; decode is slowed down by prefill</li><li>if chunk size is too small, then<ol><li>GPU utilization is bad, and FLOPs is low</li><li>it needs many batches to finish the prefill for a long prompt -&gt; prefill is slowed down by decode</li></ol></li></ul><h2 id="When-to-use-chunked-prefill"><a href="#When-to-use-chunked-prefill" class="headerlink" title="When to use chunked prefill"></a><strong>When to use chunked prefill</strong></h2><ul><li><p>prompts are extremely long (e.g., 10k or 100k tokens)</p></li><li><p>why? during attention compute, there will be temp buffers holding QKV, whose memory is proportional to context length. chunked prefill reduces context length, and thus reduces the buffer size</p></li><li><p>want smooth generation, e.g., SLO for p99 inter-token latency</p></li></ul><h1 id="Key-problems-P-D-disaggregation"><a href="#Key-problems-P-D-disaggregation" class="headerlink" title="Key problems P&#x2F;D disaggregation"></a>Key problems P&#x2F;D disaggregation</h1><h2 id="Connector-how-to-transfer-KV-cache"><a href="#Connector-how-to-transfer-KV-cache" class="headerlink" title="Connector: how to transfer KV cache?"></a><strong>Connector: how to transfer KV cache?</strong></h2><ul><li><strong>pooling mode</strong>: shared memory pool. both sender and receiver need high-bandwidth connection to the memory pool</li><li><strong>p2p mode</strong>: sender communicates with receiver directly; better performance; much harder to implement</li><li>Frameworks that support KV cache transfer: LMCache, Mooncake, NIXL</li></ul><p><strong>LMCache</strong> can do KV extraction and transfer</p><ul><li>support both pooling and p2p mode</li><li>current target use cases: prefill-decode disaggregation, KV cache offloading</li></ul><p><strong>Mooncake</strong></p><ul><li>KV cache storage: replicas, RDMA support, etc.</li><li>pooling mode</li></ul><p><strong>NIXL</strong>: p2p mode</p><ul><li>it does support p2p semantics directly. instead, it supports some lower-level data transfer features</li><li>backend is UXL, which is a more general data transfer library than NCCL’s own backend</li></ul><h2 id="How-to-extract-inject-KV-cache-in-vLLM"><a href="#How-to-extract-inject-KV-cache-in-vLLM" class="headerlink" title="How to extract &amp; inject KV cache in vLLM?"></a><strong>How to extract &amp; inject KV cache in vLLM?</strong></h2><p>connector API is called in model_runner</p><p>path: <code>vllm/worker/model_runner.py</code></p><ul><li><p>model runner is used to wrap model forward pass</p></li><li><p>preparing the input for forward</p></li><li><p>post-process forward outputs</p></li><li><p>one major part of model runner is to receive &amp; send KV cache</p></li></ul><p><strong>steps</strong></p><ul><li>before forward, try receiving KV cache and injecting into vLLM’s paged memory</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Receive KV cache in distributed KV cache transfer setting</span><br><span class="hljs-comment"># In disagg prefill setting, it will also recv hidden states and bypass</span><br><span class="hljs-comment"># model forwarding</span><br><span class="hljs-comment"># In KV cache database setting, it will change the model input so that</span><br><span class="hljs-comment"># we can skip prefilling on tokens that successfully received KV caches</span><br><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> The receive operation is blocking</span><br>bypass_model_exec = <span class="hljs-literal">False</span><br><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.need_recv_kv(model_input, kv_caches):<br>    hidden_or_intermediate_states, bypass_model_exec, model_input = \<br>        get_kv_transfer_group().recv_kv_caches_and_hidden_states(<br>            <span class="hljs-comment"># model is used to know which layer the current worker</span><br>            <span class="hljs-comment"># is working on, so that we can receive KV for only those</span><br>            <span class="hljs-comment"># layers.</span><br>            model_executable,<br>             ,<br>            kv_caches=kv_caches<br>        )<br><br></code></pre></td></tr></table></figure><ul><li>after forward, extract KV cache from vLLM’s paged memory, and send it outside</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Sending KV cache in distributed KV cache transfer setting</span><br>  <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> the send operation is non-blocking</span><br>  <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.need_send_kv(model_input, kv_caches):<br>      get_kv_transfer_group().send_kv_caches_and_hidden_states(<br>          <span class="hljs-comment"># model_executable is used to know which layer the current</span><br>          <span class="hljs-comment"># worker is working on, so that we can send KV for only those</span><br>          <span class="hljs-comment"># layers.</span><br>          model_executable,<br>          model_input,<br>          kv_caches,<br>          hidden_or_intermediate_states,<br>      )<br><br></code></pre></td></tr></table></figure><p>How are connector functions implemented?</p><p>check path <code>vllm/distributed/kv_transfer/kv_connector/</code></p><p>There are many possible connectors to use, let’s use <code>SimpleConnector</code> code as an example:</p><p>receive KV cache</p><ul><li>check if  <code>model_input</code>’s tokens exist in the outside world</li><li>if they do exist, we compute where the KV cache should be inserted into vLLM’s page memory (parse page table, use page index to find the right place)</li><li>additionally, it should rebuild the model input to tell the scheduler there is KV cache already, and no prefill should be done again</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">send_kv_caches_and_hidden_states</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    model_executable: torch.nn.Module,</span><br><span class="hljs-params">    model_input: <span class="hljs-string">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span>,</span><br><span class="hljs-params">    kv_caches: <span class="hljs-type">List</span>[torch.Tensor],</span><br><span class="hljs-params">    hidden_or_intermediate_states: <span class="hljs-type">Union</span>[torch.Tensor,</span><br><span class="hljs-params">                                         IntermediateTensors],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br><br>    <span class="hljs-comment"># some initial setup code</span><br>    <span class="hljs-comment"># ...</span><br><br>    <span class="hljs-comment"># query_lens contains new KV caches that are added to vLLM.</span><br>    <span class="hljs-comment"># so we will send them to decode instance</span><br>    <span class="hljs-comment"># FIXME(Kuntai): This assume that all requests are prefill.</span><br>    <span class="hljs-keyword">for</span> idx, slen <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(seq_lens):<br>        start_pos = <span class="hljs-built_in">sum</span>(seq_lens[:idx])<br>        end_pos = start_pos + slen<br><br>        <span class="hljs-keyword">if</span> start_pos &gt;= num_prefill_tokens:<br>            <span class="hljs-comment"># vllm/worker/model_runner.py::_prepare_model_input_tensors:</span><br>            <span class="hljs-comment"># - input_tokens[:num_prefill_tokens] contains prefill tokens.</span><br>            <span class="hljs-comment"># - input_tokens[num_prefill_tokens:] contains decode tokens.</span><br>            logger.warning(<span class="hljs-string">&quot;You have some decode requests while using &quot;</span><br>                           <span class="hljs-string">&quot;SimpleConnector. Their KVCache won&#x27;t be sent.&quot;</span>)<br>            <span class="hljs-keyword">break</span><br><br>        current_tokens = input_tokens_tensor[start_pos:end_pos]<br><br>        keys, values = [], []<br><br>        <span class="hljs-keyword">for</span> layer_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start_layer, end_layer):<br>            kv_cache = kv_caches[layer_id - start_layer]<br><br>            key_cache = kv_cache[<span class="hljs-number">0</span>].reshape(-<span class="hljs-number">1</span>, num_heads, head_size)<br>            value_cache = kv_cache[<span class="hljs-number">1</span>].reshape(-<span class="hljs-number">1</span>, num_heads, head_size)<br><br>            current_slot_mapping = slot_mapping_flat[start_pos:end_pos]<br><br>            keys.append(key_cache[current_slot_mapping].unsqueeze(<span class="hljs-number">0</span>))<br>            values.append(value_cache[current_slot_mapping].unsqueeze(<span class="hljs-number">0</span>))<br><br>        keys = torch.cat(keys, dim=<span class="hljs-number">0</span>)<br>        values = torch.cat(values, dim=<span class="hljs-number">0</span>)<br><br>        <span class="hljs-variable language_">self</span>.insert(current_tokens,<br>                    torch.ones_like(current_tokens,<br>                                    dtype=<span class="hljs-built_in">bool</span>), keys, values,<br>                    hidden_or_intermediate_states[start_pos:end_pos])<br><br>    logger.debug(<span class="hljs-string">&quot;[rank%d]: KV send DONE.&quot;</span>, torch.distributed.get_rank())<br><br></code></pre></td></tr></table></figure><p>Send KV is similar</p><h2 id="When-to-send-requests-to-P-and-D"><a href="#When-to-send-requests-to-P-and-D" class="headerlink" title="When to send requests to P and D?"></a>When to send requests to P and D?</h2><ul><li>First P then D: when P finishes, it will notify the router, and the router will tell D</li><li>First D then P: because D is the process to generate responses, let D be responsible for asking for KV cache from P</li></ul><p>###KV offloading</p><ul><li>Connector can also be used for KV cache offloading</li><li>In such cases, model sharding can be very useful. For example, in GPU-to-CPU KV cache offloading, if TP&#x3D;8, the total bandwidth is 8 * single_gpu_bandwidth.</li></ul><p>Source:</p><p><a href="https://www.youtube.com/watch?v=ih6fcJnhoJI">https://www.youtube.com/watch?v=ih6fcJnhoJI</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>vLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>

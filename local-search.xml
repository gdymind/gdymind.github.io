<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>JAX 101</title>
    <link href="/2025/12/22/jax-101/"/>
    <url>/2025/12/22/jax-101/</url>
    
    <content type="html"><![CDATA[<p>Given the length of the official <a href="https://docs.jax.dev/en/latest/beginner_guide.html#beginner-guide">JAX tutorial</a>, this note distills the core concepts, providing an  quick reference after reading the original tutorial.</p><h1 id="High-level-JAX-stack"><a href="#High-level-JAX-stack" class="headerlink" title="High-level JAX stack"></a>High-level JAX stack</h1><p><img src="/img/jax-101/image.png" alt="image.png"></p><p>Source: <a href="https://www.linkedin.com/posts/yidewang_i-have-always-been-interested-in-explaining-activity-7393113379072094208-W7DV?utm_source=share&utm_medium=member_desktop&rcm=ACoAACRIW9ABACBpoI5NhAW374N0q8ynvZDSacI">Yi Wang’s linkedin post</a></p><h1 id="Quick-start"><a href="#Quick-start" class="headerlink" title="Quick start"></a>Quick start</h1><ul><li><a href="https://docs.jax.dev/en/latest/notebooks/thinking_in_jax.html">https://docs.jax.dev/en/latest/notebooks/thinking_in_jax.html</a></li><li>JAX arrays are <em>immutable</em>.</li><li>device: <code>x.devices()</code>; sharding: <code>x.sharding</code></li><li>JIT function: <code>f_compiled = jit(f)</code><ul><li>Not all JAX code can be JIT compiled, as it requires <em><strong>array</strong></em> <em><strong>shapes</strong></em> to be static &amp; known at compile time.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_negatives</span>(<span class="hljs-params">x</span>):<br>  <span class="hljs-keyword">return</span> x[x &lt; <span class="hljs-number">0</span>]<br>x = jnp.array(np.random.randn(<span class="hljs-number">10</span>))<br>jit(get_negatives)(x)<br><span class="hljs-comment"># NonConcreteBooleanIndexError: Array boolean indices must be concrete; got bool[10]</span><br></code></pre></td></tr></table></figure><ul><li><code>jax.vmap</code>: write your function with a single data point (e.g., one vector, one image). Then <code>jax.vmap</code> transforms it to process the entire batch.<ul><li>can be composed with <code>jax.jit</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> jax<br><span class="hljs-keyword">import</span> jax.numpy <span class="hljs-keyword">as</span> jnp<br><br><span class="hljs-comment"># works on a single vector</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">params, input_vector</span>):<br>  <span class="hljs-keyword">return</span> jnp.dot(params[<span class="hljs-string">&#x27;W&#x27;</span>], input_vector) + params[<span class="hljs-string">&#x27;b&#x27;</span>]<br><br>key = jax.random.key(<span class="hljs-number">0</span>)<br>params = &#123;<br>    <span class="hljs-string">&#x27;W&#x27;</span>: jax.random.normal(key, (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)),<br>    <span class="hljs-string">&#x27;b&#x27;</span>: jax.random.normal(key, (<span class="hljs-number">3</span>,))<br>&#125;<br>single_vector = jnp.ones(<span class="hljs-number">5</span>)<br>result_single = predict(params, single_vector)<br><br>batch_of_vectors = jnp.ones((<span class="hljs-number">8</span>, <span class="hljs-number">5</span>))<br><span class="hljs-comment"># Vectorize predict using vmap:</span><br><span class="hljs-comment"># in_axes=(None, 0):</span><br><span class="hljs-comment">#  - None: Don&#x27;t map over &#x27;params&#x27; (W and b are shared across the batch).</span><br><span class="hljs-comment">#  - 0: Map over the 0-th axis of &#x27;input_vector&#x27; (the batch dimension).</span><br><span class="hljs-comment"># out_axes=0: The output should have the batch dimension at axis 0.</span><br>batched_predict = jax.vmap(predict, in_axes=(<span class="hljs-literal">None</span>, <span class="hljs-number">0</span>), out_axes=<span class="hljs-number">0</span>)<br>results_batch = batched_predict(params, batch_of_vectors)<br><br><span class="hljs-comment"># Expected shape: (batch_size, output_dim) =&gt; (8, 3)</span><br></code></pre></td></tr></table></figure><ul><li>random key: never reuse keys (unless you want identical outputs). In order to generate different and independent samples, you must <code>jax.random.split</code> the key explicitly before passing it to a random function:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):<br>  new_key, subkey = random.split(key)<br>  <span class="hljs-keyword">del</span> key  <span class="hljs-comment"># The old key is consumed by split() -- we must never use it again.</span><br><br>  val = random.normal(subkey)<br>  <span class="hljs-keyword">del</span> subkey  <span class="hljs-comment"># The subkey is consumed by normal().</span><br><br>  <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;draw <span class="hljs-subst">&#123;i&#125;</span>: <span class="hljs-subst">&#123;val&#125;</span>&quot;</span>)<br>  key = new_key  <span class="hljs-comment"># new_key is safe to use in the next iteration.</span><br><br></code></pre></td></tr></table></figure><h1 id="The-Sharp-Bits-aka-common-mistakes"><a href="#The-Sharp-Bits-aka-common-mistakes" class="headerlink" title="The Sharp Bits (aka common mistakes)"></a>The Sharp Bits (aka common mistakes)</h1><p><a href="https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html">https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html</a></p><ul><li>JAX works with pure functions<ul><li>all inputs are in function params, all outputs are function returns</li><li>given same inputs, we always have same outputs (e.g., <code>print</code> is not allowed in the function)</li><li>not recommended to use iterators and any control-flow primitive in <code>jit</code> function → error or unexpected results</li></ul></li><li>in-place updates<ul><li>JAX arrays are immutable. In-place updates like <code>jax_array[1, :] = 1.0</code> yields an error</li><li>use <code>.at</code> instead: <code>updated_array = jax_array.at[1, :].set(1.0)</code>, which is a out-of-place update that returns a new array.<ul><li>similarly, we have other ops like <code>new_jax_array = jax_array.at[::2, 3:].add(7.0)</code></li></ul></li></ul></li><li>out-of-bounds indexing<ul><li>when the indexing op is an array index <em><strong>update</strong></em> (e.g. <code>index_add</code> or <code>scatter</code>-like primitives), updates at out-of-bounds indices will be skipped<ul><li>index_add: <code>jax_array.at[index].add(value)</code> → lowered into a scatter operation on GPU&#x2F;TPU</li><li>scatter primitives: you give indices, and updates, then JAX writes them into an array</li></ul></li><li>when the op is an array index <em><strong>retrieval</strong></em> (e.g. NumPy indexing or <code>gather</code>-like primitives) the index is <em>clamped</em> to the bounds since something must be returned<ul><li>e.g., <code>jnp.arange(10)[11]</code> → the last value will be returned</li></ul></li></ul></li><li>non-jax-array inputs<ul><li>don’t use python list, tuple or other non-array types as inputs for JAX → performance issues</li><li>you can explicitly convert it to a jax array instead: <code>jnp.sum(jnp.array(x))</code></li></ul></li><li>dynamic shapes<ul><li>code used within transforms (like <code>jax.jit</code>, <code>jax.vmap</code>, and <code>jax.grad</code>) requires all output arrays and intermediate arrays to have static shape</li></ul></li><li>JAX by default enforces single-precision numbers (claims <code>dtype=jnp.float64</code> doesn’t work)<ul><li>to use double, add <code>jax_enable_x64</code> configuration variable at <em>startup</em></li></ul></li></ul><h1 id="JAX-101"><a href="#JAX-101" class="headerlink" title="JAX 101"></a>JAX 101</h1><h2 id="1-JIT"><a href="#1-JIT" class="headerlink" title="1. JIT"></a>1. JIT</h2><ul><li>JIT: make python function to be executed efficiently in XLA. recall this part</li></ul><p><img src="/img/jax-101/image%201.png" alt="image.png"></p><ul><li>How? → reducing a function to a sequence of primitives (can be viewed using <code>jax.make_jaxpr()</code>)</li><li>It does not capture the side-effect code (like <code>print()</code>, append the input to an external list, etc.). Don’t JIT-compile impure functions!<ul><li>use <code>jax.debug.print()</code>  to debug printing instead (but performance will be worse)</li></ul></li><li>When tracing, JAX wraps each arg by a tracer object (by default with shape and dtype), which records all JAX ops during the function call.</li><li>The tracer records are used to reconstruct the entire function → output a jaxpr</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> jax <span class="hljs-keyword">import</span> jit<br><span class="hljs-keyword">import</span> jax.numpy <span class="hljs-keyword">as</span> jnp<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-meta">@jit</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x, y</span>):<br>  <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Running f():&quot;</span>)<br>  <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;  x = <span class="hljs-subst">&#123;x&#125;</span>&quot;</span>)<br>  <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;  y = <span class="hljs-subst">&#123;y&#125;</span>&quot;</span>)<br>  result = jnp.dot(x + <span class="hljs-number">1</span>, y + <span class="hljs-number">1</span>)<br>  <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;  result = <span class="hljs-subst">&#123;result&#125;</span>&quot;</span>)<br>  <span class="hljs-keyword">return</span> result<br><br>x = np.random.randn(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br>y = np.random.randn(<span class="hljs-number">4</span>)<br>f(x, y)<br><br><span class="hljs-comment"># Running f():</span><br><span class="hljs-comment">#  x = JitTracer&lt;float32[3,4]&gt; </span><br><span class="hljs-comment">#  y = JitTracer&lt;float32[4]&gt;</span><br><span class="hljs-comment">#  result = JitTracer&lt;float32[3]&gt;</span><br></code></pre></td></tr></table></figure><ul><li><p>If we have a Python conditional, jaxpr only knows about the branch we take.</p></li><li><p>JIT won’t work for the following code</p><ul><li><p>traced values like x and n, can only affect control flow via their static attributes (like shape or dtype), but not via their values</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">g</span>(<span class="hljs-params">x, n</span>):<br>  i = <span class="hljs-number">0</span><br>  <span class="hljs-keyword">while</span> i &lt; n:<br>    i += <span class="hljs-number">1</span><br>  <span class="hljs-keyword">return</span> x + i<br><br>jax.jit(g)(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>)  <span class="hljs-comment"># Raises an error</span><br></code></pre></td></tr></table></figure></li><li><p>we can use special Control flow operators like <code>jax.lax.cond()</code>.</p></li><li><p>or we can JIT-compile only part of the function</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@jax.jit</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loop_body</span>(<span class="hljs-params">prev_i</span>):<br>  <span class="hljs-keyword">return</span> prev_i + <span class="hljs-number">1</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">g_inner_jitted</span>(<span class="hljs-params">x, n</span>):<br>  i = <span class="hljs-number">0</span><br>  <span class="hljs-keyword">while</span> i &lt; n:<br>    i = loop_body(i)<br>  <span class="hljs-keyword">return</span> x + i<br></code></pre></td></tr></table></figure></li><li><p>we can use <code>static_argnums</code> or <code>static_argnames</code>, but have to re-compile the function for every new value of the specified static input</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x</span>):<br>  <span class="hljs-keyword">if</span> x &lt; <span class="hljs-number">3</span>:<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">3.</span> * x ** <span class="hljs-number">2</span><br>  <span class="hljs-keyword">else</span>:<br>    <span class="hljs-keyword">return</span> -<span class="hljs-number">4</span> * x<br><br>f = jit(f, static_argnames=<span class="hljs-string">&#x27;x&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(f(<span class="hljs-number">2.</span>))<br></code></pre></td></tr></table></figure>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> partial<br><br><span class="hljs-meta">@partial(<span class="hljs-params">jit, static_argnums=(<span class="hljs-params"><span class="hljs-number">1</span>,</span>)</span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x, neg</span>):<br>  <span class="hljs-keyword">return</span> -x <span class="hljs-keyword">if</span> neg <span class="hljs-keyword">else</span> x<br><br>f(<span class="hljs-number">1</span>, <span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure></li></ul></li><li><p>JIT caching: when first calling a jit function, the function get compiled and cached.</p></li></ul><h2 id="2-Automatic-vectorization"><a href="#2-Automatic-vectorization" class="headerlink" title="2. Automatic vectorization"></a>2. Automatic vectorization</h2><ul><li><p>suppose we have a single conv function</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">x = jnp.arange(<span class="hljs-number">5</span>)<br>w = jnp.array([<span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convolve</span>(<span class="hljs-params">x, w</span>):<br>  output = []<br>  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(x)-<span class="hljs-number">1</span>):<br>    output.append(jnp.dot(x[i-<span class="hljs-number">1</span>:i+<span class="hljs-number">2</span>], w))<br>  <span class="hljs-keyword">return</span> jnp.array(output)<br><br>convolve(x, w)<br></code></pre></td></tr></table></figure></li><li><p>with <code>jax.vmap()</code>, it can receive batches of inputs (default batch dimension &#x3D; 0)</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">xs = jnp.stack([x, x])<br>ws = jnp.stack([w, w])<br>auto_batch_convolve = jax.vmap(convolve)<br>auto_batch_convolve(xs, ws) <br></code></pre></td></tr></table></figure></li><li><p>we can use <code>in_axes</code> and <code>out_axes</code> to specify batch dimensions</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">auto_batch_convolve_v2 = jax.vmap(convolve, in_axes=<span class="hljs-number">1</span>, out_axes=<span class="hljs-number">1</span>)<br>xst = jnp.transpose(xs)<br>wst = jnp.transpose(ws)<br>auto_batch_convolve_v2(xst, wst)<br></code></pre></td></tr></table></figure></li><li><p>example: convolve to a single set of weights <code>w</code> with a batch of vectors <code>x</code></p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_convolve_v3 = jax.vmap(convolve, in_axes=[<span class="hljs-number">0</span>, <span class="hljs-literal">None</span>])<br>batch_convolve_v3(xs, w)<br></code></pre></td></tr></table></figure></li><li><p><code>jax.vmap</code> can be combined with <code>jax.jit</code></p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">jitted_batch_convolve = jax.jit(auto_batch_convolve)<br></code></pre></td></tr></table></figure></li></ul><h2 id="3-Pytrees"><a href="#3-Pytrees" class="headerlink" title="3. Pytrees"></a>3. Pytrees</h2><h3 id="3-1-basics"><a href="#3-1-basics" class="headerlink" title="3.1 basics"></a>3.1 basics</h3><ul><li><p>a Pytree is simply a container of arrays. It can be a list, a tuple, a dict, or a nested combination of these. JAX functions are designed to accept these structures as inputs and return them as outputs smoothly.</p></li><li><p>a leaf is anything that’s not a pytree, such as an array, but a single leaf is also a pytree</p></li><li><p>pytree: any tree-like structure built out of container-like Python objects</p><ul><li>container-like: in pytree container registry; defaults are lists, tuples, and dicts</li><li>object whose type is not in the pytree container registry → leaf node</li></ul></li><li><p>in ML, a pytree can contain: model weights, dataset entries, RL observations, etc.</p></li><li><p><code>jax.tree.leaves()</code>: flattened leaves from the trees</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">example_trees = [<br>    [<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-built_in">object</span>()],<br>    (<span class="hljs-number">1</span>, (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), ()),<br>    [<span class="hljs-number">1</span>, &#123;<span class="hljs-string">&#x27;k1&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;k2&#x27;</span>: (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)&#125;, <span class="hljs-number">5</span>],<br>    &#123;<span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;b&#x27;</span>: (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)&#125;,<br>    jnp.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]),<br>]<br><span class="hljs-keyword">for</span> pytree <span class="hljs-keyword">in</span> example_trees:<br>  leaves = jax.tree.leaves(pytree)<br>  <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-built_in">repr</span>(pytree):&lt;<span class="hljs-number">45</span>&#125;</span> has <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(leaves)&#125;</span> leaves: <span class="hljs-subst">&#123;leaves&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;a&#x27;</span>, &lt;<span class="hljs-built_in">object</span> <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7d8bcaf08490</span>&gt;]   has <span class="hljs-number">3</span> leaves: [<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;a&#x27;</span>, &lt;<span class="hljs-built_in">object</span> <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7d8bcaf08490</span>&gt;]<br>(<span class="hljs-number">1</span>, (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), ())                               has <span class="hljs-number">3</span> leaves: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]<br>[<span class="hljs-number">1</span>, &#123;<span class="hljs-string">&#x27;k1&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;k2&#x27;</span>: (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)&#125;, <span class="hljs-number">5</span>]               has <span class="hljs-number">5</span> leaves: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]<br>&#123;<span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;b&#x27;</span>: (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)&#125;                         has <span class="hljs-number">3</span> leaves: [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]<br>Array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=int32)                 has <span class="hljs-number">1</span> leaves: [Array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=int32)]<br></code></pre></td></tr></table></figure></li><li><p><code>jax.tree.map(f, tree)</code>: apply a function to every <em>leaf</em> in a nested data structure</p><p>  <a href="https://www.notion.so/jax-tree-map-2b429d0d4b8f80799adafc9f0a17b7a5?pvs=21">jax.tree.map()</a></p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">list_of_lists = [<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]<br>]<br>jax.tree.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: x*<span class="hljs-number">2</span>, list_of_lists)<br><span class="hljs-comment"># [[2, 4, 6], [2, 4], [2, 4, 6, 8]]</span><br></code></pre></td></tr></table></figure>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">another_list_of_lists = list_of_lists<br>jax.tree.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x, y: x+y, list_of_lists, another_list_of_lists)<br></code></pre></td></tr></table></figure></li><li><p>view pytree definition</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> jax.tree_util <span class="hljs-keyword">import</span> tree_structure<br><span class="hljs-built_in">print</span>(tree_structure(<span class="hljs-built_in">object</span>))<br><span class="hljs-comment"># PyTreeDef(*)</span><br></code></pre></td></tr></table></figure></li><li><p>pytree and JAX transformations</p><ul><li>many JAX functions operates over pytrees of arrays</li><li>all JAX function transformations can be applied to functions whose inputs and outputs are pytrees of arrays</li></ul></li><li><p><code>in_axes</code> and <code>out_axes</code> arguments to <code>jax.vmap()</code> can also be pytrees (structure must match the corresponding arguments)</p><ul><li><p>example: function <code>f</code> has has an input like <code>(a1, &#123;&quot;k1&quot;: a2, &quot;k2&quot;: a3&#125;)</code></p><ul><li>a variable <code>a1</code></li><li>a dict containing keys <code>k1</code> (value <code>a2</code>) and <code>k2</code> (value <code>a3</code>)</li></ul></li><li><p>we can use <code>jax.vmap(f, in_axes=(None, &#123;&quot;k1&quot;: None, &quot;k2&quot;: 0&#125;))</code></p><ul><li>you want to vectorize (loop over) the data in <code>k2</code>, but not <code>a1</code> or <code>k1</code> (they are constants&#x2F;broadcasted)</li></ul></li><li><p>a single leaf value can be applied to an entire sub-pytree</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">jax.vmap(f, in_axes=(<span class="hljs-literal">None</span>, <span class="hljs-number">0</span>))<br><span class="hljs-comment"># equivalent to (None, &#123;&quot;k1&quot;: 0, &quot;k2&quot;: 0&#125;)</span><br></code></pre></td></tr></table></figure>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">jax.vmap(f, in_axes=<span class="hljs-number">0</span>)  <span class="hljs-comment"># equivalent to (0, &#123;&quot;k1&quot;: 0, &quot;k2&quot;: 0&#125;)</span><br></code></pre></td></tr></table></figure></li></ul></li></ul><h3 id="3-2-key-paths"><a href="#3-2-key-paths" class="headerlink" title="3.2 key paths"></a>3.2 key paths</h3><ul><li><p>each leaf has a key path (i.e., a list of keys, list length &#x3D; tree depth of the leaf)</p></li><li><p>key type depends on node type; e.g., key type for dicts is different from key type for tuples</p><ul><li><code>SequenceKey(idx: int)</code>: For lists and tuples.<ul><li>this key type is used for nodes that are ordered sequences and accessed by an integer index.</li><li>key content: An integer index specifying the position.</li></ul></li><li><code>DictKey(key: Hashable)</code>: For dictionaries.</li><li><code>GetAttrKey(name: str)</code>: For namedtuples and preferably custom pytree nodes (more in the next section)</li></ul></li><li><p><code>jax.tree_util.tree_flatten_with_path()</code>: similar to <code>jax.tree.flatten()</code>, but returns flattened key paths</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">example_trees = [<br>    [<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-built_in">object</span>()],<br>    (<span class="hljs-number">1</span>, (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), ()),<br>    jnp.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]),<br>]<br>jax.tree_util.tree_flatten_with_path(example_trees)<br><span class="hljs-comment"># ([((SequenceKey(idx=0), SequenceKey(idx=0)), 1),</span><br><span class="hljs-comment">#  ((SequenceKey(idx=0), SequenceKey(idx=1)), &#x27;a&#x27;),</span><br><span class="hljs-comment">#  ((SequenceKey(idx=0), SequenceKey(idx=2)), &lt;object at 0x70d4e8c6b600&gt;),</span><br><span class="hljs-comment">#  ((SequenceKey(idx=1), SequenceKey(idx=0)), 1),</span><br><span class="hljs-comment">#  ((SequenceKey(idx=1), SequenceKey(idx=1), SequenceKey(idx=0)), 2),</span><br><span class="hljs-comment">#  ((SequenceKey(idx=1), SequenceKey(idx=1), SequenceKey(idx=1)), 3),</span><br><span class="hljs-comment">#  ((SequenceKey(idx=2),), Array([1, 2, 3], dtype=int32))],</span><br><span class="hljs-comment"># PyTreeDef([[*, *, *], (*, (*, *), ()), *]))</span><br></code></pre></td></tr></table></figure></li><li><p><code>jax.tree_util.tree_map_with_path()</code>: similar to <code>jax.tree.map()</code>, but the inputs are also key paths</p></li><li><p><code>jax.tree_util.keystr()</code>: given a key path, returns a reader-friendly string</p></li><li><p>example</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> collections<br><br>ATuple = collections.namedtuple(<span class="hljs-string">&quot;ATuple&quot;</span>, (<span class="hljs-string">&#x27;name&#x27;</span>))<br><br>tree = [<span class="hljs-number">1</span>, &#123;<span class="hljs-string">&#x27;k1&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;k2&#x27;</span>: (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)&#125;, ATuple(<span class="hljs-string">&#x27;foo&#x27;</span>)]<br>flattened, _ = jax.tree_util.tree_flatten_with_path(tree)<br><br><span class="hljs-keyword">for</span> key_path, value <span class="hljs-keyword">in</span> flattened:<br>  <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Value of tree<span class="hljs-subst">&#123;jax.tree_util.keystr(key_path)&#125;</span>: <span class="hljs-subst">&#123;value&#125;</span>&#x27;</span>)<br>  <br><span class="hljs-comment"># Value of tree[0]: 1</span><br><span class="hljs-comment"># Value of tree[1][&#x27;k1&#x27;]: 2</span><br><span class="hljs-comment"># Value of tree[1][&#x27;k2&#x27;][0]: 3</span><br><span class="hljs-comment"># Value of tree[1][&#x27;k2&#x27;][1]: 4</span><br><span class="hljs-comment"># Value of tree[2].name: foo</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="3-3-common-mistakes"><a href="#3-3-common-mistakes" class="headerlink" title="3.3 common mistakes"></a>3.3 common mistakes</h3><ul><li><p>accidentally introducing tree nodes instead of leaves</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">a_tree = [jnp.zeros((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)), jnp.zeros((<span class="hljs-number">3</span>, <span class="hljs-number">4</span>))]<br><br><span class="hljs-comment"># Try to make another pytree with ones instead of zeros.</span><br>shapes = jax.tree.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: x.shape, a_tree)<br>jax.tree.<span class="hljs-built_in">map</span>(jnp.ones, shapes)<br><span class="hljs-comment"># [(Array([1., 1.], dtype=float32), Array([1., 1., 1.], dtype=float32)),</span><br><span class="hljs-comment"># (Array([1., 1., 1.], dtype=float32), Array([1., 1., 1., 1.], dtype=float32))]</span><br></code></pre></td></tr></table></figure><ul><li><code>shape</code> of an array is a tuple, which is a pytree node, with its elements as leaves.</li><li>the map instead of calling <code>jnp.ones</code> on e.g. <code>(2, 3)</code>, it’s called on <code>2</code> and <code>3</code>.</li></ul></li><li><p><code>jax.tree_util</code> functions treat <code>None</code> as the absence of a pytree node, not as a leaf</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">jax.tree.leaves([<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>])<br><span class="hljs-comment"># []</span><br><br><span class="hljs-comment"># fix</span><br>jax.tree.leaves([<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>], is_leaf=<span class="hljs-keyword">lambda</span> x: x <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure></li></ul><h3 id="3-4-common-pytree-patterns"><a href="#3-4-common-pytree-patterns" class="headerlink" title="3.4 common pytree patterns"></a>3.4 common pytree patterns</h3><ul><li>transposing pytrees<ul><li><p>transpose: turn a list of trees into a tree of lists</p></li><li><p>option1 (basic): <code>jax.tree.map()</code></p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">tree_transpose</span>(<span class="hljs-params">list_of_trees</span>):<br>  <span class="hljs-keyword">return</span> jax.tree.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> *xs: <span class="hljs-built_in">list</span>(xs), *list_of_trees)<br><br><span class="hljs-comment"># Convert a dataset from row-major to column-major.</span><br>episode_steps = [<span class="hljs-built_in">dict</span>(t=<span class="hljs-number">1</span>, obs=<span class="hljs-number">3</span>), <span class="hljs-built_in">dict</span>(t=<span class="hljs-number">2</span>, obs=<span class="hljs-number">4</span>)]<br>tree_transpose(episode_steps)<br><span class="hljs-comment"># *xs are tuples &#123;&#x27;obs&#x27;: (3, 4), &#x27;t&#x27;: (1, 2)&#125;</span><br><span class="hljs-comment"># with lambda, they are converted to lists &#123;&#x27;obs&#x27;: [3, 4], &#x27;t&#x27;: [1, 2]&#125;</span><br></code></pre></td></tr></table></figure></li><li><p>option2 (comple but flexible): <code>jax.tree.transpose()</code></p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">jax.tree.transpose(<br>  outer_treedef = jax.tree.structure([<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> episode_steps]),<br>  inner_treedef = jax.tree.structure(episode_steps[<span class="hljs-number">0</span>]),<br>  pytree_to_transpose = episode_steps<br>)<br></code></pre></td></tr></table></figure></li></ul></li></ul><h2 id="4-Intro-to-parallel-programming"><a href="#4-Intro-to-parallel-programming" class="headerlink" title="4. Intro to parallel programming"></a>4. Intro to parallel programming</h2><h3 id="4-1-Basics"><a href="#4-1-Basics" class="headerlink" title="4.1 Basics"></a>4.1 Basics</h3><ul><li><p>this tutorial is for SPMD program: same computation (e.g., forward pass), different data (e.g., different inputs in a batch) in parallel on different devices (e.g., TPUs)</p></li><li><p>we cover auto sharding (<code>jax.jit()</code>), explicit sharding, and fully manual sharding (<code>jax.shard_map()</code>, per-device sharding + explicit collectives)</p></li><li><p><code>jax.Array</code> is designed with distributed data and computation in mind. It has  <code>jax.sharding.Sharding</code> object.</p></li><li><p>by default, arrays are on a single device</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> jax<br>jax.config.update(<span class="hljs-string">&#x27;jax_num_cpu_devices&#x27;</span>, <span class="hljs-number">8</span>)<br>arr = jnp.arange(<span class="hljs-number">32.0</span>).reshape(<span class="hljs-number">4</span>, <span class="hljs-number">8</span>)<br>arr.devices()<br><span class="hljs-comment"># &#123;CpuDevice(id=0)&#125;</span><br></code></pre></td></tr></table></figure></li><li><p>visualize sharding: <code>jax.debug.visualize_array_sharding()</code></p></li><li><p>define sharding</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> jax.sharding <span class="hljs-keyword">import</span> PartitionSpec <span class="hljs-keyword">as</span> P<br><span class="hljs-comment"># 8 devices (2x4), 1st dimesion called &#x27;x&#x27;, 2nd &#x27;y&#x27;</span><br>mesh = jax.make_mesh((<span class="hljs-number">2</span>, <span class="hljs-number">4</span>), (<span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>))<br><span class="hljs-comment"># array row mapped to x-axis, col to y-axis</span><br>sharding = jax.sharding.NamedSharding(mesh, P(<span class="hljs-string">&#x27;x&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>))<br>arr_sharded = jax.device_put(arr, sharding)<br></code></pre></td></tr></table></figure><h3 id="4-2-auto-sharding-via-jax-jit"><a href="#4-2-auto-sharding-via-jax-jit" class="headerlink" title="4.2 auto sharding via jax.jit()"></a>4.2 auto sharding via <code>jax.jit()</code></h3><ul><li>you specify how inputs&#x2F;outputs are sharded, <code>jax.jit()</code> will automatically<ul><li>partition everything inside</li><li>compile inter-device communications</li></ul></li><li>example with element-wise function: same sharding as inputs</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@jax.jit</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f_elementwise</span>(<span class="hljs-params">x</span>):<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * jnp.sin(x) + <span class="hljs-number">1</span><br>result = f_elementwise(arr_sharded)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;shardings match:&quot;</span>, result.sharding == arr_sharded.sharding)<br><span class="hljs-comment"># True</span><br></code></pre></td></tr></table></figure><ul><li>example with reduce</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@jax.jit</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f_contract</span>(<span class="hljs-params">x</span>):<br>  <span class="hljs-keyword">return</span> x.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>)<br>result = f_contract(arr_sharded)<br>jax.debug.visualize_array_sharding(result)<br><span class="hljs-comment"># ┌───────┬───────┬───────┬───────┐</span><br><span class="hljs-comment"># │CPU 0,4│CPU 1,5│CPU 2,6│CPU 3,7│</span><br><span class="hljs-comment"># └───────┴───────┴───────┴───────┘</span><br></code></pre></td></tr></table></figure><h3 id="4-3-Explicit-sharding-sharding-in-types"><a href="#4-3-Explicit-sharding-sharding-in-types" class="headerlink" title="4.3 Explicit sharding (sharding-in-types)"></a>4.3 <strong>Explicit sharding (sharding-in-types)</strong></h3><ul><li>the JAX-level <em>type</em> of a value includes how the value is sharded</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">some_array = np.arange(<span class="hljs-number">8</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;JAX-level type of some_array: <span class="hljs-subst">&#123;jax.typeof(some_array)&#125;</span>&quot;</span>)<br><span class="hljs-comment"># JAX-level type of some_array: int32[8]</span><br></code></pre></td></tr></table></figure><ul><li>we can query the type in <code>jit</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@jax.jit</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">foo</span>(<span class="hljs-params">x</span>):<br>  <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;jax.typeof(x)&#125;</span>&quot;</span>)<br>  <span class="hljs-keyword">return</span> x + x<br></code></pre></td></tr></table></figure><ul><li>set up an explicit-sharding mesh</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> jax.sharding <span class="hljs-keyword">import</span> AxisType<br>mesh = jax.make_mesh((<span class="hljs-number">2</span>, <span class="hljs-number">4</span>), (<span class="hljs-string">&quot;X&quot;</span>, <span class="hljs-string">&quot;Y&quot;</span>),<br>                     axis_types=(AxisType.Explicit, AxisType.Explicit))<br>replicated_array = np.arange(<span class="hljs-number">8</span>).reshape(<span class="hljs-number">4</span>, <span class="hljs-number">2</span>)<br>sharded_array = jax.device_put(replicated_array,<br>jax.NamedSharding(mesh, P(<span class="hljs-string">&quot;X&quot;</span>, <span class="hljs-literal">None</span>)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;replicated_array type: <span class="hljs-subst">&#123;jax.typeof(replicated_array)&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;sharded_array type: <span class="hljs-subst">&#123;jax.typeof(sharded_array)&#125;</span>&quot;</span>)<br><span class="hljs-comment"># replicated_array type: int32[4,2]</span><br><span class="hljs-comment"># sharded_array type: int32[4@X,2]</span><br><span class="hljs-comment">#   -&gt; 1st dimesion sharded along X, others are replicated</span><br></code></pre></td></tr></table></figure><h3 id="4-4-Manual-parallelism-with-shard-map"><a href="#4-4-Manual-parallelism-with-shard-map" class="headerlink" title="4.4 Manual parallelism with shard_map"></a>4.4 Manual parallelism with shard_map</h3><ul><li><p>you write the function for a single shard, and <code>jax.shard_map()</code> constructs the full function</p></li><li><p><code>shard_map()</code> maps over shards</p><ul><li><code>in_specs</code> determines the shard sizes<ul><li>split 32 elements in <code>arr</code> to 8 devices (4 elements &#x2F; device)</li></ul></li><li><code>out_specs</code> identifies how blocks are assembled back together<ul><li>the outputs coming from the devices represent chunks split along axis <code>&#39;x&#39;</code></li><li>it must concatenate (glue) these 8 chunks back together in order to form the final global array</li></ul></li></ul>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">mesh = jax.make_mesh((<span class="hljs-number">8</span>,), (<span class="hljs-string">&#x27;x&#x27;</span>,))<br><br><span class="hljs-meta">@jax.jit</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f_elementwise</span>(<span class="hljs-params">x</span>):<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * jnp.sin(x) + <span class="hljs-number">1</span><br><br>f_elementwise_sharded = jax.shard_map(<br>    f_elementwise,<br>    mesh=mesh,<br>    in_specs=P(<span class="hljs-string">&#x27;x&#x27;</span>),<br>    out_specs=P(<span class="hljs-string">&#x27;x&#x27;</span>))<br><br>arr = jnp.arange(<span class="hljs-number">32</span>)<br>f_elementwise_sharded(arr)<br></code></pre></td></tr></table></figure></li><li><p>the function can only see a single shard</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">x = jnp.arange(<span class="hljs-number">32</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;global shape: <span class="hljs-subst">&#123;x.shape=&#125;</span>&quot;</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x</span>):<br>  <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;device local shape: <span class="hljs-subst">&#123;x.shape=&#125;</span>&quot;</span>)<br>  <span class="hljs-keyword">return</span> x * <span class="hljs-number">2</span><br>y = jax.shard_map(f, mesh=mesh, in_specs=P(<span class="hljs-string">&#x27;x&#x27;</span>), out_specs=P(<span class="hljs-string">&#x27;x&#x27;</span>))(x)<br><span class="hljs-comment"># global shape: x.shape=(32,)</span><br><span class="hljs-comment"># device local shape: x.shape=(4,)</span><br></code></pre></td></tr></table></figure></li><li><p>be careful about aggregation-like functions</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x</span>):<br>  <span class="hljs-keyword">return</span> jnp.<span class="hljs-built_in">sum</span>(x, keepdims=<span class="hljs-literal">True</span>)<br>jax.shard_map(f, mesh=mesh, in_specs=P(<span class="hljs-string">&#x27;x&#x27;</span>), out_specs=P(<span class="hljs-string">&#x27;x&#x27;</span>))(x)<br><span class="hljs-comment"># Array([  6,  22,  38,  54,  70,  86, 102, 118], dtype=int32)</span><br></code></pre></td></tr></table></figure><ul><li>you need <code>jax.lax.psum()</code> to get a single sum</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x</span>):<br>  sum_in_shard = x.<span class="hljs-built_in">sum</span>()<br>  <span class="hljs-keyword">return</span> jax.lax.psum(sum_in_shard, <span class="hljs-string">&#x27;x&#x27;</span>)<br>jax.shard_map(f, mesh=mesh, in_specs=P(<span class="hljs-string">&#x27;x&#x27;</span>), out_specs=P())(x)<br><span class="hljs-comment"># Array(496, dtype=int32)</span><br></code></pre></td></tr></table></figure><h3 id="4-5-Data-computation-placement-on-devices"><a href="#4-5-Data-computation-placement-on-devices" class="headerlink" title="4.5 Data &#x2F; computation placement on devices"></a>4.5 Data &#x2F; computation placement on devices</h3><ul><li>In JAX, the computation follows data placement</li><li>2 placement properties<ul><li>the device where the data resides</li><li>whether it is <strong>committed</strong> to the device or not</li><li>default: uncommitted on <code>jax.devices()[0]</code></li></ul></li><li>data with <code>jax.device_put()</code> becomes <strong>committed</strong> to the device. example: <code>arr = device_put(1, jax.devices()[2])</code></li></ul><h2 id="5-Structured-control-flow-with-JIT"><a href="#5-Structured-control-flow-with-JIT" class="headerlink" title="5. Structured control flow  with JIT"></a>5. Structured control flow  with JIT</h2><p>if we want to use control flow that’s traceable, and that avoids un-rolling large loops, there are 4 structured control flow:</p><p><code>lax.cond()</code></p><p>semantics:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cond</span>(<span class="hljs-params">pred, true_fun, false_fun, operand</span>):<br>  <span class="hljs-keyword">if</span> pred:<br>    <span class="hljs-keyword">return</span> true_fun(operand)<br>  <span class="hljs-keyword">else</span>:<br>    <span class="hljs-keyword">return</span> false_fun(operand)<br></code></pre></td></tr></table></figure><p>example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> jax <span class="hljs-keyword">import</span> lax<br><br>operand = jnp.array([<span class="hljs-number">0.</span>])<br>lax.cond(<span class="hljs-literal">True</span>, <span class="hljs-keyword">lambda</span> x: x+<span class="hljs-number">1</span>, <span class="hljs-keyword">lambda</span> x: x-<span class="hljs-number">1</span>, operand)<br><span class="hljs-comment"># --&gt; array([1.], dtype=float32)</span><br>lax.cond(<span class="hljs-literal">False</span>, <span class="hljs-keyword">lambda</span> x: x+<span class="hljs-number">1</span>, <span class="hljs-keyword">lambda</span> x: x-<span class="hljs-number">1</span>, operand)<br><span class="hljs-comment"># --&gt; array([-1.], dtype=float32)</span><br><br></code></pre></td></tr></table></figure><p><code>lax.while_loop()</code></p><p>semantics:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">while_loop</span>(<span class="hljs-params">cond_fun, body_fun, init_val</span>):<br>  val = init_val<br>  <span class="hljs-keyword">while</span> cond_fun(val):<br>    val = body_fun(val)<br>  <span class="hljs-keyword">return</span> val<br></code></pre></td></tr></table></figure><p>example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">init_val = <span class="hljs-number">0</span><br>cond_fun = <span class="hljs-keyword">lambda</span> x: x &lt; <span class="hljs-number">10</span><br>body_fun = <span class="hljs-keyword">lambda</span> x: x+<span class="hljs-number">1</span><br>lax.while_loop(cond_fun, body_fun, init_val)<br><span class="hljs-comment"># --&gt; array(10, dtype=int32)</span><br></code></pre></td></tr></table></figure><p><code>lax.fori_loop()</code></p><p>semantics:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">fori_loop</span>(<span class="hljs-params">start, stop, body_fun, init_val</span>):<br>  val = init_val<br>  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start, stop):<br>    val = body_fun(i, val)<br>  <span class="hljs-keyword">return</span> val<br></code></pre></td></tr></table></figure><p>example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">init_val = <span class="hljs-number">0</span><br>start = <span class="hljs-number">0</span><br>stop = <span class="hljs-number">10</span><br>body_fun = <span class="hljs-keyword">lambda</span> i,x: x+i<br>lax.fori_loop(start, stop, body_fun, init_val)<br><span class="hljs-comment"># --&gt; array(45, dtype=int32)</span><br></code></pre></td></tr></table></figure><p><img src="/img/jax-101/image%202.png" alt="image.png"></p><h2 id="6-Static-vs-traced-ops"><a href="#6-Static-vs-traced-ops" class="headerlink" title="6. Static vs traced ops"></a>6. Static vs traced ops</h2><ul><li>the following will lead to an error, because when we use <code>jnp.array</code> and <code>jnp.prod</code> on the static value <code>x.shape</code>, it becomes a traced value</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> jax.numpy <span class="hljs-keyword">as</span> jnp<br><span class="hljs-keyword">from</span> jax <span class="hljs-keyword">import</span> jit<br><br><span class="hljs-meta">@jit</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x</span>):<br>  <span class="hljs-keyword">return</span> x.reshape(jnp.array(x.shape).prod())<br><br>x = jnp.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<br>f(x)<br></code></pre></td></tr></table></figure><ul><li>use <code>numpy</code> for operations that should be static (i.e. done at compile-time)</li><li>use <code>jax.numpy</code> for operations that should be traced (i.e. compiled and executed at run-time)</li><li>fix:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> jax <span class="hljs-keyword">import</span> jit<br><span class="hljs-keyword">import</span> jax.numpy <span class="hljs-keyword">as</span> jnp<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-meta">@jit</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x</span>):<br>  <span class="hljs-comment"># np.prod will get 6 during compile-time</span><br>  <span class="hljs-comment"># while jnp.prod is like &quot;I will calculate this product later on the GPU&quot;</span><br>  <span class="hljs-keyword">return</span> x.reshape((np.prod(x.shape),))<br>  <br><br>x = jnp.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<br>f(x)<br></code></pre></td></tr></table></figure><h2 id="7-Stateful-computation"><a href="#7-Stateful-computation" class="headerlink" title="7. Stateful computation"></a>7. Stateful computation</h2><ul><li>JAX transformations (like <code>jit()</code>, <code>vmap()</code>, and <code>grad()</code>) replies on pure functions</li><li>how to make stateful computations? → add explicit state as arg</li><li>suppose we have</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> jax<br><span class="hljs-keyword">import</span> jax.numpy <span class="hljs-keyword">as</span> jnp<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Counter</span>:<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-variable language_">self</span>.n = <span class="hljs-number">0</span><br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">count</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">int</span>:<br>    <span class="hljs-variable language_">self</span>.n += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.n<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-variable language_">self</span>.n = <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><ul><li>to use it with <code>jax.jit()</code>, we can do</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">CounterState = <span class="hljs-built_in">int</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CounterV2</span>:<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">count</span>(<span class="hljs-params">self, n: CounterState</span>) -&gt; <span class="hljs-built_in">tuple</span>[<span class="hljs-built_in">int</span>, CounterState]:<br>    <span class="hljs-comment"># You could just return n+1, but here we separate its role as </span><br>    <span class="hljs-comment"># the output and as the counter state for didactic purposes.</span><br>    <span class="hljs-keyword">return</span> n+<span class="hljs-number">1</span>, n+<span class="hljs-number">1</span><br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset</span>(<span class="hljs-params">self</span>) -&gt; CounterState:<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br><br>counter = CounterV2()<br>state = counter.reset()<br><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):<br>  value, state = counter.count(state)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>JAX</tag>
      
      <tag>TPU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jeff Dean &amp; Gemini team QA at NeurIPS ‘25</title>
    <link href="/2025/12/05/Jeff-Dean-at-NeurIPS-2025/"/>
    <url>/2025/12/05/Jeff-Dean-at-NeurIPS-2025/</url>
    
    <content type="html"><![CDATA[<ul><li><strong>Q1</strong>: are we <strong>running out of pretraining data</strong>? are we hitting the <strong>scaling law</strong> <strong>wall</strong>?<ul><li>I <strong>don’t quite buy</strong> it. Gemini only use a portion of the video data to train.</li><li>We spent plenty of time on <strong>filtering the right data</strong>. For example, data generated by less-powerful models are put on the internet, which can hurt model performance. There are  efforts on automating the data filtering process.</li><li>We can track if some data is generated by AI by <strong>watermarking</strong>, but there are so many models with or without watermarking.</li></ul></li><li><strong>Q2</strong>: comments on <strong>pre-training data mixture</strong>, like healthcare and <strong>multi-language</strong> <strong>support</strong>.<ul><li><strong>Pretraining is like a zero-sum game</strong>, and needs to balance different data types and sources.</li><li>Multi-language support: since pretraining is zero-sum, we may <strong>add more modules in parameter space</strong> rather than tune the base model for multiple languages at the same time. By utilizing modules, we could bring more training data flops to the base model.</li></ul></li><li><strong>Q3</strong>: Comments on <strong>LLM benchmarks &#x2F; evaluations</strong>?<ul><li>Models may learn from public benchmarks, so the time window for a benchmark being useful is kind of limited.</li><li>Besides public benchmarks, Gemini also has <strong>internal benchmarks</strong> that are revised iteration by iteration.</li><li>When we see Gemini get almost 0% score on a benchmark, it’s probably not easy to improve it.</li><li>When evaluations shows  <strong>5~30% scores</strong>, it’s at our <strong>comfort zone</strong> and we can pay more attention to improve model’s capabilities on these aspects.</li></ul></li><li><strong>Q4</strong>: As architecture students, we see <strong>Nano Banana Pro</strong> has really good understanding of <strong>architectures and spatial reasoning</strong> <strong>capabilities</strong>. Did you specifically train it with architecture images?<ul><li>As Nano Banana Pro is trained on a large amount of different types of data, which of course includes architecture images, it shows improvement on architecture as well as many other domains.</li></ul></li><li><strong>Q5</strong>: will <strong>AI replace human</strong> on the job market?<ul><li>I do think what humans do will change. <strong>AI reshapes what people spend</strong> <strong>their time on</strong>.</li><li>I have a paper last year on this topic, including healthcare, employment, problematic misinformation, political bias, etc. Read it at <a href="http://shapingai.com/">shapingai.com</a>.</li></ul></li><li><strong>Q6</strong>: suggestions on improving LLM <strong>performance &#x2F;</strong> <strong>system</strong> <strong>efficiency</strong>?<ul><li>There is a Google-internal doc that me (Jeff Dean) and Sanjay Ghemawat created called “<strong>Performance hint</strong>”. Googlers can feel free to read it.</li></ul></li><li><strong>Q7</strong>: how is <strong>Gemini 3</strong> different from the previous <strong>Gemini 2.5 version</strong>? Any <strong>significant</strong> <strong>architecture or other improvements</strong>?<ul><li>Of course it’s still transformer architecture. I am kind of sad  it couldn’t do continual learning at this point.<ul><li>Note by gdymind: many AI pioneers also have this opinion.</li></ul></li><li>Actually the innovations of Gemini 3 came from <strong>many small ideas</strong> stacked together, where each idea contributes to say 5%, 3%, or 8% improvements.<ul><li>Note by gdymind: Andrej Karpathy also mentions this trend that many innovations are kind of equally important.</li></ul></li><li>We did <strong>many small-scale experiments</strong> on these ideas and ablation studies.</li></ul></li><li><strong>Q8</strong>: comments on <strong>open models</strong>, also on Chinese open models.<ul><li>I am actually a <strong>believer of open models</strong> for a long time. Google also has open-sourced Gemma models.</li><li>Chinese models are strong.</li><li>It’s nice to have have a bunch of open model to train for your own <strong>downstream tasks</strong>.</li></ul></li><li><strong>Q9</strong>: what’s the story behind <strong>Google Brain</strong> and <strong>DeepMind</strong> working together?<ul><li>Both teams did separate tasks at the beginning. One team was doing MoE, scaling, Transformer, etc., while the other was doing some traditional ML stuff.</li><li>Tasks and talents were <strong>fragmented</strong>. I thought it was dumb and pushed for combining the efforts.</li><li>It took some time and efforts for people to be used to work closely at different time zones.</li></ul></li><li><strong>Q10</strong>: comments on <strong>embedding learning</strong>?<ul><li>It would be great to have an e2e learning for <strong>general embeddings</strong> that can work for different downstream tasks, and also a hybrid retrieval long-context system would be great.</li></ul></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>meetup</tag>
      
      <tag>LLM</tag>
      
      <tag>Gemini</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Intro to PPO in RL</title>
    <link href="/2025/11/09/Intro-to-PPO-in-RL/"/>
    <url>/2025/11/09/Intro-to-PPO-in-RL/</url>
    
    <content type="html"><![CDATA[<h1 id="1-From-Rewards-to-Optimization"><a href="#1-From-Rewards-to-Optimization" class="headerlink" title="1. From Rewards to Optimization"></a>1. From Rewards to Optimization</h1><ul><li>In RL, an agent interacts with an environment by observing a <strong>state</strong> <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.826ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 807.3 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></svg></mjx-container>, taking an <strong>action</strong> <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.962ex" height="1.355ex" role="img" focusable="false" viewBox="0 -441 867.3 598.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(562,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></svg></mjx-container>, and receiving a <strong>reward</strong> <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.786ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 789.3 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(484,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></svg></mjx-container>.</li><li>In the context of LLM, state is the previous tokens, while action is the next token to generate.</li><li>The agent’s goal is to learn a <strong>policy</strong> ( <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.875ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3038.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mi" transform="translate(603,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mo" transform="translate(984.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1373.6,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(1902.6,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(2180.6,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(2649.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> ), which is a probability distribution over actions, that maximizes its long-term expected return.</li><li>In the context of LLM, policy is determined by model weights.</li></ul><p>Formally, the objective function is:</p><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.827ex;" xmlns="http://www.w3.org/2000/svg" width="21.258ex" height="6.785ex" role="img" focusable="false" viewBox="0 -1749.5 9395.9 2999"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g><g data-mml-node="mo" transform="translate(633,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1022,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(1491,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2157.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(3213.6,0)"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="TeXAtom" transform="translate(771,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mi" transform="translate(603,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></g><g data-mml-node="mrow" transform="translate(4897.5,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="5B" d="M269 -1249V1750H577V1677H342V-1176H577V-1249H269Z"></path></g><g data-mml-node="munderover" transform="translate(583,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(142.5,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1139,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(368.4,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"></path></g></g></g><g data-mml-node="msup" transform="translate(2193.7,0)"><g data-mml-node="mi"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></g><g data-mml-node="mi" transform="translate(627.3,413) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="msub" transform="translate(3126.2,0)"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(484,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(3915.4,0) translate(0 -0.5)"><path data-c="5D" d="M5 1677V1750H313V-1249H5V-1176H240V1677H5Z"></path></g></g></g></g></svg></mjx-container></p><p>where</p><ul><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.618ex" role="img" focusable="false" viewBox="0 -705 469 715"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></svg></mjx-container> are the model weights,</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="8.772ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3877.2 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></g><g data-mml-node="mo" transform="translate(820.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mo" transform="translate(1765.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(2154.6,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(2654.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(3099.2,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(3599.2,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container> is a discount factor weighting future rewards.</li></ul><p>In other words, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.253ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1880 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g><g data-mml-node="mo" transform="translate(633,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1022,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(1491,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> is a <strong>function of expected returns</strong>, and our goal is to find the optimal parameters <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="2.049ex" height="1.618ex" role="img" focusable="false" viewBox="0 -705 905.6 715"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(502,363) scale(0.707)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g></g></g></g></svg></mjx-container> that maximize it.</p><p>This objective was first formalized in <strong>REINFORCE</strong> (Williams, 1992) and forms the basis of all <strong>policy-gradient methods</strong>.</p><h1 id="2-The-Policy-Gradient-Theorem"><a href="#2-The-Policy-Gradient-Theorem" class="headerlink" title="2. The Policy Gradient Theorem"></a>2. The Policy Gradient Theorem</h1><p>To maximize <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.253ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1880 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g><g data-mml-node="mo" transform="translate(633,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1022,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(1491,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>, we need its gradient. The <strong>policy-gradient theorem</strong> gives us:</p><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.591ex;" xmlns="http://www.w3.org/2000/svg" width="32.497ex" height="2.287ex" role="img" focusable="false" viewBox="0 -750 14363.5 1011.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="2207" d="M46 676Q46 679 51 683H781Q786 679 786 676Q786 674 617 326T444 -26Q439 -33 416 -33T388 -26Q385 -22 216 326T46 676ZM697 596Q697 597 445 597T193 596Q195 591 319 336T445 80L697 596Z"></path></g><g data-mml-node="mi" transform="translate(866,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mi" transform="translate(1247.6,0)"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g><g data-mml-node="mo" transform="translate(1880.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2269.6,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(2738.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3405.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(4461.2,0)"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="TeXAtom" transform="translate(771,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mi" transform="translate(603,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></g><g data-mml-node="mrow" transform="translate(6145.1,0)"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="msub" transform="translate(278,0)"><g data-mml-node="mi"><path data-c="2207" d="M46 676Q46 679 51 683H781Q786 679 786 676Q786 674 617 326T444 -26Q439 -33 416 -33T388 -26Q385 -22 216 326T46 676ZM697 596Q697 597 445 597T193 596Q195 591 319 336T445 80L697 596Z"></path></g><g data-mml-node="mi" transform="translate(866,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mi" transform="translate(1692.3,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(2970.3,0)"><path data-c="2061" d=""></path></g><g data-mml-node="msub" transform="translate(3137,0)"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mi" transform="translate(603,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mo" transform="translate(4121.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(4510.6,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(562,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(5377.9,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(5655.9,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(6463.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="msub" transform="translate(6852.1,0)"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(783,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(7940.4,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></g></svg></mjx-container></p><p>Here, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.462ex" height="1.977ex" role="img" focusable="false" viewBox="0 -716 1088.3 873.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(783,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></svg></mjx-container> is the <strong>advantage function</strong>, which measures how much better (or worse) an action is compared to the policy’s average expectation in that state:</p><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="24.249ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 10718.1 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(783,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(1366,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msup" transform="translate(2421.8,0)"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="TeXAtom" transform="translate(824,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g></g></g><g data-mml-node="mo" transform="translate(3698.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(4087.9,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(4895.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(5339.8,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(562,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(6207.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(6818.3,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msup" transform="translate(7818.5,0)"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="TeXAtom" transform="translate(861.3,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g></g></g><g data-mml-node="mo" transform="translate(9132.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(9521.9,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(10329.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p><ul><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="7.913ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3497.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="TeXAtom" transform="translate(824,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1277.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1666.1,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(2135.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2579.7,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(3108.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>: expected return after taking action <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewBox="0 -441 529 451"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g></g></g></svg></mjx-container> in state <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.023ex" role="img" focusable="false" viewBox="0 -442 469 452"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g></g></g></svg></mjx-container>.</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.795ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2561.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="TeXAtom" transform="translate(861.3,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1314.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1703.4,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(2172.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>: expected return following the policy from state <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.023ex" role="img" focusable="false" viewBox="0 -442 469 452"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g></g></g></svg></mjx-container>.</li></ul><p>Using <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.462ex" height="1.977ex" role="img" focusable="false" viewBox="0 -716 1088.3 873.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(783,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></svg></mjx-container> rather than raw returns reduces variance and stabilizes training.</p><h1 id="3-The-Instability-Problem"><a href="#3-The-Instability-Problem" class="headerlink" title="3. The Instability Problem"></a>3. The Instability Problem</h1><p>Naive policy-gradient methods (like REINFORCE) update parameters directly using samples from the current policy.</p><p>But each update can drastically change the policy distribution <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.355ex;" xmlns="http://www.w3.org/2000/svg" width="2.228ex" height="1.33ex" role="img" focusable="false" viewBox="0 -431 984.6 588.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mi" transform="translate(603,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></g></svg></mjx-container>, causing divergence or collapse.</p><p>To address this, <strong>Trust Region Policy Optimization (TRPO)</strong> introduced a constraint to keep the new policy close to the old one by bounding their KL divergence.</p><p>However, TRPO is computationally expensive. It requires second-order optimization.</p><h1 id="4-PPO-Proximal-Policy-Optimization"><a href="#4-PPO-Proximal-Policy-Optimization" class="headerlink" title="4. PPO: Proximal Policy Optimization"></a>4. PPO: Proximal Policy Optimization</h1><p>PPO (Schulman et al., 2017, OpenAI) simplifies TRPO by directly penalizing or clipping updates that move too far from the previous policy.</p><p>Let</p><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.172ex;" xmlns="http://www.w3.org/2000/svg" width="18.409ex" height="5.475ex" role="img" focusable="false" viewBox="0 -1460 8136.6 2420"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(484,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(789.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1178.3,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(1647.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2314,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(3369.8,0)"><g data-mml-node="mrow" transform="translate(525.8,710)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mi" transform="translate(603,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mo" transform="translate(984.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1373.6,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(562,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(2240.9,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(2518.9,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(3326.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(220,-710)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="TeXAtom" transform="translate(603,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(500,0)"></path><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" transform="translate(778,0)"></path></g></g></g><g data-mml-node="mo" transform="translate(1596.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1985.3,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(562,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(2852.5,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(3130.5,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(3937.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="4526.8" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></p><p>be the probability ratio between the new and old policies.</p><p>PPO defines a <strong>clipped surrogate objective</strong>:</p><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -1.469ex;" xmlns="http://www.w3.org/2000/svg" width="54.625ex" height="4.07ex" role="img" focusable="false" viewBox="0 -1149.5 24144.4 1799"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="TeXAtom" transform="translate(714,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><path data-c="43" d="M56 342Q56 428 89 500T174 615T283 681T391 705Q394 705 400 705T408 704Q499 704 569 636L582 624L612 663Q639 700 643 704Q644 704 647 704T653 705H657Q660 705 666 699V419L660 413H626Q620 419 619 430Q610 512 571 572T476 651Q457 658 426 658Q322 658 252 588Q173 509 173 342Q173 221 211 151Q232 111 263 84T328 45T384 29T428 24Q517 24 571 93T626 244Q626 251 632 257H660L666 251V236Q661 133 590 56T403 -21Q262 -21 159 83T56 342Z"></path><path data-c="4C" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z" transform="translate(722,0)"></path><path data-c="49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z" transform="translate(1347,0)"></path><path data-c="50" d="M130 622Q123 629 119 631T103 634T60 637H27V683H214Q237 683 276 683T331 684Q419 684 471 671T567 616Q624 563 624 489Q624 421 573 372T451 307Q429 302 328 301H234V181Q234 62 237 58Q245 47 304 46H337V0H326Q305 3 182 3Q47 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM507 488Q507 514 506 528T500 564T483 597T450 620T397 635Q385 637 307 637H286Q237 637 234 628Q231 624 231 483V342H302H339Q390 342 423 349T481 382Q507 411 507 488Z" transform="translate(1708,0)"></path></g></g></g><g data-mml-node="mo" transform="translate(2453.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2842.3,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(3311.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3978.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(5033.8,0)"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mi" transform="translate(771,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(6110.1,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="5B" d="M224 -649V1150H455V1099H275V-598H455V-649H224Z"></path></g></g><g data-mml-node="mo" transform="translate(6748.8,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(833,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1111,0)"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(8582.4,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="28" d="M152 251Q152 646 388 850H416Q422 844 422 841Q422 837 403 816T357 753T302 649T255 482T236 250Q236 124 255 19T301 -147T356 -251T403 -315T422 -340Q422 -343 416 -349H388Q359 -325 332 -296T271 -213T212 -97T170 56T152 251Z"></path></g></g><g data-mml-node="msub" transform="translate(9040.4,0)"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(484,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(9829.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(10218.7,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(10687.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="msub" transform="translate(11076.7,0)"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(783,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(12165,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mtext" transform="translate(12609.6,0)"><path data-c="63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(444,0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(722,0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(1000,0)"></path></g><g data-mml-node="mo" transform="translate(14165.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(14554.6,0)"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(484,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(15343.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(15732.9,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(16201.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(16590.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(17035.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(17757.8,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(18758,0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g><g data-mml-node="mo" transform="translate(19164,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(19608.7,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(20330.9,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(21331.1,0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g><g data-mml-node="mo" transform="translate(21737.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="msub" transform="translate(22126.1,0)"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(783,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(23214.4,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="29" d="M305 251Q305 -145 69 -349H56Q43 -349 39 -347T35 -338Q37 -333 60 -307T108 -239T160 -136T204 27T221 250T204 473T160 636T108 740T60 807T35 839Q35 850 50 850H56H69Q197 743 256 566Q305 425 305 251Z"></path></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(23672.4,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="5D" d="M16 1099V1150H247V-649H16V-598H196V1099H16Z"></path></g></g></g></g></svg></mjx-container></p><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewBox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g></g></g></svg></mjx-container> (e.g. 0.1 – 0.2) limits how far the new policy can deviate.</p><ul><li>If <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="6.61ex" height="1.977ex" role="img" focusable="false" viewBox="0 -716 2921.8 873.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(783,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(1366,0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mn" transform="translate(2421.8,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container>, we want <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="5.934ex" height="1.864ex" role="img" focusable="false" viewBox="0 -666 2622.8 823.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(484,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(1067,0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mn" transform="translate(2122.8,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>: increase the probability of good actions.</li><li>If <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="6.61ex" height="1.977ex" role="img" focusable="false" viewBox="0 -716 2921.8 873.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(783,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(1366,0)"><path data-c="3C" d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z"></path></g><g data-mml-node="mn" transform="translate(2421.8,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container>, we want <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="5.934ex" height="1.864ex" role="img" focusable="false" viewBox="0 -666 2622.8 823.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(484,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(1067,0)"><path data-c="3C" d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z"></path></g><g data-mml-node="mn" transform="translate(2122.8,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>: decrease the probability of bad actions.</li><li>The <strong>clipping</strong> prevents overly aggressive updates.</li></ul><p>This makes PPO simple to implement, stable to train, and robust across diverse tasks.</p><h1 id="5-PPO-workflow"><a href="#5-PPO-workflow" class="headerlink" title="5. PPO workflow"></a>5. PPO workflow</h1><p>Image source: <a href="https://docs.google.com/presentation/d/1OiUV7pMp43k5wgmL1YikdRWpATOUmtv8QUSftzA85Jk/edit?slide=id.g39c0ff79865_1_43#slide=id.g39c0ff79865_1_43">TIS &amp; FlashRL - 1022@H2Lab</a></p><ul><li><strong>Generate sequences</strong> (trajectories) with the policy model (current LLM weights) for each prompt in the batch.<ul><li><strong>Trajectory</strong>: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="30.876ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 13647 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70F" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"></path></g><g data-mml-node="mo" transform="translate(794.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(1850.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(2239.6,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mn" transform="translate(502,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="mo" transform="translate(3145.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3589.8,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(562,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="mo" transform="translate(4555.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(5000,0)"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(484,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="mo" transform="translate(5887.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(6332.2,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mn" transform="translate(502,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(7237.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(7682.4,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(562,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(8648,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(9092.7,0)"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(484,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(9980.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(10424.9,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(11763.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(12208.2,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mo" transform="translate(13258,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>. That is, starting from an initial state, LLM takes actions <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.937ex" height="1.355ex" role="img" focusable="false" viewBox="0 -441 856 598.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(562,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> (i.e., generates tokens) step by step, receives rewards <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.76ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 778 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(484,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>, and transitions through different states <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.801ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 796 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> (resulting responses), forming one complete episode of experience.</li></ul></li></ul><p><img src="/img/Intro_to_PPO/image.png" alt="image.png"></p><ul><li><strong>Compute log probs</strong> <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="10.143ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4483.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(1278,0)"><path data-c="2061" d=""></path></g><g data-mml-node="msub" transform="translate(1444.7,0)"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mi" transform="translate(603,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mo" transform="translate(2429.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2818.3,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(3347.3,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(3625.3,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(4094.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> from the <em>policy</em> model for all tokens in the generated responses.</li></ul><p><img src="/img/Intro_to_PPO/image%201.png" alt="image.png"></p><ul><li><strong>Compute log probs</strong> from the <em>reference</em> model<ul><li><strong>Reference model</strong>: a <strong>frozen copy</strong> of the initial SFT, which is the baseline that doesn’t change during PPO training.<ul><li>It’s used to <strong>stabilize training</strong> and prevent the policy model from drifting too far away from the original behavior.</li><li>Specifically, PPO uses the <strong>KL-divergence</strong> between the policy and the reference. This acts as a regularization term.</li></ul></li></ul></li></ul><p><img src="/img/Intro_to_PPO/image%202.png" alt="image.png"></p><ul><li><strong>Compute the gradient</strong> and <strong>update weights</strong> of the policy model</li></ul><p><img src="/img/Intro_to_PPO/image%203.png" alt="image.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Truncated Importance Sampling (TIS) in RL</title>
    <link href="/2025/11/08/Truncated-Importance-Sampling-TIS-in-RL/"/>
    <url>/2025/11/08/Truncated-Importance-Sampling-TIS-in-RL/</url>
    
    <content type="html"><![CDATA[<h1 id="truncated-importance-sampling-tis"><a href="#truncated-importance-sampling-tis" class="headerlink" title="truncated importance sampling (tis)"></a>truncated importance sampling (tis)</h1><blockquote><p>this blog is from feng yao (ucsd phd student)’s work. i added some background and explanations to make it easier to understand.</p></blockquote><p><strong>slides</strong>: <a href="https://docs.google.com/presentation/d/1OiUV7pMp43k5wgmL1YikdRWpATOUmtv8QUSftzA85Jk/edit?slide=id.g37644bc496b_0_42#slide=id.g37644bc496b_0_42">on the rollout-training mismatch in modern rl systems</a></p><p><strong>notion blog</strong>: <a href="https://fengyao.notion.site/off-policy-rl">your efficient rl framework secretly brings you off-policy rl training</a></p><p>veRL/OpenRLHF/Slime adopts hybrid engines</p><ul><li>rollout: advanced LLM inference engines (vLLM, SGLang)</li><li>training: modern LLM training backends (FSDP, Megatron)</li></ul><p><img src="/img/TIS/image.png" alt="image.png"></p><p>the logprob produced between sampler (vLLM, SGLang, etc.) and trainer (DeepSpeed, Megatron-LM, etc.) are not exactly the same.</p><p><img src="/img/TIS/image%201.png" alt="image.png"></p><ul><li><p>formula explanation:</p><ul><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.618ex" role="img" focusable="false" viewBox="0 -705 469 715"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></svg></mjx-container>: weights; <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.364ex" height="1.489ex" role="img" focusable="false" viewBox="0 -442 603 658"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g></g></g></svg></mjx-container>: learning rate</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.111ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1817 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mo" transform="translate(570,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(959,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(1428,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>: policy function, which is the token distribution from the LLM</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewBox="0 -441 529 451"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g></g></g></svg></mjx-container>: action, which is the token that LM generates</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="8.325ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3679.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(806.8,0)"><path data-c="223C" d="M55 166Q55 241 101 304T222 367Q260 367 296 349T362 304T421 252T484 208T554 189Q616 189 655 236T694 338Q694 350 698 358T708 367Q722 367 722 334Q722 260 677 197T562 134H554Q517 134 481 152T414 196T355 248T292 293T223 311Q179 311 145 286Q109 257 96 218T80 156T69 133Q55 133 55 166Z"></path></g><g data-mml-node="mi" transform="translate(1862.6,0)"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mo" transform="translate(2432.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2821.6,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(3290.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>: the token generated following the distribution of the LLM</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.674ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2066 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mo" transform="translate(759,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1148,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(1677,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>: reward</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="12.782ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5649.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="2207" d="M46 676Q46 679 51 683H781Q786 679 786 676Q786 674 617 326T444 -26Q439 -33 416 -33T388 -26Q385 -22 216 326T46 676ZM697 596Q697 597 445 597T193 596Q195 591 319 336T445 80L697 596Z"></path></g><g data-mml-node="mi" transform="translate(866,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mi" transform="translate(1414.3,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(2692.3,0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(2859,0)"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mo" transform="translate(3429,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(3818,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(4347,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(4791.6,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(5260.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>: gradient</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.791ex;" xmlns="http://www.w3.org/2000/svg" width="25.372ex" height="2.713ex" role="img" focusable="false" viewBox="0 -849.5 11214.5 1199"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="TeXAtom" transform="translate(771,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(529,0)"><path data-c="223C" d="M55 166Q55 241 101 304T222 367Q260 367 296 349T362 304T421 252T484 208T554 189Q616 189 655 236T694 338Q694 350 698 358T708 367Q722 367 722 334Q722 260 677 197T562 134H554Q517 134 481 152T414 196T355 248T292 293T223 311Q179 311 145 286Q109 257 96 218T80 156T69 133Q55 133 55 166Z"></path></g><g data-mml-node="msub" transform="translate(1307,0)"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mi" transform="translate(603,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2441.4,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="5B" d="M202 -349V850H394V810H242V-309H394V-349H202Z"></path></g></g><g data-mml-node="mi" transform="translate(2858.4,0)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mo" transform="translate(3617.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(4006.4,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(4535.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5146.7,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="msub" transform="translate(5646.9,0)"><g data-mml-node="mi"><path data-c="2207" d="M46 676Q46 679 51 683H781Q786 679 786 676Q786 674 617 326T444 -26Q439 -33 416 -33T388 -26Q385 -22 216 326T46 676ZM697 596Q697 597 445 597T193 596Q195 591 319 336T445 80L697 596Z"></path></g><g data-mml-node="mi" transform="translate(866,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mi" transform="translate(7061.2,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(8339.2,0)"><path data-c="2061" d=""></path></g><g data-mml-node="msub" transform="translate(8505.8,0)"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mi" transform="translate(603,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mo" transform="translate(9490.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(9879.5,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(10408.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(10797.5,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="5D" d="M22 810V850H214V-349H22V-309H174V810H22Z"></path></g></g></g></g></svg></mjx-container>: the average reward-weighted policy gradients (follow the LLM distribution)</li></ul></li><li><p>now the policy function is mismatched between the training (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex;" xmlns="http://www.w3.org/2000/svg" width="4.376ex" height="1.625ex" role="img" focusable="false" viewBox="0 -431 1934.3 718.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mtext" transform="translate(603,-150) scale(0.707)"><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(306,0)"></path><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" transform="translate(700,0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(1256,0)"></path></g></g></g></g></svg></mjx-container>) and inference (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="4.544ex" height="1.332ex" role="img" focusable="false" viewBox="0 -431 2008.5 588.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mtext" transform="translate(603,-150) scale(0.707)"><path data-c="76" d="M338 431Q344 429 422 429Q479 429 503 431H508V385H497Q439 381 423 345Q421 341 356 172T288 -2Q283 -11 263 -11Q244 -11 239 -2Q99 359 98 364Q93 378 82 381T43 385H19V431H25L33 430Q41 430 53 430T79 430T104 429T122 428Q217 428 232 431H240V385H226Q187 384 184 370Q184 366 235 234L286 102L377 341V349Q377 363 367 372T349 383T335 385H331V431H338Z"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(528,0)"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(806,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1084,0)"></path></g></g></g></g></svg></mjx-container>) engine. this makes RL off-policy.</p></li><li><p>why does rollout-training mismatch occur?</p><ul><li>two common beliefs<ul><li>inaccessible true sampling probabilities: add additional gap<ul><li>during rollout, the model generates tokens by sampling from its probability distribution, but during training, we cannot precisely access the true probability with which each token was sampled.</li><li>vLLM v1 engine didn’t support directly returning the adjusted probabilities used for sampling, introducing an additional gap (now it’s fixed)</li></ul></li><li>backend numerical differences: hard to fix</li></ul></li><li>hybrid engine &amp; error propagation: different compute patterns via different backends &amp; parallelism</li></ul><p>  <img src="/img/TIS/image%202.png" alt="image.png"></p><ul><li>the vanilla importance sampling: use the ratio between trainer and sampler’s probability as part of the weights</li></ul><p>  <img src="/img/TIS/image%203.png" alt="image.png"></p><ul><li>the importance ratio can be too large and makes the training crash. in practice, we cap the importance ratio to make it more stable</li></ul></li></ul><p><img src="/img/TIS/25ed2cfe-2a91-43a1-9a51-4ee9f24823bb.png" alt="image.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>speculative decoding 02</title>
    <link href="/2025/09/19/speculative-decoding-Lily-Liu/"/>
    <url>/2025/09/19/speculative-decoding-Lily-Liu/</url>
    
    <content type="html"><![CDATA[<p><strong>Speaker</strong>: Lily Liu</p><ul><li>Working at OpenAI</li><li>Graduated from UC Berkeley in early 2025</li><li>vLLM speculative decoding TL</li></ul><h1 id="1-Why-is-LLM-generation-slow"><a href="#1-Why-is-LLM-generation-slow" class="headerlink" title="1. Why is LLM generation slow?"></a>1. Why is LLM generation slow?</h1><ul><li>GPU memory hierarchy. A100 example:<ul><li>SRAM is super fast (19 TB&#x2F;s) but super small (20 MB)</li><li>HBM: large (40 or 80 GB) but slower (1.5 TB&#x2F;s)</li><li>Main memory: very large but super slow</li></ul></li></ul><p><img src="/img/lily-liu-spec-decoding/6dd03710-9530-4361-8e95-c3297b974442.png" alt="image.png"></p><ul><li>a llama3-70B model with BF16 data type occupies 140 GB for its model weights<ul><li>the size is even too big to fit into HBM</li></ul></li><li>Suppose we can store the whole model in HBM. You still need to load model weights from HBM to SRAM</li></ul><p>Auto-regressive token generation: one token is generated each time → it’s <strong>memory-bound (need to load model weights and KV cache without enough compute)</strong></p><p><img src="/img/lily-liu-spec-decoding/cb549902-f9eb-4e25-8348-8007eac03e69.png" alt="image.png"></p><h1 id="2-What-is-speculative-decoding-SD"><a href="#2-What-is-speculative-decoding-SD" class="headerlink" title="2. What is speculative decoding (SD)?"></a>2. What is speculative decoding (SD)?</h1><ul><li><strong>Goal</strong>: reduce generation latency by amortizing bandwidth.<ul><li>The latency is critical to user experience</li><li>The two main techniques to reduce inference latency: speculative decoding and sharding<ul><li><strong>Sharding</strong>: partition LLM to multiple GPUs, which can do inference together</li><li><strong>Speculative decoding</strong>: kind of an algorithm-wise optimization, which does not need more hardware</li></ul></li></ul></li><li><strong>Intuition</strong>: Some tokens are easier to generate than others.<ul><li>Example:<ul><li>Prompt: What is your name?</li><li>Response: My name is Lily</li><li>There are some tokens that are easier to guess, for example, “is” in the response</li></ul></li><li>For easier tokens, maybe we don’t need a powerful LLM to generate</li></ul></li><li><strong>Idea</strong>: Use a small Language Model (LM) to propose tokens, and LLMs to verify multiple tokens in a single forward pass.<ul><li>The verification is done in a single forward pass.<ul><li>You pull the model weight <strong>once</strong> from HBM to SRAM, run the LLM forward pass once, then you know if T1, T2, T3 are correct or not.</li><li>It reduces memory accesses by amortizing model weights loading cost.</li></ul></li></ul></li></ul><p><img src="/img/lily-liu-spec-decoding/image.png" alt="image.png"></p><ul><li>Lossless!<ul><li>It is mathematically proven that SD will not decrease LLM’s accuracy. The output tokens would be exactly the same as if only the LLM generates tokens.</li></ul></li></ul><h1 id="3-Factors-that-affect-SD-speedup"><a href="#3-Factors-that-affect-SD-speedup" class="headerlink" title="3. Factors that affect SD speedup"></a>3. Factors that affect SD speedup</h1><ul><li><strong>Token acceptance rate</strong><ul><li>Assume in the extreme case, you have a very smart small model with only 1M parameters, that can guess all the tokens right, then you don’t need an LLM.</li><li>The point here is that how smart the small model is (i.e., how many tokens it can guess correctly) affects the SD performance.</li></ul></li><li><strong>Speculation length (k)</strong><ul><li>This defines how many tokens the small model generates for the LLM to verify</li></ul></li><li><strong>Cost of the drafting method (c &#x3D; ExecTime(draft) &#x2F; ExecTime(target))</strong><ul><li>How expensive it is to run the small model</li><li>In the extreme case, if the small model has the same size as the LLM, then there is no saving at all.</li></ul></li></ul><p><img src="/img/lily-liu-spec-decoding/image%201.png" alt="image.png"></p><h1 id="4-Draft-model-based-SD"><a href="#4-Draft-model-based-SD" class="headerlink" title="4. Draft-model based SD"></a>4. Draft-model based SD</h1><ul><li>Use a small draft LM to guess, and a large LM to verify.</li><li>The method was proposed in year 2023 when SD was first proposed</li></ul><p><img src="/img/lily-liu-spec-decoding/68671193-3377-4fe0-9bcb-6c0724ee208b.png" alt="image.png"></p><p>Some commonly seen (draft, target) model pairs</p><ul><li>(Llama-68M, Llama-2-7B)</li><li>(Llama-2-7B, Llama-2-70B)</li><li>(Llama-3-8B, Llama-3-70B)</li><li>(?, Llama-3-8B): even today, we haven’t found a good draft model for Llama3-8B → You need to train one, which is expensive</li></ul><p>Challenge: How to find the draft model?</p><p>The first version of SD in vLLM was draft-model based. Performance:</p><p><img src="/img/lily-liu-spec-decoding/image%202.png" alt="image.png"></p><ul><li>The x-axis is query per second (QPS). When QPS is low, we can have up to 1.5x speedup.</li><li>SD helps to relieve the memory-bound issue. When QPS is high, the workload is more compute-bound, and thus SD is less helpful.</li></ul><h1 id="5-Prompt-lookup-decoding"><a href="#5-Prompt-lookup-decoding" class="headerlink" title="5. Prompt lookup decoding"></a>5. Prompt lookup decoding</h1><ul><li>This method is model-free — no need for a draft model.</li><li>This is a pretty powerful method, especially in industry.<ul><li>Cursor has a blog post on how they applied this technique.</li></ul></li><li>Idea: using the following 2-gram lookup table as an example.</li><li>It tries to match the words in the “2-gram” column, and return the “3 speculative tokens”</li></ul><p><img src="/img/lily-liu-spec-decoding/image%203.png" alt="image.png"></p><p>Here is the result</p><p><img src="/img/lily-liu-spec-decoding/image%204.png" alt="image.png"></p><h1 id="6-Medusa-Eagle-MLPSpeculator-MTP"><a href="#6-Medusa-Eagle-MLPSpeculator-MTP" class="headerlink" title="6. Medusa&#x2F;Eagle&#x2F;MLPSpeculator&#x2F;MTP"></a>6. Medusa&#x2F;Eagle&#x2F;MLPSpeculator&#x2F;MTP</h1><ul><li>We don’t have to train a new model, and we don’t want a simple lookup table</li><li>Instead, we will have some additional layers on top of the original LM.</li><li>Looking at the figure, originally you will send the last hidden states to LM Head to generate the next token.</li><li>Now we are sending the last hidden states to some additional heads to propose tokens.</li><li>This way, we can utilize some info from the main model. In addition, we don’t need to train an independent model.</li></ul><p><img src="/img/lily-liu-spec-decoding/image%205.png" alt="image.png"></p><h2 id="MTP-Multi-token-prediction"><a href="#MTP-Multi-token-prediction" class="headerlink" title="MTP (Multi-token prediction)"></a>MTP (Multi-token prediction)</h2><ul><li>MTP was first proposed by Deepseek V3, and it is used more and more by open-source models.</li><li>Looking at the figure below, the leftmost part is still the main model.</li><li>There are some additional heads, which have a single layer of Transformer.</li><li>You use these additional layers to propose tokens, and use the main model for verification.</li></ul><p><img src="/img/lily-liu-spec-decoding/image%206.png" alt="image.png"></p><ul><li>MTP was originally proposed mainly for training purposes.<ul><li>Nowadays models are super large (like 100B) and therefore very powerful.</li><li>Many researchers believe these models have the capability to generate more than one token.</li><li>During training, if we can improve the model’s accuracy to propose more tokens, then it will also improve its capability to propose a single token.</li></ul></li></ul><h1 id="7-Research-Online-Speculative-Decoding"><a href="#7-Research-Online-Speculative-Decoding" class="headerlink" title="7. Research: Online Speculative Decoding"></a>7. Research: Online Speculative Decoding</h1><p>This is a project Lily did during her PhD</p><p><strong>Can we make the drafting method even better?</strong></p><h2 id="A-typical-inference-workflow-with-SD"><a href="#A-typical-inference-workflow-with-SD" class="headerlink" title="A typical inference workflow with SD"></a>A typical inference workflow with SD</h2><ul><li>Given a prompt, the draft model not only proposes tokens but also gives the probability distribution of the proposed tokens.<ul><li>Example: the proposed token t1 has the probability of 60%.</li></ul></li><li>Then the draft model sends the proposed tokens to the target model.</li><li>Then the target model verifies the proposed tokens. For example, t1 and t2 are correct, while t3 is incorrect.</li></ul><p><img src="/img/lily-liu-spec-decoding/image%207.png" alt="image.png"></p><p>Two observations:</p><ul><li>Static process, draft model is unchanged.</li><li>We have labels (positions and prob distributions) for free.</li></ul><h2 id="Online-SD"><a href="#Online-SD" class="headerlink" title="Online SD"></a>Online SD</h2><p>Instead of using a static draft model, let’s update it.</p><ul><li>We store the proposed tokens and their prob distributions in a buffer, and use them to improve the draft model.</li></ul><p><img src="/img/lily-liu-spec-decoding/image%208.png" alt="image.png"></p><p>Result: the x-axis is how many records you have seen so far, and the y-axis is the token acceptance rate (higher is better).</p><p><img src="/img/lily-liu-spec-decoding/image%209.png" alt="image.png"></p><p>Online updates can improve token acceptance rate very quickly, achieving close to offline distillation.</p><h2 id="Customized-draft-model"><a href="#Customized-draft-model" class="headerlink" title="Customized draft model"></a>Customized draft model</h2><p>Use multiple draft models depending on tasks</p><p><img src="/img/lily-liu-spec-decoding/image%2010.png" alt="image.png"></p><p><img src="/img/lily-liu-spec-decoding/image%2011.png" alt="image.png"></p><h1 id="8-Research-Dynamic-propose-length"><a href="#8-Research-Dynamic-propose-length" class="headerlink" title="8. Research: Dynamic propose length"></a>8. Research: Dynamic propose length</h1><h2 id="vLLM-SD-results"><a href="#vLLM-SD-results" class="headerlink" title="vLLM + SD results"></a>vLLM + SD results</h2><p>The results were from the first version of SD implementation in vLLM.</p><ul><li>SD has better speedup in low QPS.</li><li>When QPS &#x3D; 10, SD has slowdowns.</li></ul><p><img src="/img/lily-liu-spec-decoding/image%2012.png" alt="image.png"></p><ul><li>Intuition: 1) SD solves the problem if generation is memory-bound. It wastes extra computations for better memory access efficiency. When QPS is high, it’s more compute-bound. 2) What’s worse, the tokens proposed by the draft model may not be accepted. Those extra “wasted” computes have a larger impact when your workload is more compute-bound.</li></ul><h2 id="Dynamic-propose-length"><a href="#Dynamic-propose-length" class="headerlink" title="Dynamic propose length"></a>Dynamic propose length</h2><ul><li>Dynamically adjust the propose length based on system load and speculation accuracy.</li><li>Paper: <a href="https://arxiv.org/pdf/2406.14066">https://arxiv.org/pdf/2406.14066</a><ul><li>TurboSpec: Closed-loop Speculation Control System for Optimizing LLM Serving Goodput</li></ul></li></ul><p><img src="/img/lily-liu-spec-decoding/14f4714f-2fa3-4856-b5f3-ae40243c199e.png" alt="image.png"></p><h1 id="9-vLLM-SD"><a href="#9-vLLM-SD" class="headerlink" title="9. vLLM + SD"></a>9. vLLM + SD</h1><h2 id="Timeline"><a href="#Timeline" class="headerlink" title="Timeline"></a>Timeline</h2><p><img src="/img/lily-liu-spec-decoding/image%2013.png" alt="image.png"></p><p><img src="/img/lily-liu-spec-decoding/image%2014.png" alt="image.png"></p><p>Now not only Deepseek, Qwen model also uses MTP.</p><h2 id="Results-in-vLLM-v0-8-5"><a href="#Results-in-vLLM-v0-8-5" class="headerlink" title="Results in vLLM v0.8.5"></a>Results in vLLM v0.8.5</h2><p>about 1.5x to 2x speedup</p><p><img src="/img/lily-liu-spec-decoding/0f0b9d97-fafa-4dc3-8b95-8ec3a72998c5.png" alt="image.png"></p><p>We want to make sure the implementation in vLLM is efficient.</p><p>Let’s look at the execution time breakdown.</p><ul><li>The leftmost figure is without SD. CPU overhead is ~15%, while LLM inference takes ~85%.</li><li>The middle figure is n-gram. The CPU overhead is still ~15%. The lookup time is less than 5%. N-gram is very efficient.</li><li>For draft-model based methods as shown in the rightmost figure, the propose overhead is ~20%. But you can still see speedups. Because the additional heads is only 5% parameters of all model parameters.</li></ul><p><img src="/img/lily-liu-spec-decoding/image%2015.png" alt="image.png"></p><h1 id="10-QA"><a href="#10-QA" class="headerlink" title="10. QA"></a>10. QA</h1><p><strong>Q1: which workloads are good for SD?</strong></p><ul><li>For companies without many resources, we highly recommend n-gram.<ul><li>Suitable for code editing and summarization, where the output and the input have large overlaps. You can think about whether the task adds new info or just compresses the original info</li><li>We can see 2x to even 10x speedups on some coding tasks.</li><li>Cursor relies on n-gram very heavily.</li></ul></li><li>For chatting apps or open-ended QA, try Eagle</li></ul><p><strong>Q2: how do we reinforce the draft model?</strong></p><ul><li><p>Background</p><blockquote><p>Knowledge distillation is a general framework to align the predictive distribution of a small model (i.e., student model) with that of a larger one (i.e., teacher model).</p></blockquote><blockquote><p>Knowledge distillation is highly effective for speculative decoding. The draft model acts as the student and the target model serves as the teacher.</p></blockquote></li><li><p>Try different KL divergence metrics for the student and the teacher.</p></li><li><p>Use traditional distillation methods with different loss functions.</p></li></ul><p><img src="/img/lily-liu-spec-decoding/image%2016.png" alt="image.png"></p><p><strong>Q3: the small LM may co-locate with the main LM. What is their size ratio? Also in the project that has multiple draft models, are they placed in the same machine or not?</strong></p><ul><li>It’s a deep research question. No conclusions yet.</li><li>In production, draft models are less used due to their complexity.</li><li>Size ratio: say if you use a bigger draft model, will the acceptance rate be linearly increased?<ul><li>No. The benefit diminishes quickly.</li><li>Let’s say for (llama-3B draft, llama-70b target) can already have an 80% acceptance rate. But if you use llama3-8B, the acceptance rate is only ~85%.</li><li>Actually, for 70B-scale models, the best scale for target models is ~3B.</li></ul></li><li>Because the draft model is so small, we just put it on a single machine.</li><li>The draft model will occupy at least 20% of the main model’s GPU execution cycles. That’s why MTP, where the draft and the main LM share the same model, is more and more popular.</li><li>MTP also has KV Cache, but they are much smaller than the draft model’s KV Cache. MTP in general is a better approach.</li></ul><p><strong>Q3: Currently, n-gram in vLLM is purely running in CPU. Is there a way to use a GPU to speed up the longest prefix matching process?</strong></p><ul><li>We made that design on purpose. We tried the classical KMP algorithm, but it was not easy to implement. So we used numpy to implement this.</li><li>During benchmarking, we found that the overhead is less than 5%, which is acceptable.</li><li>String matching is not very GPU-friendly. But if you have a faster GPU kernel, we are happy to check it out.</li><li>if your input length is 10s of thousands of tokens, the overhead is usually less than 3%.</li><li>Can we use JAX to do this? It can be automatically used on a GPU. → We haven’t, but it’s worth a try.</li><li>the current implementation is already in C, not Python, so the efficiency is good enough.</li></ul><p><strong>Q4: is SD useful for reasoning models?</strong></p><ul><li>Draft-model based: it’s hard to be useful. If the model is too small, its reasoning capability may not be good enough.</li><li>MTP: unknown. Haven’t tested it yet. It might work because MTP’s acceptance rate is very high (80%-90%). MTP is trained with the main model. So we guess it will have some reasoning capabilities.</li><li>N-gram. Definitely take a try. Why? Reasoning tasks have long outputs. For long context, n-gram is still very efficient (i.e., the proposing overhead is very small). In addition, you can see many repeated patterns like “oh wait”, “no, the answer is incorrect” for reasoning models.</li><li>there are customers that are currently using SD for reasoning models.</li></ul><p><strong>Q5: Have you ever tried using the tokens that were not the most probable (like the 2nd most likely tokens)?</strong> - Haven’t tried that, but we are aware of this line of work. There is one sampling work called typical sampling: instead of using the most likely token, we also do sampling for the draft model. You have different ways to sample, and some have a higher acceptance rate.</p><ul><li>There is a PR that samples the draft model. We found that sampling is better than argmax(), especially when the temperature is high.</li><li>temperature&#x3D;0 is the worst case for SD, because it does not have error tolerance. If you propose a wrong token, it’s just wrong. But if the temperature is very high, it could be just slightly wrong.</li></ul><p><strong>Q6: are there resource limitations for the online learning project with multiple draft models?</strong></p><ul><li>We didn’t have enough engineering efforts on that.</li><li>In addition, you need a backward pass to update the draft model. So you kind of need a training framework to do auto-grad.</li><li>There are system implementation challenges. For example, are they two systems or one system?</li></ul><p><strong>Q7: for SD, what accuracy is practically useful to speedup? Is that 80%, 85%, or something else?</strong></p><ul><li>Depends on how expensive it is to run the draft model.</li><li>If the draft model is super fast, maybe 40% or 50% acceptance rate can already work.</li><li>Actually, originally we even used a 56M draft model to speculate a 70B model with acceptance rate ~50%. But it still had 1.5x speedup.</li></ul><p><strong>Q8: what is the optimal acceptance rate?</strong> - Depends on the acceptance rate.</p><ul><li>For MTP, 3 to 5 is doable. In vLLM, the most common setting is 3.</li></ul><p><strong>Q9: can we have a hierarchy of draft models? Draft1 speculates draft2, draft2 speculates draft3, …</strong></p><ul><li>Stanford has research on that. It’s not production-ready because serving multiple draft models is still challenging, especially when you need to manage their KV Cache. In addition, how to place these models among machines is also an issue.</li></ul><p>Source</p><p><a href="https://www.youtube.com/watch?v=epOCf1Ik2Cs">https://www.youtube.com/watch?v=epOCf1Ik2Cs</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM inference</tag>
      
      <tag>vLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vLLM 05 - vLLM multi-modal support</title>
    <link href="/2025/06/06/vLLM-multi-modal/"/>
    <url>/2025/06/06/vLLM-multi-modal/</url>
    
    <content type="html"><![CDATA[<p><strong>Speaker</strong>: Roger Wang</p><h1 id="1-Overview-large-multi-modal-models-LMMs"><a href="#1-Overview-large-multi-modal-models-LMMs" class="headerlink" title="1. Overview large multi-modal models (LMMs)"></a>1. Overview large multi-modal models (LMMs)</h1><ul><li>most SOTA Large Multimodal Models leverage a <strong>language model</strong> backbone with an <strong>encoder</strong> for a non-text modality. E.g., LLaVA, Qwen VL, Qwen2 VL, Qwen 2.5 VL, Kimi VL</li><li><strong>Visual encoders</strong>: img → visual embeddings</li><li><strong>Goal of vLLM</strong>: add these encoder supports<ul><li>input:  visual embeddings concatenate with text embeddings</li><li>output: text only. No plans for multi-modal outputs in vLLM (but can be in vLLM-project ecosystem) yet<ul><li>reason: architectures for open-source multi-modal-output models are not mature or unified yet</li></ul></li></ul></li></ul><p><img src="/img/vLLM-05-multi-modal/image.png" alt="image.png"></p><h1 id="2-Multi-modal-LLM-inference"><a href="#2-Multi-modal-LLM-inference" class="headerlink" title="2. Multi-modal LLM inference"></a>2. Multi-modal LLM inference</h1><ul><li><strong>User text prompt</strong>: “What’s in this image? <image>“ &amp; <strong>User image</strong>: cute_cat.jpeg</li><li>-&gt; <strong>Tokenized text prompt</strong>: [1, 10, 38, 52, 107, 48, 2, 32000]<ul><li>32000 in this example is the image placeholder’s token id, which is fixed for a specific model</li><li><strong>Image data</strong>: PIL.Image.Image -&gt; image processor -&gt; image features (torch tensors)<ul><li>image features are encoder’s inputs</li></ul></li></ul></li><li>-&gt; <strong>Expanded text token ids</strong>: [1, 10, 38, 52, 107, 48, 2, 32000, 32000, 32000, …, 32000, 32000]<ul><li>the image token id is repeated for several times.<ul><li>e.g., 32000 will be repeated 576 times in LLaVA 1.5, because LLaVA 1.5’s image embeddings are normalized to a fixed resolution and generate a fixed number (576) of embeddings</li><li>more recent models will do dynamic cutting and padding to convert images to patches, and generate one embedding for each patch. in this case, the repetition count is dynamic</li><li>Q: how do you know the number of repeating times in advance?<ul><li>for a specific model, it’s typically based on the image resolution, and you can pre-compute it.</li></ul></li><li>this process is input preprocessing on <em>CPU</em>, so the dynamic length doesn’t matter. when it’s on <em>GPU</em> later, the length is already determined</li></ul></li><li><strong>Processed image features</strong>: torch.Tensor -&gt; vision encoder (usually ViT) -&gt; image embeddings</li></ul></li><li>-&gt; <strong>Text embeddings</strong> of shape 583 x 4096 (hidden size of the language model)<ul><li><strong>Image embeddings</strong> 576 x 1024 (hidden size of vision encoder) -&gt; Projector&#x2F;MLP -&gt; 576x4096<ul><li>Projector: align hidden size between text and image embeddings</li></ul></li></ul></li><li>-&gt; Merge the two embeddings by replacing where 32000 is with image embeddings</li><li>-&gt; Language model (identical as text-only inference)<ul><li>later inference is exactly the same as text-only inference</li></ul></li></ul><h1 id="3-v0-vs-v1-for-multi-modal"><a href="#3-v0-vs-v1-for-multi-modal" class="headerlink" title="3. v0 vs v1 for multi-modal"></a>3. v0 vs v1 for multi-modal</h1><p>In vLLM V0, multimodality support was designed <strong>without</strong>…</p><ol><li><strong>Chunked prefill</strong>: we assumed requests will always be fully prefilled</li><li><strong>Prefix caching</strong>: V0 prefix caching was designed exclusively based on token IDs</li><li><strong>Efficient input processing</strong>: we assumed multimodal input processing has little CPU overhead</li></ol><h2 id="3-1-Chunked-prefill"><a href="#3-1-Chunked-prefill" class="headerlink" title="3.1 Chunked prefill"></a>3.1 Chunked prefill</h2><ul><li>Chunked prefill: Prompts can be <em>partially</em> prefilled in a step to balance between prefill &amp; decode workloads</li></ul><p><img src="/img/vLLM-05-multi-modal/image%201.png" alt="image.png"></p><p><strong>Problem</strong></p><ul><li>Text-only prefill assumes a <strong>discrete</strong>, <strong>causal</strong> nature of embeddings (1 token -&gt; 1 embedding)<ul><li>causal: previous tokens do not depend on later tokens. as a result, we can do prefill one-by-one</li></ul></li><li>Multimodal embeddings are typically <strong>continuous</strong> features and <strong>generation</strong> <strong>cannot be broken up</strong> because of encoder <strong>full-attention</strong></li><li>LMM in V0 <strong>assumes full prefilling</strong>, thus multimodal embeddings need to be fully merged with text embeddings once generated</li></ul><p>How to do chunked prefill with LMMs?</p><p><strong>One possible solution (what we could have done in V0):</strong></p><ol><li>Track multimodal embedding positions in the input sequence</li><li>Re-generate multimodal embeddings whenever needed</li><li>Merge the required portion into input sequence chunk</li></ol><ul><li><strong>Problem:</strong> Repetitive multimodal encoder execution. Example<ul><li>a 64-frame video of 448×448 resolution -&gt; 16384 embeddings</li><li>if token budget &#x3D; 2048 -&gt; 9 times of encoder execution for prefilling!</li></ul></li></ul><h3 id="V1-Encoder-cache-encoder-aware-scheduler"><a href="#V1-Encoder-cache-encoder-aware-scheduler" class="headerlink" title="V1: Encoder cache &amp; encoder-aware scheduler"></a>V1: Encoder cache &amp; encoder-aware scheduler</h3><p><img src="/img/vLLM-05-multi-modal/image%202.png" alt="image.png"></p><ul><li>track multimodal embedding positions</li><li>schedule requests based on both <em>encoder</em> &amp; decoder budget<ul><li>you don’t want encoder to block the decoder as well</li></ul></li><li>multimodal embeddings are generated from encoder execution and added to encoder cache (on GPU!)</li><li>retrieve from cache to merge with text embeddings to be sent to decoder LM<ul><li>keep the embedding if still needed for later chunks</li><li>evict otherwise<ul><li>e.g., all embeddings for a specific image are fully prefilled</li></ul></li></ul></li></ul><p>Encoder cache can be extended to support embedding caching <strong>across</strong> requests! (not implemented yet)</p><ul><li>for example, 5 requests sharing the same image</li></ul><p><strong>QA</strong></p><ul><li>Q: does vLLM support KV cache for image embeddings?<ul><li>A: after the preprocessing above, image embeddings can be considered to be normal tokens and you can keep their KV cache</li><li>the encoder cache we mentioned before is embedding cache, not the KV cache in the later stage</li></ul></li><li>Q: how can you do chunked prefill for full attention?<ul><li>A: we cannot do chunking in the encoder (due to the full attention). We run encoder once for the full image, and cache it. Then the decoder part (LLM) can fetch chunks from the encoder</li></ul></li><li>Q: so do images use causal attention in the decoder (LLM)?<ul><li>yes for most SOTA models</li><li>some models do full attention for the image part. e.g., gemma3<ul><li>you cannot do chunked prefill for such models</li></ul></li></ul></li><li>Q: what’s the relationship between images’ resolution and the sequence length?<ul><li>A: in general, higher resolution leads to longer sequence length</li><li>but the concrete relationship is determined by models<ul><li>e.g., the earliest LLaVA resizes all images to the same resolution, while QWEN2 VL partitions images to patches</li></ul></li></ul></li><li>Q: what if the step token budget is too small to prefill a full image?<ul><li>A: that’s why we need to support chunked prefill</li></ul></li><li>Q: any work to use causal attention in the <em>encoder</em>?<ul><li>Qwen omni paper: they tried to do sth similar to causal attention for audio (or maybe video) processing<ul><li>these data needs to be streamed-in, because you don’t want to start processing after the full audio data is available</li><li>it’s not the mainstream now</li></ul></li></ul></li></ul><h2 id="3-2-Prefix-caching"><a href="#3-2-Prefix-caching" class="headerlink" title="3.2 Prefix caching"></a>3.2 Prefix caching</h2><p>some <em>engineering</em> optimization</p><p>In V0, prefix caching is exclusively based on hashing tuples of token IDs</p><p>Problem:</p><ul><li>multimodal placeholder token and token ID (e.g., “”: 32000) for multimodal embeddings are always the same across requests!</li><li>correctness issue if two requests have identical prompts but different images! (e.g., “ Describe the image.”)</li><li>prefix caching is <em>always turned off</em> for multimodal models in V0</li></ul><h3 id="V1-prefix-caching-with-metadata"><a href="#V1-prefix-caching-with-metadata" class="headerlink" title="V1: prefix caching with metadata"></a>V1: prefix caching with metadata</h3><p>In V1, prefix caching is redesigned to allow additional metadata about the current block of tokens, so we can now add <strong>identifier (image hash, uuid, etc) of multimodal data</strong> too!</p><p><img src="/img/vLLM-05-multi-modal/1be0483f-de49-488e-8162-881bc338daf2.png" alt="image.png"></p><p><a href="https://docs.vllm.ai/en/stable/design/v1/prefix_caching.html">https://docs.vllm.ai/en/stable/design/v1/prefix_caching.html</a></p><p>There can be much research on caching optimization. For example, image frames of the video have similarities. Right now the multi-modal support is not mature yet.</p><h2 id="3-3-Efficient-input-processing"><a href="#3-3-Efficient-input-processing" class="headerlink" title="3.3 Efficient input processing"></a>3.3 Efficient input processing</h2><p>some <em>engineering</em> optimization</p><p>Input processing can be very expensive (sometimes longer than encoder execution)</p><h3 id="Optimized-engine-loop"><a href="#Optimized-engine-loop" class="headerlink" title="Optimized engine loop"></a>Optimized engine loop</h3><p>Image pre-processing (PIL.Image.Image → torch tensors) actually leads to large CPU overhead</p><p>V0 assumed the overhead was small, but it turned out not</p><p>V1: pre-processing and the inference engine core are in different processes </p><p><img src="/img/vLLM-05-multi-modal/image%203.png" alt="image.png"></p><h3 id="Multi-modal-feature-caching"><a href="#Multi-modal-feature-caching" class="headerlink" title="Multi-modal feature caching"></a>Multi-modal feature caching</h3><ul><li>typically we have plenty of CPU memory available, so it can be utilized to cache the generated features from raw data format</li><li>reuse same identifier for prefix caching</li><li>mirrored caches in two processes (AsyncLLM and LLMEngineCore0) for less data transfer<ul><li>next step: use shared memory that both processes can access</li></ul></li></ul><p><img src="/img/vLLM-05-multi-modal/cc2b38fe-ac5b-44ae-b502-7e1cd6a2a3ff.png" alt="image.png"></p><ul><li>for multi-turn conversations or few-shot learning, we don’t have to re-generate features</li></ul><h1 id="4-Benchmark"><a href="#4-Benchmark" class="headerlink" title="4. Benchmark"></a>4. Benchmark</h1><h2 id="Online-serving"><a href="#Online-serving" class="headerlink" title="Online serving"></a>Online serving</h2><p>Results on previous alpha v1 releases</p><p><img src="/img/vLLM-05-multi-modal/image%204.png" alt="image.png"></p><ul><li>workload: single image + single text question</li><li>when qps is low, not much room for optimization</li></ul><h2 id="Offline-inference"><a href="#Offline-inference" class="headerlink" title="Offline inference"></a>Offline inference</h2><p><img src="/img/vLLM-05-multi-modal/image%205.png" alt="image.png"></p><ul><li>v1 no caching: benefits from splitting processes</li><li>0% repeat: all images and text are unique<ul><li>v1 + feature caching + prefix caching: worse than no caching due to the extra caching overhead but they were not reused</li></ul></li><li>50% repeat: 50% of all data are repeated</li></ul><h1 id="5-Future-work"><a href="#5-Future-work" class="headerlink" title="5. Future work"></a>5. Future work</h1><ul><li>right now one AsyncLLM works for one EngineCore. Change to <strong>many-to-many mapping</strong><ul><li>we can use more processes for image pre-processing</li></ul></li><li><strong>Non-huggingface&#x2F;third-party processor plugin</strong><ul><li>currently using huggingface to process</li><li>the reason vLLM decided to use huggingface because they didn’t want developers to implement new model supports twice (on huggingface and on vLLM)</li><li>but it turned out huggingface was too slow, and developers want to add their own processors</li></ul></li><li><strong>streaming inputs support</strong><ul><li>e.g., process image frames on the fly for a video</li><li>why not supported at this point<ul><li>currently the scheduler is not stateful. it cannot receive a signal like “more frames will be streamed for the current video”</li><li>not very popular in the community: although Qwen 2.5 VL supports video understanding, most ppl use it for image understanding. needs a good video understanding model</li></ul></li></ul></li><li><strong>Blended mixed modalities</strong><ul><li>previously, data in different modalities are concatenated</li><li>“audio in video” in Qwen2.5-Omni: audio and video embeddings are mixed together<ul><li>e.g., if we have 12 audio embeddings and 15 video embeddings. we can place them as 4 audio embeddings + 5 video embeddings + 4 audio embeddings + …</li></ul></li></ul></li><li><strong>Multi-modal output: not likely in vllm-project&#x2F;vllm</strong></li></ul><p><strong>Any contribution&#x2F;discussion is welcomed!</strong></p><ul><li>Slack channel #sig-multi-modality</li><li>vllm-project -&gt; Projects -&gt; Multi-modality Core</li><li>Current core contributors:<ul><li>Roger Wang @ywang96</li><li>Cyrus Leung @DarkLight1337</li></ul></li></ul><p>Source: </p><p><a href="https://www.youtube.com/watch?v=rp2QMfhex4A">https://www.youtube.com/watch?v=rp2QMfhex4A</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM inference</tag>
      
      <tag>vLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Perplexity DeepSeek MoE</title>
    <link href="/2025/05/16/Perplexity-DeepSeek-MoE/"/>
    <url>/2025/05/16/Perplexity-DeepSeek-MoE/</url>
    
    <content type="html"><![CDATA[<p><strong>Speaker</strong>: Lequn Chen</p><p><strong>Sources</strong></p><ul><li><a href="https://www.perplexity.ai/hub/blog/lower-latency-and-higher-throughput-with-multi-node-deepseek-deployment">https://www.perplexity.ai/hub/blog/lower-latency-and-higher-throughput-with-multi-node-deepseek-deployment</a></li><li><a href="https://github.com/ppl-ai/pplx-kernels">https://github.com/ppl-ai/pplx-kernels</a></li></ul><h1 id="1-Setup"><a href="#1-Setup" class="headerlink" title="1. Setup"></a>1. Setup</h1><p>Multiple nodes to serve a <strong>single</strong> Deepseek instance (not DP with multiple replicas)</p><ul><li>Good for MoE models like Deepseek V3&#x2F;R1</li><li>In most systems, latency and throughput are often conflicting goals. Examples:<ul><li>dense models: batch size++ means throughput++ and latency++</li><li>TP in a node: reduce compute per GPU → latency–; decreases # replicas → throughput–</li></ul></li><li>MoE models can achieve both <strong>higher</strong> <strong>throughput</strong> and <strong>lower</strong> <strong>latency</strong> when utilizing more GPUs in multi-node deployments</li></ul><p><strong>Latency ↔ user experience</strong></p><p><strong>Throughput ↔ your cost</strong></p><p>It’s almost necessary to do disaggregated prefill on Deepseek models.</p><ul><li>P and D can have very different setups</li><li>P and D are implemented in different code paths</li></ul><p>We only consider D here</p><p>D is memory-bound workload, so the most important factor is batch size. P and D length doesn’t really matter</p><h1 id="2-Baseline-single-node-deployment"><a href="#2-Baseline-single-node-deployment" class="headerlink" title="2. Baseline: single-node deployment"></a>2. Baseline: single-node deployment</h1><p>Hardware: 8xH200</p><p><img src="/img/Perplexity-DeepSeek-MoE/image.png" alt="image.png"></p><ul><li><strong>Load balancer</strong>: distribute requests to DP groups (model replicas) using gRPC<ul><li>not like nginx that supports 1M qps</li><li>reasonable LLM inference qps is &lt;1k</li></ul></li><li>Each DP group maintains its own attention KV cache</li><li>GPUs are connected via NVLink → communication is fast</li><li>All DP ranks share the same experts</li></ul><p><strong>Can Deepseek fit into a single node (8xH200)?</strong></p><ul><li>DeepSeek is 671B; 1xH200 is 141GB</li><li>using EP8 DP8 TP1, model uses 100GB per H200</li><li>40GB left as KV cache per H200</li><li>one token is ~70KB KV cache. if one request is 5k tokens, each GPU can hold 100 requests<ul><li>40GB &#x2F; 70KB per token &#x2F; 5k tokens per request &#x3D; ~100 requests</li></ul></li></ul><h1 id="3-Multi-node-deployment"><a href="#3-Multi-node-deployment" class="headerlink" title="3. Multi-node deployment"></a>3. Multi-node deployment</h1><p>Hardware: multiple 8xH100 nodes</p><p><img src="/img/Perplexity-DeepSeek-MoE/image%201.png" alt="image.png"></p><ul><li>Attention under TP are connected via NVLink</li><li>Experts across nodes are connected via InfiniBand</li></ul><h1 id="4-Deepseek-MoE-parallelism"><a href="#4-Deepseek-MoE-parallelism" class="headerlink" title="4. Deepseek MoE parallelism"></a><strong>4. Deepseek MoE parallelism</strong></h1><p><a href="https://www.perplexity.ai/hub/blog/efficient-and-portable-mixture-of-experts-communication">https://www.perplexity.ai/hub/blog/efficient-and-portable-mixture-of-experts-communication</a></p><p><img src="/img/Perplexity-DeepSeek-MoE/image%202.png" alt="image.png"></p><h2 id="EP-MoE-part"><a href="#EP-MoE-part" class="headerlink" title="EP (MoE part)"></a>EP (<strong>MoE part)</strong></h2><ul><li>MLP is replaced with DeepSeekMoE (256 routed Es + 1 shared E)</li><li>Most model weights are for Es, not Attention.</li><li>So in multi-node deployment, we basically shard Es</li><li>Routed Es are <em><strong>distributed</strong></em> evenly. Example: 128 GPUs → 2 Es per GPU</li><li>The shared E is replicated</li><li>Before E compute, GPUs perform AllToAll ops to dispatch tokens</li><li>After E compute, another AllToAll to accumulate results</li></ul><h2 id="DP-MLA-attention-part"><a href="#DP-MLA-attention-part" class="headerlink" title="DP (MLA attention part)"></a>DP (<strong>MLA attention part)</strong></h2><ul><li>Attention weights are <em><strong>replicated</strong></em> among DP groups</li><li>Different DP groups receive different requests</li><li>Why DP for MLA? → Cannot split into (for example) 128 parts</li></ul><h2 id="TP"><a href="#TP" class="headerlink" title="TP"></a>TP</h2><p><strong>TP for dense models</strong></p><ul><li>shard Linear Projections along row or column</li><li>shard Attention along the attention head</li></ul><p><strong>TP for DeepSeek MLA</strong></p><ul><li>MLA first uses <code>kv_a_proj</code> to compute the latent vector, then uses <code>kv_b_proj</code> to transform it into the space of each attention head</li><li>the latent vector is <em><strong>shared</strong></em> <em><strong>by all heads</strong></em><ul><li>all TP ranks replicate <code>kv_a_proj</code> and <code>kv_b_proj</code></li><li>MLA stores the latent vector in the KV Cache. Each TP rank stores an identical copy of the KV Cache.</li></ul></li><li>TP still provides partial compute reduction</li><li>TP slightly saves more memory space: <em><strong>some</strong></em> kv projection can be split<ul><li>why “slightly”: most weights are on the MoE part</li></ul></li><li>EP &#x3D; DP * TP</li></ul><h2 id="EP-DP"><a href="#EP-DP" class="headerlink" title="EP + DP"></a>EP + DP</h2><p><img src="/img/Perplexity-DeepSeek-MoE/image%203.png" alt="image.png"></p><ul><li>GPU0 is DP group1; GPU1 is DP group2</li><li>ATTNs are computed independently; Es are shared</li></ul><h2 id="EP-TP"><a href="#EP-TP" class="headerlink" title="EP + TP"></a>EP + TP</h2><p><img src="/img/Perplexity-DeepSeek-MoE/image%204.png" alt="image.png"></p><ul><li>ATTN part is sharded. Needs AllGather (replicated) or AllReduce (reduced) to combine results</li><li>Es are still shared</li></ul><p><strong>Traditional TP in Megatron-LM</strong></p><ul><li>MLP: A is split column-wise; B is split row-wise</li><li>Attention: split among attention heads</li></ul><p><img src="/img/Perplexity-DeepSeek-MoE/image%205.png" alt="image.png"></p><blockquote><p>Blocks of Transformer with Model Parallelism. f and g are conjugate. f is an identity operator in the forward pass and all reduce in the backward pass while g is an all reduce in the forward pass and identity in the backward pass.</p></blockquote><h1 id="5-Single-vs-Multi-Node"><a href="#5-Single-vs-Multi-Node" class="headerlink" title="5. Single vs Multi-Node"></a><strong>5. Single vs Multi-Node</strong></h1><p><img src="/img/Perplexity-DeepSeek-MoE/image%206.png" alt="image.png"></p><h2 id="setup"><a href="#setup" class="headerlink" title="setup"></a>setup</h2><p>throughput vs latency under single-&#x2F;multi-node</p><ul><li><strong>x-axis</strong>: user perceived generating <strong>decoding</strong> speed (tok&#x2F;s)</li><li><strong>y-axis</strong>: <strong>PER NODE</strong> throughput (tok&#x2F;s) in log scale → can be converted to cost ($ per million toks)</li><li><strong>setups</strong>: nodes &#x3D; 1, 2, 4, 8, 16 (i.e., EP&#x3D;8, 16, 32, 64, 128); TP &#x3D; 1, 2, 4, 8; batch size &#x3D; 1, 2, 4, 8, 16, 32, 64, 128</li><li>one data points ↔ one specific setup combination</li></ul><h2 id="pareto-frontiers"><a href="#pareto-frontiers" class="headerlink" title="pareto frontiers"></a><strong>pareto frontiers</strong></h2><p>points on the line: <strong>pareto frontiers</strong> for each EP setup</p><ul><li>given the same x, it gives the best y</li><li>given the same y, it gives the best x</li><li>we only choose pareto frontiers to deploy based on our x-y tradeoff</li></ul><h2 id="single-node"><a href="#single-node" class="headerlink" title="single-node"></a>single-node</h2><p>The rightmost yellow point (max output speed)</p><ul><li>EP8 → deployed on a single node; DP1 → one DP group; TP8 → shard attention to 8 parts; NodeBs&#x3D;1 → one request on the node</li><li>why fastest?<ul><li>Attention is fast: TP8 → attention compute is split</li><li>MoE is fast: single node → no inter-node communication</li><li>batch size &#x3D; 1 → only 37B activated → memory need is not too big</li></ul></li></ul><p>Points on the yellow line: batch size++ → y++ and x–</p><p>Compared to other lines, when batch size increases, latency decreases more dramatically on the yellow line</p><ul><li>when NodeBs&#x3D;1, only 37B are activated</li><li>when NodeBs&gt;1, more weights are activated, and the performance will be limited by the <strong>memory bandwidth</strong> of a single node</li></ul><h2 id="multi-node"><a href="#multi-node" class="headerlink" title="multi-node"></a>multi-node</h2><p>with more nodes (i.e., larger EP), we have better results (excluding the yellow line → no inter-node communication)</p><ul><li>we achieve <strong>better latency and throughput</strong> at the same time!</li></ul><h2 id="scalability"><a href="#scalability" class="headerlink" title="scalability"></a>scalability</h2><p>Q: in real workloads, <strong>batch size is dynamic</strong><br>    - then your x-y will move on the EP line</p><p><strong>sub-linear scalability on a single node</strong>: when NodeBs x 8, the throughput won’t be 8x (e.g., only 2x)</p><ul><li>bottleneck is on loading Es</li></ul><p><strong>horizontal scalability on more nodes</strong>: with EP&#x3D;128, each node only holds 2 experts, this time throughput is more proportional to NodeBs</p><ul><li>istribute your batch size on more nodes → aggregated bandwidth is larger</li><li>one metric to estimate the scalability is, # of experts to read each time &#x2F; # of experts stored on this node. if this metric is larger, the scalability is better<ul><li>EP8 DP1 TP8 point: the single node reads one expert each time, and it stores all the experts → this metric is very low → sub-linear scalability</li><li>why? is that due to arithmetic intensity?</li></ul></li></ul><h2 id="single-node-vs-multi-node"><a href="#single-node-vs-multi-node" class="headerlink" title="single-node vs multi-node"></a>single-node vs multi-node</h2><p>When NodeBs&#x3D;128, single node has very good performance (better than multi nodes) → the current inter-node communication implementation is not good</p><ul><li>Perplexity’s EP128, DP128, TP1, NodeBs&#x3D;1024 result is (13.5, 13.8k), while Deepseek’s is (20, 15k)</li></ul><p><img src="/img/Perplexity-DeepSeek-MoE/image%207.png" alt="image.png"></p><p>you need large batch size to achieve good GroupGEMM performance</p><h1 id="5-EP-load-balancing"><a href="#5-EP-load-balancing" class="headerlink" title="5. EP load balancing"></a>5. EP load balancing</h1><p>Question: if Es are <strong>imbalanced</strong> among nodes, will it affect the pareto frontiers?</p><ul><li>Deepseek’s solution: replicate Es on nodes</li><li><strong>Expert Parallelism Load Balancer</strong> <a href="https://github.com/deepseek-ai/EPLB">https://github.com/deepseek-ai/EPLB</a><ul><li>hotter Es are put on more nodes</li><li>replica placement is adjusted every 5 or 10 minutes</li></ul></li></ul><p><strong>Classical system techniques</strong>: replicating, sharding, batching, caching, scheduling, pipelining</p><h1 id="6-Compute-communication-overlapping"><a href="#6-Compute-communication-overlapping" class="headerlink" title="6. Compute&#x2F;communication overlapping"></a>6. <strong>Compute&#x2F;communication overlapping</strong></h1><h2 id="5-stage-pipeline"><a href="#5-stage-pipeline" class="headerlink" title="5-stage pipeline"></a>5-stage pipeline</h2><ul><li>GPUs are idle during MoE Layer communication, and Infiniband is slow.</li><li>Without overlapping: the “dispatch” and “combine” in the figure below takes a long time.</li><li>Overlap dispatch: put the shared E computation between “dispatch send” and “dispatch receive”. Only 1~2 loc changes.</li><li>5-stage pipeline (from deepseek)<ul><li>split one batch to microbatches</li><li>when one microbatch is doing GPU compute, the other can do MoE communication</li></ul></li></ul><p><img src="/img/Perplexity-DeepSeek-MoE/image%208.png" alt="image.png"></p><h2 id="implementation"><a href="#implementation" class="headerlink" title="implementation"></a><strong>implementation</strong></h2><p>Before, Proj and Attn are in the same torch.nn.Module. now we can split them to different stages</p><p>Q: how are stages implemented? yielding ops or hardcode the workflow in a larger script?</p><ul><li>solution1: save intermediate results from proj (manually maintaining a state machine). This was Lequn’s initial implementation.</li><li>solution2: use python’s <code>yield</code>. This was the modified implementation.<ul><li>Will <code>yield</code> impact the CUDA graph?  No. It’s a static execution.</li></ul></li></ul><p>Q: are you using two CUDA streams?</p><ul><li>one stream is enough. two microbatches in the figure has no overlapping computation</li></ul><h2 id="Deepseek-trace"><a href="#Deepseek-trace" class="headerlink" title="Deepseek trace"></a>Deepseek trace</h2><p><a href="https://github.com/deepseek-ai/profile-data">https://github.com/deepseek-ai/profile-data</a></p><ul><li>Decoding: EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU.</li><li>The all-to-all communication during decoding does not occupy GPU SMs</li></ul><p><img src="/img/Perplexity-DeepSeek-MoE/image%209.png" alt="image.png"></p><h2 id="Micro-batching-improvement"><a href="#Micro-batching-improvement" class="headerlink" title="Micro-batching improvement"></a>Micro-batching improvement</h2><p>When batch size &lt; 32, micro-batching harms the throughput</p><ul><li>why not always improving? → micro-batch size &#x3D; batch size &#x2F; 2 → each kernel’s computing efficiency is worse → need large enough batch size</li><li>why 32 is the current turning point? the current communication implementation is not very good</li></ul><p><img src="/img/Perplexity-DeepSeek-MoE/image%2010.png" alt="image.png"></p><h1 id="7-Layer-latency-breakdown"><a href="#7-Layer-latency-breakdown" class="headerlink" title="7. Layer latency breakdown"></a>7. Layer latency breakdown</h1><p><strong>Total</strong>: w&#x2F; micro-batching, EP128 communication latency is hidden, and the total latency is reduced a lot (still slightly worse than EP8)</p><p><strong>Dispatch</strong>: compared to EP128 No Overlap, EP128 Dispatch Overlap saves the shared E time</p><p><strong>Perplexity vs Deepseek kernel latency</strong>: Perplexity’s multi-node implementation on infiniband is 1x slower than Deepseek’s implementation (DeepEP)</p><p><strong>GroupGEMM</strong> (i.e. MoE computation): GroupGEMM latency is the most important metric to show that multi-node is better than single-node. multi-node → larger batch size → better performance</p><p><strong>Kernel latency percentage</strong></p><ul><li>communication (dispatch&#x2F;combine) is the slowest part</li><li>MLA latency is already the secondary slowest w&#x2F; context length &#x3D; 54k. w&#x2F; larger context length, it will be worse<ul><li>one reason is that the current batch size is very large, which requires a lot of KV cache loading</li></ul></li></ul><p><img src="/img/Perplexity-DeepSeek-MoE/image%2011.png" alt="image.png"></p><p>The following part is to show that micro-batching indeed is slightly slower.</p><p>GroupGEMM’s benefit is greater than the latency increase here</p><p><img src="/img/Perplexity-DeepSeek-MoE/image%2012.png" alt="image.png"></p><h1 id="8-Roofline-analysis"><a href="#8-Roofline-analysis" class="headerlink" title="8. Roofline analysis"></a>8. Roofline analysis</h1><ul><li>Its horizontal axis is Arithmetic Intensity, the ratio of FLOP to memory I&#x2F;O bytes.</li><li>The horizontal axis value can be calculated directly from the kernel’s semantics.</li><li>The vertical axis represents achieved performance, calculated by dividing FLOP by benchmark latency.</li><li>The closer your implementation is to the white dotted line, the better your implementation is.</li><li>Slope of the white dotted line: memory bandwidth.</li></ul><h2 id="GroupGEMM"><a href="#GroupGEMM" class="headerlink" title="GroupGEMM"></a>GroupGEMM</h2><ul><li>The implementation is from Deepseek’s DeepGEMM</li><li>The figure shows lines of different group numbers. Example: EP8 → 256 &#x2F; 8 &#x3D; 32 groups</li><li>Dots on the line represent different batch sizes. Larger batch size gives you better performance.</li></ul><p><img src="/img/Perplexity-DeepSeek-MoE/image%2013.png" alt="image.png"></p><h2 id="GEMM"><a href="#GEMM" class="headerlink" title="GEMM"></a>GEMM</h2><p><img src="/img/Perplexity-DeepSeek-MoE/image%2014.png" alt="image.png"></p><h1 id="9-Multi-Token-Prediction-MTP"><a href="#9-Multi-Token-Prediction-MTP" class="headerlink" title="9. Multi-Token Prediction (MTP)"></a>9. Multi-Token Prediction (MTP)</h1><ul><li>MTP Module inputs: main model hidden states + predicted tokens</li><li>We can use the MTP module to do spec decoding.</li><li><strong>This is a very important optimization.</strong><ul><li>This blog uses MTP&#x3D;2. The original implementation used MTP&#x3D;1 and the performance was bad.</li></ul></li></ul><p><img src="/img/Perplexity-DeepSeek-MoE/image%2015.png" alt="image.png"></p><h1 id="10-CUDA-graph"><a href="#10-CUDA-graph" class="headerlink" title="10. CUDA graph"></a>10. CUDA graph</h1><p>CUDA graph: a <em><strong>static</strong></em> computing workflow.</p><ul><li>input&#x2F;output buffers’ pointer must be fixed</li><li>kernel launching parameters must be fixed</li><li>tensor shapes must be fixed</li></ul><p>How to put MoE computations into CUDA graph?</p><ul><li><code>torch.all_to_all_single()</code> requires all GPUs to use the same batch size</li><li>After implementing their own AllToAll Kernel, they no longer require all GPUs to use the same batch size.</li></ul><h2 id="11-Future-Work"><a href="#11-Future-Work" class="headerlink" title="11. Future Work"></a><strong>11. Future Work</strong></h2><p>The most important next optimization is Prefill Disaggregation.</p><p>The Prefill phase and Decode phase of the DeepSeek-V3&#x2F;R1 model have very different computational characteristics.</p><p>Both can use different optimization strategies and deployment schemes.</p><ul><li>E.g., different EP configuration</li></ul><h1 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h1><ol><li>Any implementation for training?<ul><li>This work is mainly for inference. You can go check FSDP&#x2F;ZeRO</li><li>Training workload is good for MoE, because the amount of data is large enough.</li></ul></li><li>Deepseek R1 w&#x2F; quantization can be deployed on a single 8xH100 node</li><li>Q: what are GroupGEMM’s dimensions [# <em><strong>activated</strong></em> Es on the GPU, # requests on the GPU]?<ul><li>It should be [# Es on this GPU, 8*n&#x2F;256] (ideal case where Es are tokens are evenly distributed)</li><li>8*n&#x2F;256 is the average # tokens per E<ul><li>n is the batch size</li><li>each token activates 8 Es</li><li>there are 256 Es in total</li></ul></li></ul></li><li>Are decoding optimizations more algorithm-wise or system-wise?<ul><li>spec decoding is both (but more algorithm-wise)</li><li>quantization is more algorithm-wise, with some system insight<ul><li>GPU may optimize for specialized format</li><li>4090&#x2F;5090 GPU optimizes INT4, while H100 does not</li><li>B100&#x2F;B200 optimized FP4</li></ul></li></ul></li><li>What is the kv cache hit rate in production?<ul><li>Check out character.ai’s blogs<ul><li><a href="https://research.character.ai/optimizing-inference/?ref=blog.character.ai">https://research.character.ai/optimizing-inference/?ref=blog.character.ai</a></li><li><a href="https://research.character.ai/optimizing-ai-inference-at-character-ai-part-deux/">https://research.character.ai/optimizing-ai-inference-at-character-ai-part-deux/</a></li></ul></li><li>Check out mooncake’s paper</li><li>Highly related to KV cache capacity → single GPU is not good; KV cache offloading is important</li></ul></li><li>Computing power increases 1.3x per year, while memory bandwidth increases 1.2x per year. → Memory-bound operator will be more and more compute-bound.</li></ol><p>Source</p><p><a href="https://www.youtube.com/watch?v=UMf5-K4PX8Q">https://www.youtube.com/watch?v=UMf5-K4PX8Q</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>MoE</tag>
      
      <tag>LLM inference</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MoE history and OpenMoE</title>
    <link href="/2025/04/25/MoE-history-and-OpenMoE/"/>
    <url>/2025/04/25/MoE-history-and-OpenMoE/</url>
    
    <content type="html"><![CDATA[<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>This article is compiled from a livestream.<br>The guest speaker is Fuzhao Xue, a Google Deepmind Senior Research Scientist and the author of OpenMoE</p><ul><li>Main research areas: Gemini Pretraining, Model Architecture, and Multi-Modal LLM</li><li>Key works: OpenMoE, Token-Crisis, AdaTape, Sequence Parallelism, and LongVILA.</li></ul><p><strong>Fuzhao: Seven Takeaways in My PhD</strong></p><ol><li><strong>Engineering</strong> is the foundation of research.</li><li>Working with talented individuals is incredibly helpful for improving <strong>research taste</strong>.</li><li>Aim for <strong>a concise and insightful 45-minute presentation</strong> rather than a long publication list throughout the PhD.</li><li><strong>Focus on a few key papers</strong> and understand them deeply rather than skimming many.</li><li>When onboarding to a new topic, <strong>reading papers along the timeline</strong> to study the evolution of research trends.<ul><li>“supervised practice”: what would I do next based on the current paper?</li></ul></li><li><strong>Transposition thinking</strong> is a highly effective way to improve writing and presentation.</li><li>A <strong>PhD degree is helpful but not a prerequisite</strong> for a career in LLM research.</li></ol><h1 id="OpenMoE"><a href="#OpenMoE" class="headerlink" title="OpenMoE"></a>OpenMoE</h1><h2 id="The-original-MoE"><a href="#The-original-MoE" class="headerlink" title="The original MoE"></a>The original MoE</h2><p>Common misunderstanding: MoE is a mix of many <em><strong>models</strong></em></p><p>Actually, it’s a single model with multiple <em><strong>experts</strong></em> in a FFN layer</p><p><strong>Router</strong> decides which experts to activate</p><p>Router is a simple <strong>linear layer</strong> doing dot product</p><p>Figure from <em>Switch Transformers (2022)</em></p><p><a href="https://arxiv.org/abs/2101.03961">https://arxiv.org/abs/2101.03961</a></p><p><img src="/img/OpenMoE/image.png" alt="image.png"></p><h2 id="History-of-MoEs"><a href="#History-of-MoEs" class="headerlink" title="History of MoEs"></a>History of MoEs</h2><h3 id="2017-LSTM-MoE"><a href="#2017-LSTM-MoE" class="headerlink" title="2017: LSTM + MoE"></a>2017: LSTM + MoE</h3><p>Gating network</p><ul><li>Naive hard selection is not differentiable.</li><li>This work used some tricks to make it differentiable.</li></ul><p><img src="/img/OpenMoE/image%201.png" alt="image.png"></p><h3 id="2021-Gshard-from-Google-MLSys"><a href="#2021-Gshard-from-Google-MLSys" class="headerlink" title="2021: Gshard from Google (MLSys)"></a>2021: Gshard from Google (MLSys)</h3><p>MoE: increase # parameters without increasing FLOPs → more sparse → more memory</p><p>Expert parallelism: different E on different GPUs</p><p>introduces extra all-to-all communication overhead</p><p><img src="/img/OpenMoE/image%202.png" alt="image.png"></p><h3 id="2021-Switch-Transformer"><a href="#2021-Switch-Transformer" class="headerlink" title="2021: Switch Transformer"></a>2021: Switch Transformer</h3><p>The first open-source MoE. Very important work.</p><p>Built atop T5. Because T5 is too large, not many ppl used it.</p><p><img src="/img/OpenMoE/image%203.png" alt="image.png"></p><p>Later works: GLaM, ST-MoE…</p><p>Awesome-mix-of-experts: 10 must-read papers <a href="https://github.com/XueFuzhao/awesome-mixture-of-experts">https://github.com/XueFuzhao/awesome-mixture-of-experts</a></p><p>Google did lots of works at that time.</p><h3 id="2023-OpenMoE-→-a-family-of-open-sourced-MoE-models"><a href="#2023-OpenMoE-→-a-family-of-open-sourced-MoE-models" class="headerlink" title="2023: OpenMoE → a family of open-sourced MoE models"></a>2023: OpenMoE → a family of open-sourced MoE models</h3><p>GPT4 used MoE</p><p><strong>Why Fuzhao chose MoE then</strong>: when scaling up, MoE still worked well</p><p>Trained for one year on TPU.</p><p>performance is so so nowadays. The main contributions are <strong>insights</strong> with <strong>visualizations</strong>.</p><p>MoE will be more popular as it’s cost-effective. vLLM is trying to support MoE more efficiently</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul><li>2017: LSTM + MoE</li><li>2021: Gshard (MoE MLSys)</li><li>2021: Switch Transformer (MoE)</li><li>GLaM, ST-MoE</li><li>2023: OpenMoE, Mixtral</li><li>2024: DeepSeek-MoE, OLMoE <a href="https://github.com/allenai/OLMoE?tab=readme-ov-file">https://github.com/allenai/OLMoE?tab=readme-ov-file</a></li><li>2025: LLaMA-4</li></ul><p>MoE was hard to train</p><h1 id="Formal-definition"><a href="#Formal-definition" class="headerlink" title="Formal definition"></a>Formal definition</h1><p>Given E experts:</p><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.819ex;" xmlns="http://www.w3.org/2000/svg" width="24.641ex" height="6.735ex" role="img" focusable="false" viewBox="0 -1730.8 10891.1 2976.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><g data-mml-node="mi" transform="translate(1051,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1536,0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mo" transform="translate(2300,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2689,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(3261,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3927.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(4983.6,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(148.2,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(451.9,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g></g></g><g data-mml-node="mi" transform="translate(6594.2,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(7071.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7460.2,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="msub" transform="translate(8032.2,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(422,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="msub" transform="translate(8748.2,0)"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(499,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(9541.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(9930.1,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(10502.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="3.554ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1571 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(499,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(793,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(1182,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> is a non-linear transformation of i-th expert, and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="3.579ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1582 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(477,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(866,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(422,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> is i-th element of the output of trainable router <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.839ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1255 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(477,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(866,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p><p>Usually, both <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.814ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1244 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mo" transform="translate(466,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(855,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.839ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1255 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(477,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(866,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> are parameterized by neural networks.</p><p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.839ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1255 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(477,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(866,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> scale-up experiment: no obvious benefit</p><h2 id="TopK-selection"><a href="#TopK-selection" class="headerlink" title="TopK selection"></a>TopK selection</h2><p>To ensure sparse routing <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.839ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1255 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(477,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(866,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>, <code>TopK()</code> is used to select the top ranked experts.</p><p>The formula shown is:</p><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="32.242ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 14251 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(477,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(866,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1438,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2104.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mtext" transform="translate(3160.6,0)"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(722,0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(1222,0)"></path><path data-c="4B" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q57 680 180 680Q315 680 324 683H335V637H313Q235 637 233 620Q232 618 232 462L233 307L379 449Q425 494 479 546Q518 584 524 591T531 607V608Q531 630 503 636Q501 636 498 636T493 637H489V683H499Q517 680 630 680Q704 680 716 683H722V637H708Q633 633 589 597Q584 592 495 506T406 419T515 254T631 80Q644 60 662 54T715 46H736V0H728Q719 3 615 3Q493 3 472 0H461V46H469Q515 46 515 72Q515 78 512 84L336 351Q332 348 278 296L232 251V156Q232 62 235 58Q243 47 302 46H335V0H324Q303 3 180 3Q45 3 36 0H25V46H58Q100 47 109 49T128 61V622Z" transform="translate(1778,0)"></path></g><g data-mml-node="mo" transform="translate(5716.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mtext" transform="translate(6105.6,0)"><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(394,0)"></path><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(894,0)"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(1200,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1589,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(2422,0)"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(2922,0)"></path></g><g data-mml-node="mo" transform="translate(9555.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(9944.6,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(10494.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(10883.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(11455.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(12066.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(13067,0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g><g data-mml-node="mo" transform="translate(13473,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(13862,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="3.005ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1328 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(550,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(939,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> is routing linear, and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewBox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g></g></g></svg></mjx-container> is a Gaussian noise used for exploration of expert routing.</p><h2 id="Balanced-loading"><a href="#Balanced-loading" class="headerlink" title="Balanced loading"></a>Balanced loading</h2><p>EP: if one expert is hot out of 4, 3 GPUs are often idle → 3 E are wastes of weights</p><p>An unbalanced token assignment can reduce the MoE model’s throughput and performance.</p><p>To prevent this, two issues need to be avoided:</p><ol><li><strong>Too many tokens</strong> being <strong>sent</strong> to a single expert</li><li><strong>Too few tokens</strong> being <strong>received</strong> by a single expert</li></ol><h3 id="1-Too-many-tokens-being-sent"><a href="#1-Too-many-tokens-being-sent" class="headerlink" title="1. Too many tokens being sent"></a><strong>1. Too many tokens</strong> <strong>being</strong> <strong>sent</strong></h3><p>To address the first issue, they define a <strong>buffer capacity B</strong>: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="12.015ex" height="1.781ex" role="img" focusable="false" viewBox="0 -705 5310.6 787"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(1036.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2092.6,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mi" transform="translate(2852.6,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mi" transform="translate(3741.6,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mi" transform="translate(4629.6,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g></svg></mjx-container></p><p>Where:</p><ul><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container> is the capacity ratio</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></svg></mjx-container> is the number of selected experts per token</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g></g></svg></mjx-container> is the batch size on each device</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.541ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 681 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g></svg></mjx-container> is the sequence length</li></ul><p>For each expert, they <strong>preserve at most B tokens</strong> regardless of how many tokens are dispatched to that expert.</p><h3 id="2-Too-few-tokens-being-received"><a href="#2-Too-few-tokens-being-received" class="headerlink" title="2. Too few tokens being received"></a>2. <strong>Too few tokens</strong> being <strong>received</strong></h3><p>To solve the second issue, the following <strong>auxiliary loss</strong> is added to the total model loss during training:</p><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.819ex;" xmlns="http://www.w3.org/2000/svg" width="22.683ex" height="6.735ex" role="img" focusable="false" viewBox="0 -1730.8 10025.9 2976.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mtext" transform="translate(331,-150) scale(0.707)"><path data-c="62" d="M307 -11Q234 -11 168 55L158 37Q156 34 153 28T147 17T143 10L138 1L118 0H98V298Q98 599 97 603Q94 622 83 628T38 637H20V660Q20 683 22 683L32 684Q42 685 61 686T98 688Q115 689 135 690T165 693T176 694H179V543Q179 391 180 391L183 394Q186 397 192 401T207 411T228 421T254 431T286 439T323 442Q401 442 461 379T522 216Q522 115 458 52T307 -11ZM182 98Q182 97 187 90T196 79T206 67T218 55T233 44T250 35T271 29T295 26Q330 26 363 46T412 113Q424 148 424 212Q424 287 412 323Q385 405 300 405Q270 405 239 390T188 347L182 339V98Z"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(556,0)"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(1056,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(1334,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1834,0)"></path><path data-c="63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z" transform="translate(2390,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(2834,0)"></path></g></g><g data-mml-node="mo" transform="translate(2976.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(4032.5,0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mo" transform="translate(5018.7,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="munderover" transform="translate(5518.9,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(148.2,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(451.9,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g></g></g><g data-mml-node="msub" transform="translate(7129.6,0)"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(911,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(8556.7,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="msub" transform="translate(9057,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(675,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container></p><ul><li><code>m_i</code>: the fraction of tokens dispatched to expert <code>i</code></li><li><code>P_i</code>：the <strong>softmax output</strong> for the i-th expert inside the router (i.e., a probability value).</li></ul><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -3.014ex;" xmlns="http://www.w3.org/2000/svg" width="18.245ex" height="6.935ex" role="img" focusable="false" viewBox="0 -1733 8064.1 3065.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(911,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1482.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2538.5,0)"><g data-mml-node="mn" transform="translate(310.5,676)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(220,-686)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><rect width="881" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(3826.2,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(124.5,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g><g data-mml-node="mo" transform="translate(412,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1190,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(481.2,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g><g data-mml-node="mi" transform="translate(5436.8,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(6012.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(6401.8,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="msub" transform="translate(7348.2,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(422,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container></p><ul><li><code>L</code>: Total number of tokens (batch size × sequence length).</li><li><code>h(x_j)_i</code>: For the j-th token, it indicates whether the token is dispatched to expert <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> (1 if yes, 0 otherwise).</li></ul><p>So <code>m_i</code> is:</p><ul><li>Counting how many tokens are routed to expert i,</li><li>Then normalizing by the total number of tokens, resulting in the <strong>fraction of load on expert i</strong>.</li></ul><p><code>m</code> is non-differentiable. Therefore, we define a differentiable <code>P_i</code> as:</p><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="23.498ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 10385.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(675,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1246.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mtext" transform="translate(2302.5,0)"><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(394,0)"></path><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(894,0)"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(1200,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1589,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(2422,0)"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(2922,0)"></path></g><g data-mml-node="mo" transform="translate(5752.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(6141.5,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(6691.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7080.5,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(7652.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(8263.7,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(9264,0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g><g data-mml-node="msub" transform="translate(9670,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(422,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container></p><p>When we minimize <code>l_{balance}</code>, we can see both <code>m</code> and <code>P</code> would be close to a uniform distribution.</p><p>Note: <code>P_i</code> is the <strong>input of Top K within router</strong> <code>g()</code></p><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="32.242ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 14251 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(477,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(866,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1438,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2104.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mtext" transform="translate(3160.6,0)"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(722,0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(1222,0)"></path><path data-c="4B" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q57 680 180 680Q315 680 324 683H335V637H313Q235 637 233 620Q232 618 232 462L233 307L379 449Q425 494 479 546Q518 584 524 591T531 607V608Q531 630 503 636Q501 636 498 636T493 637H489V683H499Q517 680 630 680Q704 680 716 683H722V637H708Q633 633 589 597Q584 592 495 506T406 419T515 254T631 80Q644 60 662 54T715 46H736V0H728Q719 3 615 3Q493 3 472 0H461V46H469Q515 46 515 72Q515 78 512 84L336 351Q332 348 278 296L232 251V156Q232 62 235 58Q243 47 302 46H335V0H324Q303 3 180 3Q45 3 36 0H25V46H58Q100 47 109 49T128 61V622Z" transform="translate(1778,0)"></path></g><g data-mml-node="mo" transform="translate(5716.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mtext" transform="translate(6105.6,0)"><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(394,0)"></path><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(894,0)"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(1200,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1589,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(2422,0)"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(2922,0)"></path></g><g data-mml-node="mo" transform="translate(9555.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(9944.6,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(10494.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(10883.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(11455.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(12066.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(13067,0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g><g data-mml-node="mo" transform="translate(13473,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(13862,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p><p>At that time, MoE can save 50% FLOPs. Now maybe it can be better.</p><h2 id="Training-on-MoE-multi-turn-is-worse"><a href="#Training-on-MoE-multi-turn-is-worse" class="headerlink" title="Training on MoE: multi-turn is worse??"></a>Training on MoE: multi-turn is worse??</h2><p>MTBench: an early multi-turn conversation benchmark.</p><p>multi-turn results are worse than single-turn results. Similarly, few shot is worse than zero shot. The reason will be discussed later</p><p><img src="/img/OpenMoE/477dc85a-efe2-4d24-8b70-814690bf575c.png" alt="image.png"></p><h2 id="Does-MoE-specialize"><a href="#Does-MoE-specialize" class="headerlink" title="Does MoE specialize?"></a>Does MoE specialize?</h2><p>Domain level? No specialization. Tokens are dispatched to different E uniformly.</p><p><img src="/img/OpenMoE/7675cc3f-05d4-4b90-a639-c15eeaf47fd3.png" alt="image.png"></p><p>Coding language? No</p><p>Natural language? To some degree, yes. Also similar languages to similar Es.</p><p><img src="/img/OpenMoE/d9ed9a79-1559-45f4-90b0-dc4e37d02785.png" alt="image.png"></p><p><img src="/img/OpenMoE/738e10de-667a-4dfd-9aa7-2c19efb355bb.png" alt="image.png"></p><p>why some E are served for many tasks.</p><ul><li>one possible case is that E handles system prompts</li></ul><p>is MoE just lazy and dispatching based on positions? No clear pattern.</p><p><img src="/img/OpenMoE/a93c893d-f2ce-4f0f-8bdc-1e7248ba291f.png" alt="image.png"></p><h2 id="Context-independent-specialization"><a href="#Context-independent-specialization" class="headerlink" title="Context-independent specialization"></a><strong>Context-independent specialization</strong></h2><p>Specialize in Token ID? <strong>YES</strong></p><p><img src="/img/OpenMoE/7458f9f0-0d4d-42bc-9cb1-3d5caf45cb1f.png" alt="image.png"></p><p>router is just a linear layer. Not learning high-level things. Just routing based on token IDs.</p><p>Deepseek V2 is also like this. Deepseek V3 is not tested yet.</p><p>From audience: Deepseek R1 has no token specialization</p><h3 id="Are-experts-clustering-similar-tokens"><a href="#Are-experts-clustering-similar-tokens" class="headerlink" title="Are experts clustering similar tokens?"></a>Are experts clustering similar tokens?</h3><p>Yes</p><p><img src="/img/OpenMoE/3e85675f-7e39-4485-8dbc-526cdb514663.png" alt="image.png"></p><p>E21 likes coding tokens</p><p>E31 likes modal verbs</p><p>E0 and E1 like garbage \n</p><h2 id="Why-Context-independent-Early-routing-learning"><a href="#Why-Context-independent-Early-routing-learning" class="headerlink" title="Why Context-independent? Early routing learning"></a>Why <strong>Context-independent? Early routing learning</strong></h2><p><img src="/img/OpenMoE/0c5ab82f-7c10-485e-9321-753b14b4bc1d.png" alt="image.png"></p><p>routing decision is learned very early when high-level semantics has not been learned well.</p><p><strong>QA: MoE on consumer GPU</strong></p><ul><li>Es are frequently swapped in and out with CPU memory/disk</li><li>How to reduce I/O?</li><li>prefetching E based on some correlations. For example, if Ex is activated in layer i, then Ey is likely to be activated in layer i+1.</li></ul><p>Task specialization is actually token specialization: </p><ul><li>Coding expert: like coding tokens</li><li>Math expert: like number tokens</li><li>…</li></ul><p><a href="https://arxiv.org/abs/2412.14219">A Survey on Inference Optimization Techniques for Mixture of Experts Models</a></p><h2 id="Drop-towards-the-End"><a href="#Drop-towards-the-End" class="headerlink" title="Drop towards the End"></a>Drop towards the End</h2><p><img src="/img/OpenMoE/image%204.png" alt="image.png"></p><p>why multi-turn worse? Why few shots worse? Looks like it’s worse with longer context length.</p><p>recall we have capacity ratio: if E receives too many tokens, it will drop the <strong>latter</strong> ones.</p><p>Even with load balancing, tokens dispatched to a specific E can be still too many.</p><ul><li>Inference distribution can be very different from training distribution. E.g., we can have a bunch of coding tasks coming in a short period of time.</li></ul><p>with longer context length, the load can be more imbalanced.</p><p>can this be solved during SFT? Tried but failed.</p><p>now with <strong>Megablock</strong> work, there is no C costraint anymore </p><h2 id="Recent-milestone-Megablock"><a href="#Recent-milestone-Megablock" class="headerlink" title="Recent milestone: Megablock"></a>Recent milestone: Megablock</h2><p>Recent MoEs remove capacity ratio C → dropless-MoE (dMoE) → load is imbalanced</p><ul><li>→ reduce EP.  For example, 8 or 16 experts per GPU</li><li>→ MegaBlocks: convert dense batched matmul to sparse ones</li></ul><p><a href="https://github.com/databricks/megablocks">https://github.com/databricks/megablocks</a></p><p><a href="https://arxiv.org/abs/2211.15841">https://arxiv.org/abs/2211.15841</a></p><p><img src="/img/OpenMoE/image%205.png" alt="image.png"></p><p><img src="/img/OpenMoE/image%206.png" alt="image.png"></p><p><img src="/img/OpenMoE/image%207.png" alt="image.png"></p><blockquote><p>Figure 3. <strong>Expert Computation in an MoE Layer</strong>. Shown with num expert=3.</p></blockquote><blockquote><p>(A) State-of-the-art MoE implementations use batched matrix multiplication to compute all experts within a layer in parallel. This introduces the constraints that all experts are assigned the same number of tokens and that all experts have the same shape.</p></blockquote><blockquote><p>(B) Expert computation can be analogously posed in terms of block diagonal matrix multiplication with identically sized blocks.</p></blockquote><blockquote><p>(C) In order to relax these constraints, we can construct a block diagonal matrix with variable sized blocks made up of many smaller blocks. <strong>We can compute this matrix efficiently using block-sparse matrix multiplication.</strong></p></blockquote><p>there are many ways to compute <strong>block-sparse matmul</strong> efficiently.</p><p>A new trend: attention-MoE disaggregation</p><h2 id="DeepSeek-MoE"><a href="#DeepSeek-MoE" class="headerlink" title="DeepSeek MoE"></a>DeepSeek MoE</h2><ul><li>More Es, and each E is smaller: more smooth routing</li><li>More Es are activated</li><li>Shared E: always on<ul><li>w/ EP, at least a GPU is not idle (it can do shared E computing)</li></ul></li></ul><p><img src="/img/OpenMoE/7bd7ae81-9c9e-4609-84ab-c2eb737d5eb3.png" alt="image.png"></p><p><strong>KTransformers</strong>: attention on GPU, MoE on CPU</p><p><a href="https://kvcache-ai.github.io/ktransformers/en/deepseek-v2-injection.html">https://kvcache-ai.github.io/ktransformers/en/deepseek-v2-injection.html</a></p><p><a href="https://github.com/kvcache-ai/ktransformers">https://github.com/kvcache-ai/ktransformers</a></p><h2 id="More-Thoughts"><a href="#More-Thoughts" class="headerlink" title="More Thoughts"></a>More Thoughts</h2><ul><li>MoE is tricky to train  (load balance, training stability)<ul><li>Example: UL2 on OpenMoE becomes unstable at 34B parameters, limiting its use to 50B tokens on OpenMoE 34B</li></ul></li><li>MoE is more data hungry<ul><li>MoE models are prone to overfitting when trained on limited or repeated data</li></ul></li><li>MoE is more sensitive to data diversity</li></ul><p><strong>DeepSeek</strong></p><ul><li>expert level lossless routing</li><li>device level: 4 Es per GPU. load is balanced among GPUs. Es within the same GPU can be imbalanced</li></ul><p><img src="/img/OpenMoE/image%208.png" alt="image.png"></p><p><img src="/img/OpenMoE/215f11cb-ea62-404b-807f-1077fcc2accc.png" alt="image.png"></p><p><strong>Dense upscaling</strong>: routing is more balanced as semantics are already learned in dense models</p><p><strong>Token-choice vs expert-choice MoE</strong></p><ul><li>Token-choice: choose E for each token</li><li>Expert-choice: all tokens come, each E will choose which token to accept<ul><li>Context leak issue. Cannot be used in decoders<ul><li>Unless ur batch size is super large, and only select on the batch dimension</li></ul></li></ul></li></ul><h2 id="Takeaways-at-that-time"><a href="#Takeaways-at-that-time" class="headerlink" title="Takeaways at that time"></a>Takeaways at that time</h2><ul><li><strong>Strength</strong><ul><li>MoE can save around 50% training cost. (and <em>more</em> nowadays)</li><li>MoE can save less than 50% inference cost.</li></ul></li><li><strong>Weakness</strong><ul><li>MoE is tricky to train (load balance, training stability).</li><li>MoE is more data hungry.</li><li>MoE is more sensitive to data diversity.</li><li>MoE is more communication expensive (EP. not that hardware friendly).</li></ul></li></ul><p>When shall we use MoE?</p><ul><li>Knowledge is important -&gt; We use more parameters to store the knowledge</li><li>A lot of queries in parallel -&gt; We can utilize more experts efficiently.</li><li>When we have enough data.</li><li>When we have more time to debug (at least in this stage) 😊</li></ul><h1 id="vLLM-MoE-support"><a href="#vLLM-MoE-support" class="headerlink" title="vLLM MoE support"></a>vLLM MoE support</h1><p>Core: handling EP</p><p>Route → MoE → Route back</p><p>Source</p><p><a href="https://www.youtube.com/watch?v=mHUBwzlsWjg">https://www.youtube.com/watch?v=mHUBwzlsWjg</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>MoE</tag>
      
      <tag>LLM inference</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vLLM 04 - vLLM v1 version</title>
    <link href="/2025/04/18/vLLM-04-vLLM-v1-version/"/>
    <url>/2025/04/18/vLLM-04-vLLM-v1-version/</url>
    
    <content type="html"><![CDATA[<p><strong>Official V1 blog</strong></p><p><a href="https://blog.vllm.ai/2025/01/27/v1-alpha-release.html">https://blog.vllm.ai/2025/01/27/v1-alpha-release.html</a></p><h1 id="Why-V1"><a href="#Why-V1" class="headerlink" title="Why V1?"></a>Why V1?</h1><ul><li><strong>V0 is slow</strong>: CPU overhead is high</li><li><strong>V0 is hard to read and develop</strong><ul><li>e.g., V0 scheduler is 2k LOC, V1 is 800 LOC<ul><li>V0 code decoupling is bad. Modifying one part can lead to crashes of other parts</li><li>Testing workload is high. Now the full testing suite takes 1h+ on A100&#x2F;H100</li></ul></li><li>GPU-cluster based CI&#x2F;CD is a good startup direction</li></ul></li><li>We want V1 to be easy-to-hack</li></ul><h2 id="How-to-push-out-refactor"><a href="#How-to-push-out-refactor" class="headerlink" title="How to push out refactor?"></a>How to push out refactor?</h2><ul><li><p>switch after stabilized (private company dev)**</p><ul><li>Was doing MLE in a big tech company</li><li>Need to put v0 into production in 1-2 months after v0 was released</li><li>Based on business needs, 3 ppl need to add customized features on v0 (also model dev and testing)</li><li>when features stabilized, add them to v1 as well</li><li>make v0 and v1 online together, and gradually switch to v1</li></ul></li><li><p><strong>Switching after finishing dev in vllm</strong>: impractical, as new features &amp; new models are coming very fast</p></li><li><p><strong>vLLM’s most important feature</strong>: new model support!!</p><ul><li>candidate1: easy-to-use</li><li>candidate2: performance</li><li>candidate3: new model support</li><li>however, <strong>day0 support</strong> leads to many <strong>technical debt</strong></li><li>some temp code will be used by others, and become harder and harder to remove</li><li>Example1: <code>prepare_input()</code> was 1000+ LOC</li><li>Example2: Attention kernel. Hard to have unified abstraction</li></ul></li></ul><h3 id="vLLM-team’s-eventual-plan"><a href="#vLLM-team’s-eventual-plan" class="headerlink" title="vLLM team’s eventual plan"></a><strong>vLLM team’s eventual plan</strong></h3><ul><li>Stage1: dev V1</li><li>Stage2: V0 &amp; V1 stay together</li><li>Stage3: V1 on by default</li><li>Stage4: V1 has more appealing features than V0</li><li>Stage5: remove V0 completely</li></ul><h3 id="Why-Pytorch-wins-Tensorflow"><a href="#Why-Pytorch-wins-Tensorflow" class="headerlink" title="Why Pytorch wins Tensorflow?"></a><strong>Why Pytorch wins Tensorflow?</strong></h3><ul><li>What TensorFlow claimed: they had the richest features, best hardware support, and best performance</li><li>The actual reason: <strong>researchers</strong> preferred PyTorch more as it’s <strong>easier to use</strong>, and then those researchers graduated</li><li>Note: vLLM v0 is not researcher-friendly enough</li></ul><h1 id="Key-changes-in-V1"><a href="#Key-changes-in-V1" class="headerlink" title="Key changes in V1"></a>Key changes in V1</h1><p><strong>Scheduler</strong></p><ul><li>Simplified scheduling: chunked prefill by default</li><li>Synchronous scheduling</li></ul><p><strong>General architecture</strong></p><ul><li>Scheduler, API server, (de) tokenizer in separate processes</li></ul><p><strong>Worker</strong></p><ul><li>Persistent batching</li><li>Piecewise cuda graph</li><li>Attention kernel<ul><li>Simplified configuration</li><li>Cascade inference</li></ul></li></ul><p><strong>Multi-modality</strong></p><ul><li>Embedding as the KV cache reference</li><li>KV cache management (incoming)</li><li>Hybrid memory allocator</li></ul><h1 id="1-Simplified-Scheduler"><a href="#1-Simplified-Scheduler" class="headerlink" title="1. Simplified Scheduler"></a>1. Simplified Scheduler</h1><p><strong>Simplified API</strong>: how many <em>extra</em> tokens do we schedule for each request compared to last time</p><p>→ unified code path for different types of optimizations (prefix sharing, spec decode, …)</p><table><thead><tr><th><strong>Algorithm</strong></th><th><strong>Description</strong></th><th><strong>API</strong></th></tr></thead><tbody><tr><td>Prefill</td><td>Prefill entire prompt at once</td><td>{r, 500}</td></tr><tr><td>Decode</td><td>Generate one new token</td><td>{r, 1}</td></tr><tr><td>Chunked prefill</td><td>chunk size &#x3D; 256</td><td>{r, 256}, {r, 244}</td></tr><tr><td>Prefix caching</td><td>r hits 200 cached tokens</td><td>{r: 300}</td></tr><tr><td>Spec decoding</td><td>guess 5 tokens per step</td><td>{r: 5}</td></tr><tr><td>Multi-modality</td><td>r is a seq of 100 text tokens + 500 img tokens + 100 text tokens</td><td>{r: 100}, {r: 500}, {r:100}</td></tr></tbody></table><p>Again, for <em><strong>different</strong></em> optimization algorithms, you just need to figure out <strong>how many new tokens to schedule</strong></p><p>[Public] vLLM x Ollama Meetup @ YC <a href="https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/">https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U&#x2F;</a></p><p><img src="/img/vLLM-04-vLLM-v1-version/image.png" alt="image.png"></p><p>scheduler path: <code>vllm/v1/core/sched/scheduler.py</code></p><ul><li>much easier to read than V0 scheduler <code>vllm/core/scheduler.py</code></li></ul><h1 id="2-General-arch-separate-processes"><a href="#2-General-arch-separate-processes" class="headerlink" title="2. General arch: separate processes"></a>2. General arch: separate processes</h1><h2 id="2-1-b-w-API-scheduler-de-tokenizer"><a href="#2-1-b-w-API-scheduler-de-tokenizer" class="headerlink" title="2.1  b&#x2F;w API&#x2F;scheduler&#x2F;(de) tokenizer"></a>2.1  b&#x2F;w API&#x2F;scheduler&#x2F;(de) tokenizer</h2><p>EngineCore will not be slowed down by others</p><p><img src="/img/vLLM-04-vLLM-v1-version/image1.png" alt="image.png"></p><p><strong>Lessons</strong></p><ul><li>Backend optimization is still important!</li><li>However, backend guys don’t really know frontend logic.<ul><li>LLM guys need to tell backend guys what to optimize.</li><li>Efficient communication is hard but critical.</li></ul></li></ul><h3 id="Open-discussion-microservice-for-LLM"><a href="#Open-discussion-microservice-for-LLM" class="headerlink" title="Open discussion: microservice for LLM?"></a><strong>Open discussion: microservice for LLM?</strong></h3><ul><li><p>Microservice will increase communication for better scalability. However, communication regarding LLM inference involves tensors → large overhead</p></li><li><p>When scalability issue is really bad, microservice will be more popular.</p><ul><li>Prediction: it will be popular in 2-3 years</li><li><strong>Time Machine Theory</strong>: an investment strategy that leverages time gaps between markets by replicating successful business models from advanced economies in emerging ones.</li></ul></li><li><p>P&#x2F;D disaggregation is a kind of microservice. P service + D service</p></li><li><p>P&#x2F;D disaggregation works well, but when other types of microservices work is still not clear</p></li><li><p>MoE disaggregation</p></li><li><p>Spec decoding disaggregation</p></li></ul><h3 id="Open-discussion-LLM-API-will-not-be-popular"><a href="#Open-discussion-LLM-API-will-not-be-popular" class="headerlink" title="Open discussion: LLM API will not be popular"></a>Open discussion: LLM API will not be popular</h3><ul><li>API essentially mixes different workloads</li><li>Different LLM workloads will interfere with each other</li><li>For example, if you put prefill-heavy workload and decode-heavy workloads together</li><li>prefill TTFT will be worse</li><li>decode ITL will be worse</li></ul><h2 id="2-2-between-scheduler-worker-for-TP"><a href="#2-2-between-scheduler-worker-for-TP" class="headerlink" title="2.2  between scheduler&#x2F;worker for TP"></a>2.2  between scheduler&#x2F;worker for TP</h2><ul><li><p><strong>V0</strong>: Scheduler &amp; Rank0 worker co-locate in the same process (previously in v0)</p><ul><li>Process 0 will broadcast the input → large overhead</li></ul></li><li><p><strong>V1</strong>: Scheduler &amp; Worker in separate process.</p><ul><li>Only broadcast the scheduling decision (a tiny object).</li><li>No input broadcast. Good for TP.</li></ul></li><li><p><strong>V1: strict SPMD</strong> (Single Program, Multiple Data) among workers</p><ul><li>Necessary for RLHF</li></ul></li></ul><p><img src="/img/vLLM-04-vLLM-v1-version/image2.png" alt="image.png"></p><h1 id="3-Worker"><a href="#3-Worker" class="headerlink" title="3. Worker"></a>3. Worker</h1><h2 id="3-1-Persistent-batching"><a href="#3-1-Persistent-batching" class="headerlink" title="3.1 Persistent batching"></a><strong>3.1 Persistent batching</strong></h2><ul><li>We only send the <strong>delta between the GPU tensors</strong> of previous batch and the GPU tensors of this batch from CPU to GPU.</li><li>We don’t construct all tensors from input requests each time</li><li>This technique is <em>not</em> new (<em>maybe</em> from LightLLM)</li></ul><p><strong>An extreme example</strong></p><ul><li>a request has 1M tokens, and each time one more token is added</li><li>in this case, we need to serialize 1M tokens each time</li><li>and we need to transfer those tensors from CPU to GPU</li><li>with persistent batching, only one token tensor is transferred to GPU</li></ul><p><strong>Code path</strong></p><ul><li><code>vllm/v1/worker/gpu_input_batch.py</code></li><li><code>vllm/v1/worker/gpu_model_runner.py</code></li></ul><p>Sample code from <code>gpu_input_batch.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.token_ids_cpu_tensor = torch.zeros(<br>    (max_num_reqs, max_model_len),<br>    device=<span class="hljs-string">&quot;cpu&quot;</span>,<br>    dtype=torch.int32,<br>    pin_memory=<span class="hljs-literal">False</span>,<br>)<br><span class="hljs-variable language_">self</span>.token_ids_cpu = <span class="hljs-variable language_">self</span>.token_ids_cpu_tensor.numpy()<br><span class="hljs-variable language_">self</span>.num_tokens = np.zeros(max_num_reqs, dtype=np.int32)<br><span class="hljs-variable language_">self</span>.num_tokens_no_spec = np.zeros(max_num_reqs, dtype=np.int32)<br><span class="hljs-variable language_">self</span>.num_prompt_tokens = np.zeros(max_num_reqs, dtype=np.int32)<br><span class="hljs-variable language_">self</span>.num_computed_tokens_cpu_tensor = torch.zeros(<br>    (max_num_reqs, ),<br>    device=<span class="hljs-string">&quot;cpu&quot;</span>,<br>    dtype=torch.int32,<br>    pin_memory=pin_memory,<br>)<br><span class="hljs-variable language_">self</span>.num_computed_tokens_cpu = \<br>    <span class="hljs-variable language_">self</span>.num_computed_tokens_cpu_tensor.numpy()<br><br></code></pre></td></tr></table></figure><p>The <code>InputBatch</code> class:</p><ul><li><p>Holds <strong>persistent, pinned-CPU buffers</strong> for every live request (prompt + already-generated tokens)</p></li><li><p><strong><code>token_ids_cpu_tensor</code></strong> (and its NumPy view); <strong><code>num_computed_tokens_cpu_tensor</code></strong></p></li><li><p>a <strong><code>BlockTable</code></strong> for tracking KV-cache block IDs</p></li><li><p>per-request sampling parameters (temperature, top-p&#x2F;k, penalties, logit-bias, etc.) in CPU &amp; device tensors</p></li><li><p>LoRA mappings, prompt-token tensors, generator handles, etc.</p></li><li><p>Exposes methods to <strong>add</strong> or <strong>remove</strong> a request</p></li><li><p><strong><code>add_request(request)</code></strong> copies only that request’s prompt + output token IDs into the CPU buffers, updates the block table and all sampling&#x2F;LoRA metadata</p></li><li><p><strong><code>remove_request(req_id)</code></strong> + <strong><code>condense(empty_indices)</code></strong> will clear out finished slots and pack the live ones into a dense batch</p></li><li><p>Lets the GPU runner do <strong>incremental H2D transfers</strong></p></li><li><p>In each <strong><code>_prepare_inputs</code></strong> call, GPUModelRunner simply</p><ul><li>a) re-indexes into the <em>same</em> pinned-CPU tensor (via <strong><code>torch.index_select</code></strong> on the flattened buffer) to gather “new” tokens for all requests</li><li>b) non-blocking-copies only those slices into the pre-allocated GPU input tensors</li><li>c) reuses the same block-table and sampling-metadata tensors on device</li></ul></li></ul><p>It is especially useful for <strong>long-running or streaming requests</strong> where only a <strong>small part of the input changes</strong> at each step.</p><h2 id="3-2-Piecewise-CUDA-graph"><a href="#3-2-Piecewise-CUDA-graph" class="headerlink" title="3.2 Piecewise CUDA graph"></a>3.2 Piecewise CUDA graph</h2><p>CUDA graph basics:</p><ul><li><p>Records a series of CUDA kernel operations to replay later</p></li><li><p>While CPU launching CUDA kernel is slow, individual CUDA kernels can be very fast</p></li><li><p><strong>Benefit</strong>: creates an abstraction to launch the whole sequence of kernels as one operator → significantly reduces CPU-GPU communication (each communication is close to 1 ms)</p></li><li><p><strong>Limitation</strong>: Does not record CPU operations, which reduces flexibility.</p><ul><li>No dynamic tensors are allowed.</li><li>Tensor shapes must be static.</li><li>Index ops must be in GPU.</li><li>Hard to debug. For example, some ops are wrongly put on CPU (while you think they are on GPU) and will not be recorded.</li></ul></li><li><p><strong>Key observation</strong>: Flexibility is typically needed in attention layer, not MLP layer</p><ul><li>For example, you may wanna switch to sparse attention, Mamba attention, RAGAttention, etc. and customize them</li></ul></li><li><p><strong>Solution</strong>: Piece-wise CUDA Graph - only records CUDA graph for MLP layers while keeping attention in PyTorch eager mode</p></li></ul><p><strong>How to deal with LoRA?</strong></p><ul><li>Put index to GPU buffer.</li><li>MLP layer: load corresponding LoRA adapter based on GPU buffer index</li></ul><p><strong>The philosophy behind this solution (classical system engineering technique) :</strong></p><ul><li>FeatureA is easy to use, but slow</li><li>FeatureB is fast, but hard to use</li><li>Hybrid solution (FeatureA + FeatureB) is preferred</li></ul><h1 id="4-Attention-kernel"><a href="#4-Attention-kernel" class="headerlink" title="4. Attention kernel"></a>4. Attention kernel</h1><h2 id="4-1-simplified-configuration"><a href="#4-1-simplified-configuration" class="headerlink" title="4.1 simplified configuration"></a>4.1 simplified configuration</h2><p><strong>Before</strong>: vLLM will prepare ~20 attention-related tensors</p><p><strong>Key observation</strong>: the info needed for each attention kernel is only 6-7 tensors</p><p><strong>Solution</strong>: only construct those key 6-7 tensors, and let the kernel do the rest</p><h2 id="4-2-Cascade-inference"><a href="#4-2-Cascade-inference" class="headerlink" title="4.2 Cascade inference"></a>4.2 Cascade inference</h2><p>Useful for the following scenarios:</p><ul><li><p>System prompt: 10k tokens</p></li><li><p>10 user chats: each chat with 100 tokens</p></li><li><p><strong>Normal attention</strong>: how many memory reads?</p><ul><li>(10k + 100) * 10 tokens</li></ul></li><li><p><strong>Cascade inference</strong>: Common prefix is read only once</p><ul><li>10k + 100 * 10 tokens</li></ul></li><li><p>vLLM has a performance model to decide when to use cascade inference</p></li></ul><p>code path: <code>vllm/v1/attention/backends/flash_attn.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">use_cascade_attention</span>(<span class="hljs-params"></span><br><span class="hljs-params">    common_prefix_len: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    query_lens: np.ndarray,</span><br><span class="hljs-params">    num_query_heads: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    num_kv_heads: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    use_alibi: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">    use_sliding_window: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">    num_sms: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Decide whether to use cascade attention.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    This function</span><br><span class="hljs-string">    1) checks whether cascade attention</span><br><span class="hljs-string">    is supported with the</span><br><span class="hljs-string">    given configuration,</span><br><span class="hljs-string">    and 2) heuristically decides whether</span><br><span class="hljs-string">    using cascade</span><br><span class="hljs-string">    attention can improve performance.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br></code></pre></td></tr></table></figure><h1 id="5-Multi-modality"><a href="#5-Multi-modality" class="headerlink" title="5. Multi-modality"></a>5. Multi-modality</h1><ul><li>Embedding as the KV cache reference</li><li>KV cache management (incoming)<ul><li>Hybrid memory allocator</li></ul></li></ul><p>Source</p><p><a href="https://www.youtube.com/watch?v=6AcgEPmpHIc">https://www.youtube.com/watch?v=6AcgEPmpHIc</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM inference</tag>
      
      <tag>vLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vLLM 03 - prefix caching</title>
    <link href="/2025/04/11/vLLM-03-prefix-caching/"/>
    <url>/2025/04/11/vLLM-03-prefix-caching/</url>
    
    <content type="html"><![CDATA[<h1 id="KV-cache-aware-routing-in-multi-host-serving"><a href="#KV-cache-aware-routing-in-multi-host-serving" class="headerlink" title="KV-cache-aware routing in multi-host serving"></a>KV-cache-aware routing in multi-host serving</h1><p><a href="https://github.com/vllm-project/production-stack/issues/59">https://github.com/vllm-project/production-stack/issues/59</a></p><p><a href="https://github.com/kubernetes-sigs/gateway-api-inference-extension/issues/498">https://github.com/kubernetes-sigs/gateway-api-inference-extension/issues/498</a></p><h2 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution 1"></a>Solution 1</h2><ul><li>Use string matching instead of token-ID-based matching<ul><li>tokenization itself is pretty slow (it takes several microseconds) so running it for every request creates huge overhead.</li></ul></li><li>implement the string server (e.g., using Redis) as the storage backend</li></ul><h2 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution 2"></a>Solution 2</h2><ul><li>Router can send a request to the KV cache management system: which server has the longest matched prefix?</li><li>vLLM production stack team wants to use this solution: it decouples the logic</li></ul><p><a href="https://github.com/vllm-project/production-stack/issues/59">https://github.com/vllm-project/production-stack/issues/59</a></p><p><strong>KV-cache-aware routing vs load balancing</strong>: Needs tradeoff. KV-cache-aware routing may route requests to the same node, which is bad for load balancing.</p><h1 id="KV-cache-store-interface"><a href="#KV-cache-store-interface" class="headerlink" title="KV cache store interface"></a>KV cache store interface</h1><p>Back in 2023 when vLLM was not out, let’s take a look at Huggingface’s LLM interface:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">llm.inference(<br>    <span class="hljs-built_in">input</span>.tokens: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>], <span class="hljs-comment"># N tokens</span><br>    previous_kv_cache: <span class="hljs-built_in">list</span>[Tensor], <span class="hljs-comment"># M tokens&#x27; KV cache, where M &lt; N</span><br>) -&gt; output_tokens, new_kv_cache<br><br>output_tokens: <span class="hljs-comment"># N&#x27; new tokens</span><br>new_kv_cache: <span class="hljs-comment"># (N+N&#x27;) tokens&#x27; KV cache</span><br><br></code></pre></td></tr></table></figure><p>Let’s not worry about PagedAttention or other complicated things in vLLM.</p><p>How do you design a KV cache?</p><p>KV cache design is essentially similar to traditional <strong>key-value store</strong> system design (such as in Redis, Object store, database, etc.)</p><p>Our key-value pair in this context is:</p><ul><li>key: tokens</li><li>value: KV cache tensors</li></ul><p>Then naturally, the interface should be like</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">KVCacheStore</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">store</span>(<span class="hljs-params">tokens, kv_cache_tensors</span>):<br>        <span class="hljs-keyword">pass</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">retrieve</span>(<span class="hljs-params">tokens</span>) -&gt; kv_cache_tensors<br>        <span class="hljs-keyword">pass</span><br><br></code></pre></td></tr></table></figure><p>Traditional key-value store usually supports <strong>exact match</strong> (given the full key, return the value)</p><p>KVCacheStore needs another query called <strong>prefix matching</strong> (actually, some traditional KV store also support this)</p><h1 id="Prefix-matching"><a href="#Prefix-matching" class="headerlink" title="Prefix matching"></a>Prefix matching</h1><p>Tokens1: ABCD<strong>E</strong> → [KV1, KV2, KV3, KV4, <strong>KV5</strong>]</p><p>Tokens2: ABCD<strong>F</strong> → [KV1 KV2, KV3, KV4, <strong>KV6</strong>]</p><p><code>kv_cache_store.store(”ABCDE”, [KV1, KV2, KV3, KV4, **KV5**])</code></p><p>When do <code>kv_cache_store.retrieve(”ABCDF”)</code>,</p><p>we expect the matched prefix part <code>[KV1, KV2, KV3, KV4]</code> to be returned</p><p>Trie is a good data structure that supports prefix matching.</p><p>vLLM implements a simplified “hash of chunks” structure to simulate a Trie:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># given tokens</span><br><span class="hljs-string">&quot;ABCDEF&quot;</span><br><span class="hljs-comment"># assume chunk size = 2, chunking:</span><br><span class="hljs-string">&quot;AB&quot;</span>, <span class="hljs-string">&quot;CD&quot;</span>, <span class="hljs-string">&quot;EF&quot;</span><br><span class="hljs-comment"># chunk hashes:</span><br>h1 = <span class="hljs-built_in">hash</span>(<span class="hljs-string">&quot;AB&quot;</span>)<br>h2 = <span class="hljs-built_in">hash</span>(h1 + <span class="hljs-string">&quot;CD&quot;</span>)<br>h3 = <span class="hljs-built_in">hash</span>(h2 + <span class="hljs-string">&quot;EF&quot;</span>)<br><br></code></pre></td></tr></table></figure><p>With prefix matching, store and retrieve become:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># store</span><br><span class="hljs-keyword">for</span> chunk_hash, chunk_kv <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(...)<br>    redis.put(chunk_hash, chunk_kv)<br>    <br>    <br><span class="hljs-comment"># retrieve</span><br><span class="hljs-keyword">for</span> chunk_hash <span class="hljs-keyword">in</span> ...:<br>    kv_chunk  = redis.get(chunk_hash)<br>    <span class="hljs-keyword">if</span> kv_chunk <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">break</span><br>    ...<br><br></code></pre></td></tr></table></figure><p><strong>Chunk size</strong>: tradeoff between matching granularity (hit rate) and management overhead</p><ul><li><p>inside serving engine:</p><ul><li>in vLLM, block size &#x3D; 16, so chunk size is also 16</li><li>SGLang can support chunk size &#x3D; 1</li></ul></li><li><p>outside serving engine (on disk):</p><ul><li>performance determined by # I&#x2F;O</li><li>bad performance for small objects, so we need a large chunk size</li></ul></li></ul><h2 id="Semantic-caching"><a href="#Semantic-caching" class="headerlink" title="Semantic caching"></a><strong>Semantic caching</strong></h2><p><strong>How to deal with semantically similar prefix</strong>: When receiving a request, before sending it to LLM, do a vector similarity search for similar requests. If similar request exists, we can bypass LLM and return the previous response.</p><h1 id="Eviction-policy"><a href="#Eviction-policy" class="headerlink" title="Eviction policy"></a>Eviction policy</h1><p>Which KV cache tensors to evict?</p><p>Inside each request:</p><ul><li>“ABCDEF” → [“AB”, KV1], [”CD”, KV2], [”EF”, KV3]</li><li>Evict from tail to head: KV3, KV2, KV1</li></ul><p>Among different requests: LRU, LFU, …</p><h1 id="vLLM-implementation"><a href="#vLLM-implementation" class="headerlink" title="vLLM implementation"></a>vLLM implementation</h1><h2 id="Retrieve"><a href="#Retrieve" class="headerlink" title="Retrieve"></a>Retrieve</h2><p>pass in requests to the kv_cache_manager</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">computed_blocks, num_computed_tokens = \<br>   <span class="hljs-variable language_">self</span>.kv_cache_manager.get_computed_blocks(request)<br><br></code></pre></td></tr></table></figure><p>get_computed_block() in V1</p><ol><li>chunking tokens to blocks of 16 tokens</li><li>use <code>block_pool.get_cached_block</code> to get computed blocks</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_computed_blocks</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, request: Request</span>) -&gt; <span class="hljs-built_in">tuple</span>[<span class="hljs-built_in">list</span>[KVCacheBlock], <span class="hljs-built_in">int</span>]:<br>    <span class="hljs-string">&quot;&quot;&quot;Get the computed (cached) blocks for the request.</span><br><span class="hljs-string">    Note that the computed blocks must be full.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        request: The request to get the computed blocks.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        A tuple containing:</span><br><span class="hljs-string">            - A list of blocks that are computed for the request.</span><br><span class="hljs-string">            - The number of computed tokens.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.enable_caching:<br>        <span class="hljs-comment"># Prefix caching is disabled.</span><br>        <span class="hljs-keyword">return</span> [], <span class="hljs-number">0</span><br><br>    <span class="hljs-comment"># The block hashes for the request may already be computed</span><br>    <span class="hljs-comment"># if the scheduler has tried to schedule the request before.</span><br>    block_hashes = <span class="hljs-variable language_">self</span>.req_to_block_hashes[request.request_id]<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> block_hashes:<br>        block_hashes = hash_request_tokens(<span class="hljs-variable language_">self</span>.block_size, request)<br>        <span class="hljs-variable language_">self</span>.req_to_block_hashes[request.request_id] = block_hashes<br><br>    <span class="hljs-variable language_">self</span>.prefix_cache_stats.requests += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">if</span> request.sampling_params.prompt_logprobs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># Check for cache hits</span><br>        computed_blocks = []<br>        <span class="hljs-keyword">for</span> block_hash <span class="hljs-keyword">in</span> block_hashes:<br>            <span class="hljs-comment"># block_hashes is a chain of block hashes. If a block hash</span><br>            <span class="hljs-comment"># is not in the cached_block_hash_to_id, the following</span><br>            <span class="hljs-comment"># block hashes are not computed yet for sure.</span><br>            <span class="hljs-keyword">if</span> cached_block := <span class="hljs-variable language_">self</span>.block_pool.get_cached_block(<br>                    block_hash):<br>                computed_blocks.append(cached_block)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">break</span><br><br>        <span class="hljs-variable language_">self</span>.prefix_cache_stats.queries += <span class="hljs-built_in">len</span>(block_hashes)<br>        <span class="hljs-variable language_">self</span>.prefix_cache_stats.hits += <span class="hljs-built_in">len</span>(computed_blocks)<br><br>        <span class="hljs-comment"># NOTE(woosuk): Since incomplete blocks are not eligible for</span><br>        <span class="hljs-comment"># sharing, `num_computed_tokens` is always a multiple of</span><br>        <span class="hljs-comment"># `block_size`.</span><br>        num_computed_tokens = <span class="hljs-built_in">len</span>(computed_blocks) * <span class="hljs-variable language_">self</span>.block_size<br>        <span class="hljs-keyword">return</span> computed_blocks, num_computed_tokens<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># Skip cache hits for prompt logprobs</span><br>        <span class="hljs-keyword">return</span> [], <span class="hljs-number">0</span><br><br><br></code></pre></td></tr></table></figure><h2 id="Store"><a href="#Store" class="headerlink" title="Store"></a>Store</h2><p>Defined in <code>/vllm/v1/core/block_pool.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BlockPool</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;BlockPool that manages KVCacheBlocks.</span><br><span class="hljs-string">    It provides methods to allocate, free and cache the KV cache blocks. The </span><br><span class="hljs-string">    free_block_queue stores the free blocks in eviction order to enable </span><br><span class="hljs-string">    allocation, free, and cache eviction. The cached_block_hash_to_block </span><br><span class="hljs-string">    maps between block hash and cached block to support finding cached blocks </span><br><span class="hljs-string">    by their block hash.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        num_gpu_blocks: The number of blocks in the pool.</span><br><span class="hljs-string">        enable_caching: Whether to enable prefix caching.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br></code></pre></td></tr></table></figure><p>The function is called <code>cache_full_blocks</code></p><p>It caches a list of full blocks for prefix caching.</p><p>This function takes a list of blocks that will have their block hash metadata to be updated and cached.</p><p>Given a request, it computes the block hashes for the blocks starting from <code>num_cached_blocks</code> to <code>num_full_blocks</code>, updating the metadata for each block and caching them in the <code>cached_block_hash_to_block</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cache_full_blocks</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    request: Request,</span><br><span class="hljs-params">    blocks: <span class="hljs-built_in">list</span>[KVCacheBlock],</span><br><span class="hljs-params">    block_hashes: <span class="hljs-built_in">list</span>[BlockHashType],</span><br><span class="hljs-params">    num_cached_blocks: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    num_full_blocks: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    block_size: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Cache a list of full blocks for prefix caching.</span><br><span class="hljs-string">    This function takes a list of blocks that will have their block hash</span><br><span class="hljs-string">    metadata to be updated and cached. Given a request, it computes the</span><br><span class="hljs-string">    block hashes for the blocks starting from `num_cached_blocks` to </span><br><span class="hljs-string">    `num_full_blocks`, updating the metadata for each block</span><br><span class="hljs-string">    and caching them in the `cached_block_hash_to_block`.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        request: The request to cache the blocks.</span><br><span class="hljs-string">        blocks: All blocks in the request.</span><br><span class="hljs-string">        block_hashes: Block hashes of the blocks in the request. Note that</span><br><span class="hljs-string">        this list may be shorter than the blocks list. In this case the </span><br><span class="hljs-string">        missed block hash will be computed in this function.</span><br><span class="hljs-string">        num_cached_blocks: The number of blocks that are already cached.</span><br><span class="hljs-string">        num_full_blocks: The number of blocks that are full and should </span><br><span class="hljs-string">            be cached after this function.</span><br><span class="hljs-string">        block_size: Number of tokens in each block.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> num_cached_blocks == num_full_blocks:<br>        <span class="hljs-keyword">return</span><br>    new_full_blocks = blocks[num_cached_blocks:num_full_blocks]<br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(block_hashes) &gt;= num_cached_blocks<br>    new_block_hashes = block_hashes[num_cached_blocks:]<br><br>    <span class="hljs-comment"># Update the new blocks with the block hashes through the chain.</span><br>    <span class="hljs-keyword">if</span> num_cached_blocks == <span class="hljs-number">0</span>:<br>        prev_block_hash_value = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">else</span>:<br>        prev_block = blocks[num_cached_blocks - <span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">assert</span> prev_block.block_hash <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        prev_block_hash_value = prev_block.block_hash.hash_value<br><br>    <span class="hljs-keyword">for</span> i, blk <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(new_full_blocks):<br>        <span class="hljs-keyword">assert</span> blk.block_hash <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> i &lt; <span class="hljs-built_in">len</span>(new_block_hashes):<br>            <span class="hljs-comment"># The block hash may already be computed in</span><br>            <span class="hljs-comment"># &quot;get_computed_blocks&quot; if the tokens are not generated by</span><br>            <span class="hljs-comment"># this request (either the prompt tokens or the previously</span><br>            <span class="hljs-comment"># generated tokens with preemption). In this case we simply</span><br>            <span class="hljs-comment"># reuse the block hash.</span><br>            block_hash = new_block_hashes[i]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># Otherwise compute the block hash and cache it in the request</span><br>            <span class="hljs-comment"># in case it will be preempted in the future.</span><br>            blk_idx = num_cached_blocks + i<br>            start_token_idx = blk_idx * block_size<br>            end_token_idx = (blk_idx + <span class="hljs-number">1</span>) * block_size<br>            block_tokens = request.all_token_ids[<br>                start_token_idx:end_token_idx]<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(block_tokens) == block_size, (<br>                <span class="hljs-string">f&quot;Expected <span class="hljs-subst">&#123;block_size&#125;</span> tokens, got &quot;</span><br>                <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(block_tokens)&#125;</span> at <span class="hljs-subst">&#123;blk_idx&#125;</span>th block for request &quot;</span><br>                <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;request.request_id&#125;</span>(<span class="hljs-subst">&#123;request&#125;</span>)&quot;</span>)<br><br>            <span class="hljs-comment"># Generate extra keys for multi-modal inputs. Note that since</span><br>            <span class="hljs-comment"># we reach to this branch only when the block is completed with</span><br>            <span class="hljs-comment"># generated tokens, we only need to consider the last mm input.</span><br>            extra_keys, _ = generate_block_hash_extra_keys(<br>                request, start_token_idx, end_token_idx, -<span class="hljs-number">1</span>)<br><br>            <span class="hljs-comment"># Compute the hash of the current block.</span><br>            block_hash = hash_block_tokens(prev_block_hash_value,<br>                                           block_tokens, extra_keys)<br>            block_hashes.append(block_hash)<br><br>        <span class="hljs-comment"># Update and add the full block to the cache.</span><br>        blk.block_hash = block_hash<br>        <span class="hljs-variable language_">self</span>.cached_block_hash_to_block[block_hash][blk.block_id] = blk<br>        prev_block_hash_value = block_hash.hash_value<br><br></code></pre></td></tr></table></figure><h2 id="Eviction"><a href="#Eviction" class="headerlink" title="Eviction"></a>Eviction</h2><p>still in <code>BlockPool</code></p><p>If a block is cached in <code>cached_block_hash_to_block</code>, we reset its hash metadata and evict it from the cache.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_maybe_evict_cached_block</span>(<span class="hljs-params">self, block: KVCacheBlock</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    If a block is cached in `cached_block_hash_to_block`, we reset its hash</span><br><span class="hljs-string">    metadata and evict it from the cache.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        block: The block to evict.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        True if the block is evicted, False otherwise.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    block_hash = block.block_hash<br>    <span class="hljs-keyword">if</span> block_hash <span class="hljs-keyword">and</span> block_hash <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.cached_block_hash_to_block:<br>        block.reset_hash()<br>        <span class="hljs-keyword">del</span> <span class="hljs-variable language_">self</span>.cached_block_hash_to_block[block_hash][block.block_id]<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.cached_block_hash_to_block[block_hash]) == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">del</span> <span class="hljs-variable language_">self</span>.cached_block_hash_to_block[block_hash]<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br><br><br></code></pre></td></tr></table></figure><p>Evictor class <code>FreeKVCacheBlockQueue</code>: use a doubly linked list for LRU eviction</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FreeKVCacheBlockQueue</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;This class organizes a list of KVCacheBlock objects to a doubly linked</span><br><span class="hljs-string">    list of free blocks. We implement this class instead of using Python</span><br><span class="hljs-string">    builtin deque to support removing a block in the middle of the queue</span><br><span class="hljs-string">    in O(1) time. To close the performance gap to the builtin deque which is</span><br><span class="hljs-string">    implemented in C++, this class does not allocate any Python objects when</span><br><span class="hljs-string">    manipulating the linked list. Instead, this class manipulates the </span><br><span class="hljs-string">    prev_free_block and next_free_block attributes of the given blocks.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    The queue is ordered by block ID in the beginning. When a block is allocated</span><br><span class="hljs-string">    and then freed, it will be appended back with the eviction order:</span><br><span class="hljs-string">    1. The least recently used block is at the front (LRU).</span><br><span class="hljs-string">    2. If two blocks have the same last accessed time (allocated by the</span><br><span class="hljs-string">       same sequence), the one with more hash tokens (the tail of a block</span><br><span class="hljs-string">       chain) is at the front.</span><br><span class="hljs-string">    Note that we maintain this order by reversing the block order when free</span><br><span class="hljs-string">    blocks of a request. This operation is outside of this class.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br></code></pre></td></tr></table></figure><p>Source:</p><p><a href="https://www.youtube.com/watch?v=mWvqA_BNtsU&list=PLJj_urhaf2_qxpg8A5-6xoMvMLBKQMTX1&index=18">https://www.youtube.com/watch?v=mWvqA_BNtsU&amp;list&#x3D;PLJj_urhaf2_qxpg8A5-6xoMvMLBKQMTX1&amp;index&#x3D;18</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM inference</tag>
      
      <tag>vLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vLLM 02 - speculative decoding</title>
    <link href="/2025/04/04/vLLM-02-speculative-decoding/"/>
    <url>/2025/04/04/vLLM-02-speculative-decoding/</url>
    
    <content type="html"><![CDATA[<h1 id="Why-Speculative-Decoding-SD"><a href="#Why-Speculative-Decoding-SD" class="headerlink" title="Why Speculative Decoding (SD)?"></a>Why Speculative Decoding (SD)?</h1><ul><li><p>Decoding is memory-bound: loading KV cache and model takes a long time</p></li><li><p>memory-bound cases: big matrix * small matrix; vector * matrix → O(n^2)</p></li><li><p>compute-bound cases: large matrix * matrix → O(n^3)</p></li><li><p>Find a way to <strong>increase compute</strong> while <strong>not</strong> significantly increasing GPU <strong>memory</strong> access</p></li><li><p><strong>Solution</strong>: Guess multiple tokens and verify</p><ul><li>Example: In terms of token generation per iteration:<ul><li>Guess 3 tokens, acceptance rate 2&#x2F;3</li><li>2 tokens of guessing are correct + LLM inference will generate a new token –&gt; 3 tokens</li></ul></li><li>Iteration time<ul><li>Computation: (1+3)x</li><li>Memory:<ul><li>w&#x2F;o SD: Model parameters (8x2 GB) + KV caches (n * 100 KB)</li><li>w&#x2F; SD: Model parameters (8x2 GB) + KV caches ((n+3) * 100 KB)</li></ul></li><li>Iteration time almost unchanged</li></ul></li></ul></li></ul><h1 id="How-to-guess-tokens"><a href="#How-to-guess-tokens" class="headerlink" title="How to guess tokens?"></a>How to guess tokens?</h1><h2 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h2><ul><li>most-used algorithm in production; simple and effective</li><li>N-grams essentially build a mapping like: if last 3 tokens are A, B, C, next 2 tokens are D, E</li><li>Example with “To be or not to be, this is a question”:<ul><li>[To be or] –&gt; [not to]</li><li>[be or not] –&gt; [to be]</li><li>…</li><li>[that is] –&gt; [a question]</li></ul></li></ul><p>Build N-gram from request input, use this N-gram to guess tokens.</p><ul><li>Example:</li></ul><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs applescript">The followings are Shakespeare&#x27;s <br>famous quotes:<br>.... <br>To be <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">to</span> be, this <span class="hljs-keyword">is</span> a <br>question<br>....<br>What <span class="hljs-keyword">is</span> <span class="hljs-keyword">the</span> best <span class="hljs-literal">quote</span> <span class="hljs-keyword">for</span> <span class="hljs-keyword">my</span> life?<br><br></code></pre></td></tr></table></figure><ul><li>Assume that LLM already generated:</li></ul><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-string">&quot;Sure! We recommend you</span><br>this quote<span class="hljs-punctuation">:</span> &#x27;To be or&#x27;<span class="hljs-string">&quot;</span><br><br></code></pre></td></tr></table></figure><ul><li>[To be or] –&gt; [not to]</li><li>Guess: next two tokens are [not to]</li><li>Verify: yes yes</li></ul><p>Bottleneck of SD: token-guessing algorithm</p><h3 id="Tree-verification"><a href="#Tree-verification" class="headerlink" title="Tree verification"></a>Tree verification</h3><ul><li>N-gram mapping can be one-to-many, because some phrases appear multiple times with different suffixes</li><li>[To be or] –&gt; [not to], [sleep in], [go to]</li><li>[To be or] guess multiple choices: [not to], [sleep in], [go to]</li><li>We need an efficient kernel for tree verification</li></ul><h3 id="How-to-tell-if-the-verification-is-right-or-wrong"><a href="#How-to-tell-if-the-verification-is-right-or-wrong" class="headerlink" title="How to tell if the verification is right or wrong?"></a>How to tell if the verification is right or wrong?</h3><ul><li>Deterministic sampling: bad case for SD, because only one correct answer and the acceptance rate may be low</li><li>Random sampling: correct when guess probability &gt; threshold</li></ul><p>Example: guessed tokens are <strong>right</strong></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs vim">Input: [To <span class="hljs-keyword">be</span> <span class="hljs-built_in">or</span>] (already-decoded <br>output) [not <span class="hljs-keyword">to</span>] (guessed tokens)<br>[To <span class="hljs-keyword">be</span> <span class="hljs-built_in">or</span> not <span class="hljs-keyword">to</span>]<br>[To -&gt; <span class="hljs-keyword">be</span>]<br>[<span class="hljs-keyword">be</span> -&gt; <span class="hljs-built_in">or</span>]<br>[<span class="hljs-built_in">or</span> -&gt; not] our guess <span class="hljs-string">&quot;not&quot;</span> <span class="hljs-keyword">is</span> correct<br>[not -&gt; <span class="hljs-keyword">to</span>] our guess <span class="hljs-string">&quot;to&quot;</span> <span class="hljs-keyword">is</span> correct<br>[<span class="hljs-keyword">to</span> -&gt; <span class="hljs-keyword">be</span>] <span class="hljs-keyword">be</span> <span class="hljs-keyword">is</span> the <span class="hljs-keyword">right</span> <span class="hljs-keyword">next</span> token<br><br></code></pre></td></tr></table></figure><p>Example: guessed tokens are <strong>wrong</strong></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-symbol">Input:</span> [To <span class="hljs-keyword">be </span><span class="hljs-keyword">or] </span>(already-decoded<br>output) [not <span class="hljs-keyword">be] </span>(guessed tokens)<br><span class="hljs-keyword">LLM: </span>[To <span class="hljs-keyword">be </span><span class="hljs-keyword">or </span>not <span class="hljs-keyword">be]</span><br><span class="hljs-keyword"></span>[To -&gt; <span class="hljs-keyword">be]</span><br><span class="hljs-keyword"></span>[<span class="hljs-keyword">be </span>-&gt; <span class="hljs-keyword">or]</span><br><span class="hljs-keyword"></span>[<span class="hljs-keyword">or </span>-&gt; not] our guess <span class="hljs-string">&quot;not&quot;</span> is <br>correct<br>[not -&gt; to] our guess <span class="hljs-string">&quot;be&quot;</span> is wrong, it <span class="hljs-keyword">should </span><span class="hljs-keyword">be </span><span class="hljs-string">&quot;to&quot;</span><br><br></code></pre></td></tr></table></figure><h2 id="Model-based-draft-model"><a href="#Model-based-draft-model" class="headerlink" title="Model-based (draft model)"></a>Model-based (draft model)</h2><ul><li><p>Parallel guessing: guesses next few tokens independently</p><ul><li>Fast but worse performance</li></ul></li><li><p>Autoregressive guessing</p></li></ul><h1 id="Why-deployment-production-is-so-hard"><a href="#Why-deployment-production-is-so-hard" class="headerlink" title="Why deployment&#x2F;production is so hard?"></a><strong>Why deployment&#x2F;production is so hard?</strong></h1><p>Acceptance rate is high enough (&gt; 75%)… so that’s not an issue</p><p><strong>SD is beneficial or not?</strong></p><ul><li>Workload may already be compute-bound</li><li>Batch size small: more memory-bound</li><li>Batch size large: more compute-bound</li><li>SD makes it move from memory-bound to compute-bound.</li><li>In practice, the workload may already be compute-bound, and SD will make it worse</li></ul><p><strong>How many tokens should we guess?</strong> - It should be determined based on arithmetic intensity</p><ul><li>Arithmetic intensity: metric to measure whether it’s compute-bound or memory-bound.</li><li>Arithmetic intensity &#x3D; FLOPs &#x2F; Bytes (or memory bandwidth)</li><li>Every hardware has a suitable intensity</li></ul><p><strong>Other engineering issues</strong></p><ul><li><p>Small model needs KV cache. How can we allocate that?</p><ul><li>If put with vLLM, extra mem</li><li>If not, not a single pool</li></ul></li><li><p>Small model may need different parallel config. For example, model TP&#x3D;8, draft model may need TP&#x3D;2</p><ul><li>Assume small model is no TP, and is on GPU0 + vLLM forces same GPU utilization on different GPUs → memory waste on other GPU memory</li></ul></li><li><p>Pre-allocate KV cache for guessed tokens.</p><ul><li>You break the system assumption that only one token is generated each time. You may need to change the whole vLLM interface</li><li>What if pre-allocated tokens cross vLLM’s block boundary?</li><li>Need to discard wrong tokens</li></ul></li><li><p>Sampling → verification</p></li><li><p>Minimize overhead (ngram) for loop is slow</p></li><li><p>How many # of tokens should we guess</p></li><li><p>How to distinguish between requests</p><ul><li>Different requests: different # of tokens, part of them do not run spec decode</li></ul></li></ul><p><strong>Summary of challenges above:</strong></p><ul><li>When SD is beneficial, how to find the best configuration</li><li>How to detect when SD is not beneficial? How to turn off SD in such cases?</li></ul><p>Paper reading:</p><ul><li>Optimizing Speculative Decoding for Serving Large Language Models Using Goodput</li><li>LLM Inference Unveiled: Survey and Roofline Model Insights</li></ul><p>Source</p><p><a href="https://www.youtube.com/watch?v=WF5xaQqtKUE&list=PLJj_urhaf2_qxpg8A5-6xoMvMLBKQMTX1&index=19">https://www.youtube.com/watch?v=WF5xaQqtKUE&amp;list=PLJj_urhaf2_qxpg8A5-6xoMvMLBKQMTX1&amp;index&#x3D;19</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM inference</tag>
      
      <tag>vLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vLLM 01 - P/D disaggregation</title>
    <link href="/2025/03/28/vllm-P-D-disaggregation/"/>
    <url>/2025/03/28/vllm-P-D-disaggregation/</url>
    
    <content type="html"><![CDATA[<h1 id="Why-P-D-disaggregation"><a href="#Why-P-D-disaggregation" class="headerlink" title="Why P&#x2F;D disaggregation?"></a>Why P&#x2F;D disaggregation?</h1><ul><li><strong>Initial scheduler logic in vLLM</strong>: prioritize prefill for good throughput</li><li><strong>Problem</strong>: prefill may slow down other requests’ decode</li></ul><h1 id="How-to-mix-P-and-D-together"><a href="#How-to-mix-P-and-D-together" class="headerlink" title="How to mix P and D together?"></a><strong>How to mix P and D together?</strong></h1><ul><li>Well, even their input shapes are different</li><li>Decode is vector*matrix (e.g., Q projection is 1xd * dxd). not many FLOPs + needs to load model weights and KV cache -&gt; it’s memory bound</li><li>Prefill is matrix*matrix (nxd * dxd). model weights are reused -&gt; compute bound</li><li><strong>Solutions</strong>: P&#x2F;D disaggregation, chunked prefill</li></ul><h1 id="Chunked-prefill"><a href="#Chunked-prefill" class="headerlink" title="Chunked prefill"></a>Chunked prefill</h1><h2 id="Motivation-of-chunked-prefill"><a href="#Motivation-of-chunked-prefill" class="headerlink" title="Motivation of chunked prefill"></a><strong>Motivation of chunked prefill</strong></h2><ul><li>Unify prefill and decode procedure: based on some KV cache, P or D does some attention &amp; linear computes, to generate some new tokens</li><li>The compute flow of prefill and decode are the same, and they just have difference in their input and output shapes</li><li>Chunked prefill becomes possible if the kernel can accept different shapes.</li><li>Now the scheduler can make simpler decision: only care about how many tokens to schedule in the current batch</li></ul><h2 id="Chunk-size"><a href="#Chunk-size" class="headerlink" title="Chunk size"></a><strong>Chunk size</strong></h2><ul><li>chunk size is very important</li><li>if chunk size is too large, then a decode can be slow when batching with a prefill -&gt; decode is slowed down by prefill</li><li>if chunk size is too small, then<ol><li>GPU utilization is bad, and FLOPs is low</li><li>it needs many batches to finish the prefill for a long prompt -&gt; prefill is slowed down by decode</li></ol></li></ul><h2 id="When-to-use-chunked-prefill"><a href="#When-to-use-chunked-prefill" class="headerlink" title="When to use chunked prefill"></a><strong>When to use chunked prefill</strong></h2><ul><li><p>prompts are extremely long (e.g., 10k or 100k tokens)</p></li><li><p>why? during attention compute, there will be temp buffers holding QKV, whose memory is proportional to context length. chunked prefill reduces context length, and thus reduces the buffer size</p></li><li><p>want smooth generation, e.g., SLO for p99 inter-token latency</p></li></ul><h1 id="Key-problems-P-D-disaggregation"><a href="#Key-problems-P-D-disaggregation" class="headerlink" title="Key problems P&#x2F;D disaggregation"></a>Key problems P&#x2F;D disaggregation</h1><h2 id="Connector-how-to-transfer-KV-cache"><a href="#Connector-how-to-transfer-KV-cache" class="headerlink" title="Connector: how to transfer KV cache?"></a><strong>Connector: how to transfer KV cache?</strong></h2><ul><li><strong>pooling mode</strong>: shared memory pool. both sender and receiver need high-bandwidth connection to the memory pool</li><li><strong>p2p mode</strong>: sender communicates with receiver directly; better performance; much harder to implement</li><li>Frameworks that support KV cache transfer: LMCache, Mooncake, NIXL</li></ul><p><strong>LMCache</strong> can do KV extraction and transfer</p><ul><li>support both pooling and p2p mode</li><li>current target use cases: prefill-decode disaggregation, KV cache offloading</li></ul><p><strong>Mooncake</strong></p><ul><li>KV cache storage: replicas, RDMA support, etc.</li><li>pooling mode</li></ul><p><strong>NIXL</strong>: p2p mode</p><ul><li>it does support p2p semantics directly. instead, it supports some lower-level data transfer features</li><li>backend is UXL, which is a more general data transfer library than NCCL’s own backend</li></ul><h2 id="How-to-extract-inject-KV-cache-in-vLLM"><a href="#How-to-extract-inject-KV-cache-in-vLLM" class="headerlink" title="How to extract &amp; inject KV cache in vLLM?"></a><strong>How to extract &amp; inject KV cache in vLLM?</strong></h2><p>connector API is called in model_runner</p><p>path: <code>vllm/worker/model_runner.py</code></p><ul><li><p>model runner is used to wrap model forward pass</p></li><li><p>preparing the input for forward</p></li><li><p>post-process forward outputs</p></li><li><p>one major part of model runner is to receive &amp; send KV cache</p></li></ul><p><strong>steps</strong></p><ul><li>before forward, try receiving KV cache and injecting into vLLM’s paged memory</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Receive KV cache in distributed KV cache transfer setting</span><br><span class="hljs-comment"># In disagg prefill setting, it will also recv hidden states and bypass</span><br><span class="hljs-comment"># model forwarding</span><br><span class="hljs-comment"># In KV cache database setting, it will change the model input so that</span><br><span class="hljs-comment"># we can skip prefilling on tokens that successfully received KV caches</span><br><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> The receive operation is blocking</span><br>bypass_model_exec = <span class="hljs-literal">False</span><br><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.need_recv_kv(model_input, kv_caches):<br>    hidden_or_intermediate_states, bypass_model_exec, model_input = \<br>        get_kv_transfer_group().recv_kv_caches_and_hidden_states(<br>            <span class="hljs-comment"># model is used to know which layer the current worker</span><br>            <span class="hljs-comment"># is working on, so that we can receive KV for only those</span><br>            <span class="hljs-comment"># layers.</span><br>            model_executable,<br>             ,<br>            kv_caches=kv_caches<br>        )<br><br></code></pre></td></tr></table></figure><ul><li>after forward, extract KV cache from vLLM’s paged memory, and send it outside</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Sending KV cache in distributed KV cache transfer setting</span><br>  <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> the send operation is non-blocking</span><br>  <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.need_send_kv(model_input, kv_caches):<br>      get_kv_transfer_group().send_kv_caches_and_hidden_states(<br>          <span class="hljs-comment"># model_executable is used to know which layer the current</span><br>          <span class="hljs-comment"># worker is working on, so that we can send KV for only those</span><br>          <span class="hljs-comment"># layers.</span><br>          model_executable,<br>          model_input,<br>          kv_caches,<br>          hidden_or_intermediate_states,<br>      )<br><br></code></pre></td></tr></table></figure><p>How are connector functions implemented?</p><p>check path <code>vllm/distributed/kv_transfer/kv_connector/</code></p><p>There are many possible connectors to use, let’s use <code>SimpleConnector</code> code as an example:</p><p>receive KV cache</p><ul><li>check if  <code>model_input</code>’s tokens exist in the outside world</li><li>if they do exist, we compute where the KV cache should be inserted into vLLM’s page memory (parse page table, use page index to find the right place)</li><li>additionally, it should rebuild the model input to tell the scheduler there is KV cache already, and no prefill should be done again</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">send_kv_caches_and_hidden_states</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    model_executable: torch.nn.Module,</span><br><span class="hljs-params">    model_input: <span class="hljs-string">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span>,</span><br><span class="hljs-params">    kv_caches: <span class="hljs-type">List</span>[torch.Tensor],</span><br><span class="hljs-params">    hidden_or_intermediate_states: <span class="hljs-type">Union</span>[torch.Tensor,</span><br><span class="hljs-params">                                         IntermediateTensors],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br><br>    <span class="hljs-comment"># some initial setup code</span><br>    <span class="hljs-comment"># ...</span><br><br>    <span class="hljs-comment"># query_lens contains new KV caches that are added to vLLM.</span><br>    <span class="hljs-comment"># so we will send them to decode instance</span><br>    <span class="hljs-comment"># FIXME(Kuntai): This assume that all requests are prefill.</span><br>    <span class="hljs-keyword">for</span> idx, slen <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(seq_lens):<br>        start_pos = <span class="hljs-built_in">sum</span>(seq_lens[:idx])<br>        end_pos = start_pos + slen<br><br>        <span class="hljs-keyword">if</span> start_pos &gt;= num_prefill_tokens:<br>            <span class="hljs-comment"># vllm/worker/model_runner.py::_prepare_model_input_tensors:</span><br>            <span class="hljs-comment"># - input_tokens[:num_prefill_tokens] contains prefill tokens.</span><br>            <span class="hljs-comment"># - input_tokens[num_prefill_tokens:] contains decode tokens.</span><br>            logger.warning(<span class="hljs-string">&quot;You have some decode requests while using &quot;</span><br>                           <span class="hljs-string">&quot;SimpleConnector. Their KVCache won&#x27;t be sent.&quot;</span>)<br>            <span class="hljs-keyword">break</span><br><br>        current_tokens = input_tokens_tensor[start_pos:end_pos]<br><br>        keys, values = [], []<br><br>        <span class="hljs-keyword">for</span> layer_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start_layer, end_layer):<br>            kv_cache = kv_caches[layer_id - start_layer]<br><br>            key_cache = kv_cache[<span class="hljs-number">0</span>].reshape(-<span class="hljs-number">1</span>, num_heads, head_size)<br>            value_cache = kv_cache[<span class="hljs-number">1</span>].reshape(-<span class="hljs-number">1</span>, num_heads, head_size)<br><br>            current_slot_mapping = slot_mapping_flat[start_pos:end_pos]<br><br>            keys.append(key_cache[current_slot_mapping].unsqueeze(<span class="hljs-number">0</span>))<br>            values.append(value_cache[current_slot_mapping].unsqueeze(<span class="hljs-number">0</span>))<br><br>        keys = torch.cat(keys, dim=<span class="hljs-number">0</span>)<br>        values = torch.cat(values, dim=<span class="hljs-number">0</span>)<br><br>        <span class="hljs-variable language_">self</span>.insert(current_tokens,<br>                    torch.ones_like(current_tokens,<br>                                    dtype=<span class="hljs-built_in">bool</span>), keys, values,<br>                    hidden_or_intermediate_states[start_pos:end_pos])<br><br>    logger.debug(<span class="hljs-string">&quot;[rank%d]: KV send DONE.&quot;</span>, torch.distributed.get_rank())<br><br></code></pre></td></tr></table></figure><p>Send KV is similar</p><h2 id="When-to-send-requests-to-P-and-D"><a href="#When-to-send-requests-to-P-and-D" class="headerlink" title="When to send requests to P and D?"></a>When to send requests to P and D?</h2><ul><li>First P then D: when P finishes, it will notify the router, and the router will tell D</li><li>First D then P: because D is the process to generate responses, let D be responsible for asking for KV cache from P</li></ul><p>###KV offloading</p><ul><li>Connector can also be used for KV cache offloading</li><li>In such cases, model sharding can be very useful. For example, in GPU-to-CPU KV cache offloading, if TP&#x3D;8, the total bandwidth is 8 * single_gpu_bandwidth.</li></ul><p>Source:</p><p><a href="https://www.youtube.com/watch?v=ih6fcJnhoJI">https://www.youtube.com/watch?v=ih6fcJnhoJI</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM inference</tag>
      
      <tag>vLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>

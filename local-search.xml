<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>vLLM 02 - speculative decoding</title>
    <link href="/2025/04/04/vLLM-02-speculative-decoding/"/>
    <url>/2025/04/04/vLLM-02-speculative-decoding/</url>
    
    <content type="html"><![CDATA[<h1 id="Why-Speculative-Decoding-SD"><a href="#Why-Speculative-Decoding-SD" class="headerlink" title="Why Speculative Decoding (SD)?"></a>Why Speculative Decoding (SD)?</h1><ul><li><p>Decoding is memory-bound: loading KV cache and model takes a long time</p></li><li><p>memory-bound cases: big matrix * small matrix; vector * matrix → O(n^2)</p></li><li><p>compute-bound cases: large matrix * matrix → O(n^3)</p></li><li><p>Find a way to <strong>increase compute</strong> while <strong>not</strong> significantly increasing GPU <strong>memory</strong> access</p></li><li><p><strong>Solution</strong>: Guess multiple tokens and verify</p><ul><li>Example: In terms of token generation per iteration:<ul><li>Guess 3 tokens, acceptance rate 2&#x2F;3</li><li>2 tokens of guessing are correct + LLM inference will generate a new token –&gt; 3 tokens</li></ul></li><li>Iteration time<ul><li>Computation: (1+3)x</li><li>Memory:<ul><li>w&#x2F;o SD: Model parameters (8x2 GB) + KV caches (n * 100 KB)</li><li>w&#x2F; SD: Model parameters (8x2 GB) + KV caches ((n+3) * 100 KB)</li></ul></li><li>Iteration time almost unchanged</li></ul></li></ul></li></ul><h1 id="How-to-guess-tokens"><a href="#How-to-guess-tokens" class="headerlink" title="How to guess tokens?"></a>How to guess tokens?</h1><h2 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h2><ul><li>most-used algorithm in production; simple and effective</li><li>N-grams essentially build a mapping like: if last 3 tokens are A, B, C, next 2 tokens are D, E</li><li>Example with “To be or not to be, this is a question”:<ul><li>[To be or] –&gt; [not to]</li><li>[be or not] –&gt; [to be]</li><li>…</li><li>[that is] –&gt; [a question]</li></ul></li></ul><p>Build N-gram from request input, use this N-gram to guess tokens.</p><ul><li>Example:</li></ul><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs applescript">The followings are Shakespeare&#x27;s <br>famous quotes:<br>.... <br>To be <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">to</span> be, this <span class="hljs-keyword">is</span> a <br>question<br>....<br>What <span class="hljs-keyword">is</span> <span class="hljs-keyword">the</span> best <span class="hljs-literal">quote</span> <span class="hljs-keyword">for</span> <span class="hljs-keyword">my</span> life?<br><br></code></pre></td></tr></table></figure><ul><li>Assume that LLM already generated:</li></ul><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-string">&quot;Sure! We recommend you</span><br>this quote<span class="hljs-punctuation">:</span> &#x27;To be or&#x27;<span class="hljs-string">&quot;</span><br><br></code></pre></td></tr></table></figure><ul><li>[To be or] –&gt; [not to]</li><li>Guess: next two tokens are [not to]</li><li>Verify: yes yes</li></ul><p>Bottleneck of SD: token-guessing algorithm</p><h3 id="Tree-verification"><a href="#Tree-verification" class="headerlink" title="Tree verification"></a>Tree verification</h3><ul><li>N-gram mapping can be one-to-many, because some phrases appear multiple times with different suffixes</li><li>[To be or] –&gt; [not to], [sleep in], [go to]</li><li>[To be or] guess multiple choices: [not to], [sleep in], [go to]</li><li>We need an efficient kernel for tree verification</li></ul><h3 id="How-to-tell-if-the-verification-is-right-or-wrong"><a href="#How-to-tell-if-the-verification-is-right-or-wrong" class="headerlink" title="How to tell if the verification is right or wrong?"></a>How to tell if the verification is right or wrong?</h3><ul><li>Deterministic sampling: bad case for SD, because only one correct answer and the acceptance rate may be low</li><li>Random sampling: correct when guess probability &gt; threshold</li></ul><p>Example: guessed tokens are <strong>right</strong></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs vim">Input: [To <span class="hljs-keyword">be</span> <span class="hljs-built_in">or</span>] (already-decoded <br>output) [not <span class="hljs-keyword">to</span>] (guessed tokens)<br>[To <span class="hljs-keyword">be</span> <span class="hljs-built_in">or</span> not <span class="hljs-keyword">to</span>]<br>[To -&gt; <span class="hljs-keyword">be</span>]<br>[<span class="hljs-keyword">be</span> -&gt; <span class="hljs-built_in">or</span>]<br>[<span class="hljs-built_in">or</span> -&gt; not] our guess <span class="hljs-string">&quot;not&quot;</span> <span class="hljs-keyword">is</span> correct<br>[not -&gt; <span class="hljs-keyword">to</span>] our guess <span class="hljs-string">&quot;to&quot;</span> <span class="hljs-keyword">is</span> correct<br>[<span class="hljs-keyword">to</span> -&gt; <span class="hljs-keyword">be</span>] <span class="hljs-keyword">be</span> <span class="hljs-keyword">is</span> the <span class="hljs-keyword">right</span> <span class="hljs-keyword">next</span> token<br><br></code></pre></td></tr></table></figure><p>Example: guessed tokens are <strong>wrong</strong></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-symbol">Input:</span> [To <span class="hljs-keyword">be </span><span class="hljs-keyword">or] </span>(already-decoded<br>output) [not <span class="hljs-keyword">be] </span>(guessed tokens)<br><span class="hljs-keyword">LLM: </span>[To <span class="hljs-keyword">be </span><span class="hljs-keyword">or </span>not <span class="hljs-keyword">be]</span><br><span class="hljs-keyword"></span>[To -&gt; <span class="hljs-keyword">be]</span><br><span class="hljs-keyword"></span>[<span class="hljs-keyword">be </span>-&gt; <span class="hljs-keyword">or]</span><br><span class="hljs-keyword"></span>[<span class="hljs-keyword">or </span>-&gt; not] our guess <span class="hljs-string">&quot;not&quot;</span> is <br>correct<br>[not -&gt; to] our guess <span class="hljs-string">&quot;be&quot;</span> is wrong, it <span class="hljs-keyword">should </span><span class="hljs-keyword">be </span><span class="hljs-string">&quot;to&quot;</span><br><br></code></pre></td></tr></table></figure><h2 id="Model-based-draft-model"><a href="#Model-based-draft-model" class="headerlink" title="Model-based (draft model)"></a>Model-based (draft model)</h2><ul><li><p>Parallel guessing: guesses next few tokens independently</p><ul><li>Fast but worse performance</li></ul></li><li><p>Autoregressive guessing</p></li></ul><h1 id="Why-deployment-production-is-so-hard"><a href="#Why-deployment-production-is-so-hard" class="headerlink" title="Why deployment&#x2F;production is so hard?"></a><strong>Why deployment&#x2F;production is so hard?</strong></h1><p>Acceptance rate is high enough (&gt; 75%)… so that’s not an issue</p><p><strong>SD is beneficial or not?</strong></p><ul><li>Workload may already be compute-bound</li><li>Batch size small: more memory-bound</li><li>Batch size large: more compute-bound</li><li>SD makes it move from memory-bound to compute-bound.</li><li>In practice, the workload may already be compute-bound, and SD will make it worse</li></ul><p><strong>How many tokens should we guess?</strong> - It should be determined based on arithmetic intensity</p><ul><li>Arithmetic intensity: metric to measure whether it’s compute-bound or memory-bound.</li><li>Arithmetic intensity &#x3D; FLOPS &#x2F; Bytes (or memory bandwidth)</li><li>Every hardware has a suitable intensity</li></ul><p><strong>Other engineering issues</strong></p><ul><li><p>Small model needs KV cache. How can we allocate that?</p><ul><li>If put with vLLM, extra mem</li><li>If not, not a single pool</li></ul></li><li><p>Small model may need different parallel config. For example, model TP&#x3D;8, draft model may need TP&#x3D;2</p><ul><li>Assume small model is no TP, and is on GPU0 + vLLM forces same GPU utilization on different GPUs → memory waste on other GPU memory</li></ul></li><li><p>Pre-allocate KV cache for guessed tokens.</p><ul><li>You break the system assumption that only one token is generated each time. You may need to change the whole vLLM interface</li><li>What if pre-allocated tokens cross vLLM’s block boundary?</li><li>Need to discard wrong tokens</li></ul></li><li><p>Sampling → verification</p></li><li><p>Minimize overhead (ngram) for loop is slow</p></li><li><p>How many # of tokens should we guess</p></li><li><p>How to distinguish between requests</p><ul><li>Different requests: different # of tokens, part of them do not run spec decode</li></ul></li></ul><p><strong>Summary of challenges above:</strong></p><ul><li>When SD is beneficial, how to find the best configuration</li><li>How to detect when SD is not beneficial? How to turn off SD in such cases?</li></ul><p>Paper reading:</p><ul><li>Optimizing Speculative Decoding for Serving Large Language Models Using Goodput</li><li>LLM Inference Unveiled: Survey and Roofline Model Insights</li></ul><p>Source</p><p><a href="https://www.youtube.com/watch?v=WF5xaQqtKUE&list=PLJj_urhaf2_qxpg8A5-6xoMvMLBKQMTX1&index=19">https://www.youtube.com/watch?v=WF5xaQqtKUE&amp;list=PLJj_urhaf2_qxpg8A5-6xoMvMLBKQMTX1&amp;index&#x3D;19</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>vLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vLLM 01 - P/D disaggregation</title>
    <link href="/2025/03/28/vllm-P-D-disaggregation/"/>
    <url>/2025/03/28/vllm-P-D-disaggregation/</url>
    
    <content type="html"><![CDATA[<h1 id="Why-P-D-disaggregation"><a href="#Why-P-D-disaggregation" class="headerlink" title="Why P&#x2F;D disaggregation?"></a>Why P&#x2F;D disaggregation?</h1><ul><li><strong>Initial scheduler logic in vLLM</strong>: prioritize prefill for good throughput</li><li><strong>Problem</strong>: prefill may slow down other requests’ decode</li></ul><h1 id="How-to-mix-P-and-D-together"><a href="#How-to-mix-P-and-D-together" class="headerlink" title="How to mix P and D together?"></a><strong>How to mix P and D together?</strong></h1><ul><li>Well, even their input shapes are different</li><li>Decode is vector*matrix (e.g., Q projection is 1xd * dxd). not many FLOPs + needs to load model weights and KV cache -&gt; it’s memory bound</li><li>Prefill is matrix*matrix (nxd * dxd). model weights are reused -&gt; compute bound</li><li><strong>Solutions</strong>: P&#x2F;D disaggregation, chunked prefill</li></ul><h1 id="Chunked-prefill"><a href="#Chunked-prefill" class="headerlink" title="Chunked prefill"></a>Chunked prefill</h1><h2 id="Motivation-of-chunked-prefill"><a href="#Motivation-of-chunked-prefill" class="headerlink" title="Motivation of chunked prefill"></a><strong>Motivation of chunked prefill</strong></h2><ul><li>Unify prefill and decode procedure: based on some KV cache, P or D does some attention &amp; linear computes, to generate some new tokens</li><li>The compute flow of prefill and decode are the same, and they just have difference in their input and output shapes</li><li>Chunked prefill becomes possible if the kernel can accept different shapes.</li><li>Now the scheduler can make simpler decision: only care about how many tokens to schedule in the current batch</li></ul><h2 id="Chunk-size"><a href="#Chunk-size" class="headerlink" title="Chunk size"></a><strong>Chunk size</strong></h2><ul><li>chunk size is very important</li><li>if chunk size is too large, then a decode can be slow when batching with a prefill -&gt; decode is slowed down by prefill</li><li>if chunk size is too small, then<ol><li>GPU utilization is bad, and FLOPs is low</li><li>it needs many batches to finish the prefill for a long prompt -&gt; prefill is slowed down by decode</li></ol></li></ul><h2 id="When-to-use-chunked-prefill"><a href="#When-to-use-chunked-prefill" class="headerlink" title="When to use chunked prefill"></a><strong>When to use chunked prefill</strong></h2><ul><li><p>prompts are extremely long (e.g., 10k or 100k tokens)</p></li><li><p>why? during attention compute, there will be temp buffers holding QKV, whose memory is proportional to context length. chunked prefill reduces context length, and thus reduces the buffer size</p></li><li><p>want smooth generation, e.g., SLO for p99 inter-token latency</p></li></ul><h1 id="Key-problems-P-D-disaggregation"><a href="#Key-problems-P-D-disaggregation" class="headerlink" title="Key problems P&#x2F;D disaggregation"></a>Key problems P&#x2F;D disaggregation</h1><h2 id="Connector-how-to-transfer-KV-cache"><a href="#Connector-how-to-transfer-KV-cache" class="headerlink" title="Connector: how to transfer KV cache?"></a><strong>Connector: how to transfer KV cache?</strong></h2><ul><li><strong>pooling mode</strong>: shared memory pool. both sender and receiver need high-bandwidth connection to the memory pool</li><li><strong>p2p mode</strong>: sender communicates with receiver directly; better performance; much harder to implement</li><li>Frameworks that support KV cache transfer: LMCache, Mooncake, NIXL</li></ul><p><strong>LMCache</strong> can do KV extraction and transfer</p><ul><li>support both pooling and p2p mode</li><li>current target use cases: prefill-decode disaggregation, KV cache offloading</li></ul><p><strong>Mooncake</strong></p><ul><li>KV cache storage: replicas, RDMA support, etc.</li><li>pooling mode</li></ul><p><strong>NIXL</strong>: p2p mode</p><ul><li>it does support p2p semantics directly. instead, it supports some lower-level data transfer features</li><li>backend is UXL, which is a more general data transfer library than NCCL’s own backend</li></ul><h2 id="How-to-extract-inject-KV-cache-in-vLLM"><a href="#How-to-extract-inject-KV-cache-in-vLLM" class="headerlink" title="How to extract &amp; inject KV cache in vLLM?"></a><strong>How to extract &amp; inject KV cache in vLLM?</strong></h2><p>connector API is called in model_runner</p><p>path: <code>vllm/worker/model_runner.py</code></p><ul><li><p>model runner is used to wrap model forward pass</p></li><li><p>preparing the input for forward</p></li><li><p>post-process forward outputs</p></li><li><p>one major part of model runner is to receive &amp; send KV cache</p></li></ul><p><strong>steps</strong></p><ul><li>before forward, try receiving KV cache and injecting into vLLM’s paged memory</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Receive KV cache in distributed KV cache transfer setting</span><br><span class="hljs-comment"># In disagg prefill setting, it will also recv hidden states and bypass</span><br><span class="hljs-comment"># model forwarding</span><br><span class="hljs-comment"># In KV cache database setting, it will change the model input so that</span><br><span class="hljs-comment"># we can skip prefilling on tokens that successfully received KV caches</span><br><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> The receive operation is blocking</span><br>bypass_model_exec = <span class="hljs-literal">False</span><br><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.need_recv_kv(model_input, kv_caches):<br>    hidden_or_intermediate_states, bypass_model_exec, model_input = \<br>        get_kv_transfer_group().recv_kv_caches_and_hidden_states(<br>            <span class="hljs-comment"># model is used to know which layer the current worker</span><br>            <span class="hljs-comment"># is working on, so that we can receive KV for only those</span><br>            <span class="hljs-comment"># layers.</span><br>            model_executable,<br>             ,<br>            kv_caches=kv_caches<br>        )<br><br></code></pre></td></tr></table></figure><ul><li>after forward, extract KV cache from vLLM’s paged memory, and send it outside</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Sending KV cache in distributed KV cache transfer setting</span><br>  <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> the send operation is non-blocking</span><br>  <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.need_send_kv(model_input, kv_caches):<br>      get_kv_transfer_group().send_kv_caches_and_hidden_states(<br>          <span class="hljs-comment"># model_executable is used to know which layer the current</span><br>          <span class="hljs-comment"># worker is working on, so that we can send KV for only those</span><br>          <span class="hljs-comment"># layers.</span><br>          model_executable,<br>          model_input,<br>          kv_caches,<br>          hidden_or_intermediate_states,<br>      )<br><br></code></pre></td></tr></table></figure><p>How are connector functions implemented?</p><p>check path <code>vllm/distributed/kv_transfer/kv_connector/</code></p><p>There are many possible connectors to use, let’s use <code>SimpleConnector</code> code as an example:</p><p>receive KV cache</p><ul><li>check if  <code>model_input</code>’s tokens exist in the outside world</li><li>if they do exist, we compute where the KV cache should be inserted into vLLM’s page memory (parse page table, use page index to find the right place)</li><li>additionally, it should rebuild the model input to tell the scheduler there is KV cache already, and no prefill should be done again</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">send_kv_caches_and_hidden_states</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    model_executable: torch.nn.Module,</span><br><span class="hljs-params">    model_input: <span class="hljs-string">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span>,</span><br><span class="hljs-params">    kv_caches: <span class="hljs-type">List</span>[torch.Tensor],</span><br><span class="hljs-params">    hidden_or_intermediate_states: <span class="hljs-type">Union</span>[torch.Tensor,</span><br><span class="hljs-params">                                         IntermediateTensors],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br><br>    <span class="hljs-comment"># some initial setup code</span><br>    <span class="hljs-comment"># ...</span><br><br>    <span class="hljs-comment"># query_lens contains new KV caches that are added to vLLM.</span><br>    <span class="hljs-comment"># so we will send them to decode instance</span><br>    <span class="hljs-comment"># FIXME(Kuntai): This assume that all requests are prefill.</span><br>    <span class="hljs-keyword">for</span> idx, slen <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(seq_lens):<br>        start_pos = <span class="hljs-built_in">sum</span>(seq_lens[:idx])<br>        end_pos = start_pos + slen<br><br>        <span class="hljs-keyword">if</span> start_pos &gt;= num_prefill_tokens:<br>            <span class="hljs-comment"># vllm/worker/model_runner.py::_prepare_model_input_tensors:</span><br>            <span class="hljs-comment"># - input_tokens[:num_prefill_tokens] contains prefill tokens.</span><br>            <span class="hljs-comment"># - input_tokens[num_prefill_tokens:] contains decode tokens.</span><br>            logger.warning(<span class="hljs-string">&quot;You have some decode requests while using &quot;</span><br>                           <span class="hljs-string">&quot;SimpleConnector. Their KVCache won&#x27;t be sent.&quot;</span>)<br>            <span class="hljs-keyword">break</span><br><br>        current_tokens = input_tokens_tensor[start_pos:end_pos]<br><br>        keys, values = [], []<br><br>        <span class="hljs-keyword">for</span> layer_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start_layer, end_layer):<br>            kv_cache = kv_caches[layer_id - start_layer]<br><br>            key_cache = kv_cache[<span class="hljs-number">0</span>].reshape(-<span class="hljs-number">1</span>, num_heads, head_size)<br>            value_cache = kv_cache[<span class="hljs-number">1</span>].reshape(-<span class="hljs-number">1</span>, num_heads, head_size)<br><br>            current_slot_mapping = slot_mapping_flat[start_pos:end_pos]<br><br>            keys.append(key_cache[current_slot_mapping].unsqueeze(<span class="hljs-number">0</span>))<br>            values.append(value_cache[current_slot_mapping].unsqueeze(<span class="hljs-number">0</span>))<br><br>        keys = torch.cat(keys, dim=<span class="hljs-number">0</span>)<br>        values = torch.cat(values, dim=<span class="hljs-number">0</span>)<br><br>        <span class="hljs-variable language_">self</span>.insert(current_tokens,<br>                    torch.ones_like(current_tokens,<br>                                    dtype=<span class="hljs-built_in">bool</span>), keys, values,<br>                    hidden_or_intermediate_states[start_pos:end_pos])<br><br>    logger.debug(<span class="hljs-string">&quot;[rank%d]: KV send DONE.&quot;</span>, torch.distributed.get_rank())<br><br></code></pre></td></tr></table></figure><p>Send KV is similar</p><h2 id="When-to-send-requests-to-P-and-D"><a href="#When-to-send-requests-to-P-and-D" class="headerlink" title="When to send requests to P and D?"></a>When to send requests to P and D?</h2><ul><li>First P then D: when P finishes, it will notify the router, and the router will tell D</li><li>First D then P: because D is the process to generate responses, let D be responsible for asking for KV cache from P</li></ul><p>###KV offloading</p><ul><li>Connector can also be used for KV cache offloading</li><li>In such cases, model sharding can be very useful. For example, in GPU-to-CPU KV cache offloading, if TP&#x3D;8, the total bandwidth is 8 * single_gpu_bandwidth.</li></ul><p>Source:</p><p><a href="https://www.youtube.com/watch?v=ih6fcJnhoJI">https://www.youtube.com/watch?v=ih6fcJnhoJI</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>vLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
